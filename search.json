[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alonso Silva‚Äôs Homepage",
    "section": "",
    "text": "Hi üëã My name is Alonso Silva and I‚Äôm a Senior Researcher on Generative AI at Nokia Bell Labs.\nI received my Ph.D.¬†in Physics from the √âcole Sup√©rieure d‚Äô√âlectricit√© (Sup√©lec) in 2010. I did my Ph.D.¬†at INRIA Sophia-Antipolis under the direction of Professor Eitan Altman. Prior to my Ph.D., I received my Mathematical Engineering degree from the Department of Mathematical Engineering (DIM) at the Universidad de Chile in 2006. I have previously worked as a Researcher at Safran from September 2018 to September 2022, a Researcher at Bell Labs from September 2012 to August 2018, Postdoctoral Researcher in the Department of EECS at the University of California, Berkeley, from September 2011 to August 2012 at INRIA Paris Rocquencourt from September 2010 to August 2011 and as Reseach Consultant/Intern at Bell Labs Headquarters in New Jersey from March to May 2010. I have received the Best Paper Award at IEEE SmartGridComm‚Äô17, UNet‚Äô17, the Second Best Paper Award at MSN‚Äô11 and the Best Student Paper Award at Valuetools‚Äô08."
  },
  {
    "objectID": "index.html#short-bio",
    "href": "index.html#short-bio",
    "title": "Alonso Silva‚Äôs Homepage",
    "section": "",
    "text": "Hi üëã My name is Alonso Silva and I‚Äôm a Senior Researcher on Generative AI at Nokia Bell Labs.\nI received my Ph.D.¬†in Physics from the √âcole Sup√©rieure d‚Äô√âlectricit√© (Sup√©lec) in 2010. I did my Ph.D.¬†at INRIA Sophia-Antipolis under the direction of Professor Eitan Altman. Prior to my Ph.D., I received my Mathematical Engineering degree from the Department of Mathematical Engineering (DIM) at the Universidad de Chile in 2006. I have previously worked as a Researcher at Safran from September 2018 to September 2022, a Researcher at Bell Labs from September 2012 to August 2018, Postdoctoral Researcher in the Department of EECS at the University of California, Berkeley, from September 2011 to August 2012 at INRIA Paris Rocquencourt from September 2010 to August 2011 and as Reseach Consultant/Intern at Bell Labs Headquarters in New Jersey from March to May 2010. I have received the Best Paper Award at IEEE SmartGridComm‚Äô17, UNet‚Äô17, the Second Best Paper Award at MSN‚Äô11 and the Best Student Paper Award at Valuetools‚Äô08."
  },
  {
    "objectID": "index.html#recent-talks",
    "href": "index.html#recent-talks",
    "title": "Alonso Silva‚Äôs Homepage",
    "section": "Recent talks",
    "text": "Recent talks\n\nBuilding Knowledge Graph-Based Agents with Structured Text Generation and Open-Weights Models, A. Silva, PyData Global, December 3-5, 2024 [slides] [code] \nEnhancing RAG-based apps by constructing and leveraging knowledge graphs with open-weights LLMs, A. Silva, PyData Paris, September 25-26, 2024 [slides] [code] \nHow to create reproducible notebooks (uv+marimo), A. Silva, Presentation at LINCS, November 8, 2024 [slides] [code] \n‚ÄúLarge Language Models Playing Mixed Strategy Nash Equilibrium Games,‚Äù A. Silva, NETGCOOP, Oct 9-11, 2024 [paper] [slides][code] \n‚ÄúLarge Language Models Playing Mixed Strategy Nash Equilibrium Games,‚Äù A. Silva, LINCS Annual Workshop, June 27, 2024 [paper] [slides][code] \nHow to create pure Python web apps (solara+anywidget) A. Silva, Presentation at LINCS, May 22, 2024 [slides][code] \nKeynote: Catching up on the weird world of LLMs, A. Silva, PyCon Chile, November 24-26, 2023 [slides] [code] \nWhen we cease to understand the world, A. Silva, Intervention at the European Parliament, November 16, 2023. \nKeynote: ‚ÄúImproving Retrieval Augmented Generation (RAG) with Hybrid RAG,‚Äù A. Silva, Summer school in computational intelligence (EVIC), December 11-13, 2024.\nKeynote: ‚ÄúImproving Retrieval Augmented Generation (RAG) with Hybrid RAG,‚Äù A. Silva, IADevs, Dec 9, 2024.\n‚ÄúImprove (almost) any retrieval augmented generation (RAG) with hybrid RAG,‚Äù A. Silva, PyCon Chile, November 30-December 1, 2024."
  },
  {
    "objectID": "blog/posts/2025-07-14-A-Void-by-Georges-Perec/A_Void.html",
    "href": "blog/posts/2025-07-14-A-Void-by-Georges-Perec/A_Void.html",
    "title": "A Void by Georges Perec",
    "section": "",
    "text": "Georges Perec (1936-1982) was a French novelist. He wrote a \\(300\\)-page novel La disparition, without using during the whole novel the letter e which is the most used vowel in French. The English translation A Void also kept the constraint of not using the letter e, while the Spanish translation El Secuestro is written without using the letter a (the most used vowel in Spanish).\nThis type of writing is called a lipogram and it is a type of constrained writing. Writing a lipogram is a trivial task when avoiding uncommon letters (e.g Z, J, Q, or X), but it is much more challenging to avoid common letters like (e.g.¬†E, T, or A) in the English language, as an author must omit many ordinary words.\n\nimport torch\nfrom typing import List\nfrom transformers.generation import LogitsProcessor\n\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\nfrom transformers.generation import LogitsProcessor\n\nmodel_id = \"Qwen/Qwen3-0.6B\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, cache_dir=\"/big_storage/llms/hf_models/\"\n).to(\"cuda\")\nstreamer = TextIteratorStreamer(tokenizer, skip_prompt=True)\n\n\nclass GeorgePerecLogitsProcessor(LogitsProcessor):\n    def __init__(self, min_length: int, eos_token_id: List[int], forbidden_tokens: List[int], device: str = \"cuda\"):\n        self.min_length = min_length\n        self.eos_token_id = eos_token_id\n        self.forbidden_tokens = forbidden_tokens\n        self.prompt_length_to_skip = None\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -&gt; torch.FloatTensor:\n        first_time = self.prompt_length_to_skip is None\n        if first_time:\n            self.prompt_length_to_skip = input_ids.shape[-1]\n        scores_processed = scores.clone()\n        token_count = input_ids.shape[-1] - self.prompt_length_to_skip\n        if token_count &lt; self.min_length:\n            for eos_token_id in self.eos_token_id:\n                scores_processed[0][self.eos_token_id] = float('-inf')\n            token_chosen_id = np.argmax(scores_processed.cpu())\n            \n        print(f\"token count: {token_count}, token: {token_chosen_id}: {tokenizer.decode([token_chosen_id])}\")\n        vocab_tensor = torch.arange(scores.shape[-1], device=scores.device)\n        forbidden_tokens = torch.tensor(self.forbidden_tokens, device=scores.device)\n        print(forbidden_tokens)\n        eos_token_mask = torch.isin(vocab_tensor, forbidden_tokens)\n        print(eos_token_mask)\n        scores_processed = torch.where(eos_token_mask, float('-inf'), scores)\n        # for token in self.forbidden_tokens:\n        #     scores_processed[0][token] = float('-inf')\n\n        return scores_processed\n\n\ntokens_containing_letter_e = []\nfor token in range(tokenizer.vocab_size):\n    if \"e\" in tokenizer.decode(token) or \"E\" in tokenizer.decode(token) or '–µ' in tokenizer.decode(token):\n        tokens_containing_letter_e.append(token)\nlen(tokens_containing_letter_e)\n\n48686\n\n\n\nlogits_processors = [\n    GeorgePerecLogitsProcessor(\n        min_length=20,\n        eos_token_id=[tokenizer.eos_token_id, tokenizer.pad_token_id],\n        forbidden_tokens=tokens_containing_letter_e,\n    )\n]\n\n\n[tokenizer.decode(token) for token in tokens_containing_letter_e[:20]]\n\n['E',\n 'e',\n 'er',\n 're',\n 'en',\n 'le',\n ' the',\n 'es',\n 'ed',\n 'et',\n 'el',\n 'ent',\n ' re',\n 'se',\n 'ex',\n 'em',\n 'ce',\n 'ate',\n 'ue',\n 'ew']\n\n\n\nspecial_tokens = tokenizer.special_tokens_map\n\n\nflattened_list = []\nfor item in special_tokens.values():\n    if isinstance(item, list):\n        flattened_list.extend(item)\n    else:\n        flattened_list.append(item)\n\n\nspecial_token_ids = {token: tokenizer.convert_tokens_to_ids(token) for token in flattened_list}"
  },
  {
    "objectID": "blog/posts/2025-07-13-Understanding-Logits-Processors/Understanding_Logits_Processors.html",
    "href": "blog/posts/2025-07-13-Understanding-Logits-Processors/Understanding_Logits_Processors.html",
    "title": "Understanding Logits Processors",
    "section": "",
    "text": "Logits processors are incredibly powerful and I honestly think they should receive more attention from the community. Logits processors, as their name implies, process the logits, that is, they process the outputs of the last layer of the neural network or the raw scores of the tokens. We can modify the raw scores and get a completely different result than the one the language model would have generated on its own. We will clarify this with some examples.\nIn this post, we will see some simple logits processors examples (minimum length and minimum new tokens length), as well as some more complex ones (replacing the end of sequence by a word and replacing the end of sequence by a phrase). We then conclude with two practical applications of logits processors: make reasoning models stop thinking by specifying a thinking budget as well as forcing reasoning models to think for a longer time for particularly difficult questions."
  },
  {
    "objectID": "blog/posts/2025-07-13-Understanding-Logits-Processors/Understanding_Logits_Processors.html#basic-example",
    "href": "blog/posts/2025-07-13-Understanding-Logits-Processors/Understanding_Logits_Processors.html#basic-example",
    "title": "Understanding Logits Processors",
    "section": "Basic Example",
    "text": "Basic Example\nLet‚Äôs start with a basic example of a logit processor. In this section, we won‚Äôt use the thinking capabilities of the language model.\nWe first download a small language model (0.6B parameters) and its tokenizer.\n\n\nShow the code\nimport torch\nfrom typing import List\nfrom threading import Thread\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\nfrom transformers.generation import LogitsProcessor\n\nmodel_id = \"Qwen/Qwen3-0.6B\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, cache_dir=\"/big_storage/llms/hf_models/\"\n).to(\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nstreamer = TextIteratorStreamer(tokenizer, skip_prompt=True)\n\n\nWe can ask the question:\n\nWhat‚Äôs 2 + 2?\n\nand see what the language model would have responded without any logits processor.\n\n\nShow the code\nuser_input = \"What's 2 + 2?\"\n\ndef generate_response(user_input, logits_processor=[], enable_thinking=False):\n    messages = [\n        {\"role\": \"user\", \"content\": user_input},\n    ]\n\n    prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n        enable_thinking=enable_thinking,\n    )\n\n    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    prompt_length = model_inputs['input_ids'].shape[-1]\n\n    generation_kwargs = dict(\n        model_inputs,\n        streamer=streamer,\n        logits_processor=logits_processor,\n        max_new_tokens=4 * 1024,\n        do_sample=False,\n        temperature=1.0,\n        top_p=1.0,\n        top_k=50,\n    )\n\n    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n    assistant_response = \"\"\n    for chunk in streamer:\n        assistant_response += chunk\n        # print(chunk, end=\"\")\n\n    clean_assistant_response = assistant_response.split(\"&lt;|im_end|&gt;\")[0]\n\n    if enable_thinking:\n        reasoning_trace = assistant_response.split(\"&lt;think&gt;\")[-1].split(\"&lt;/think&gt;\")[0]\n        thinking_length = len(tokenizer.encode(reasoning_trace))\n        if \"&lt;/think&gt;\" in assistant_response:\n            response_without_reasoning_trace = assistant_response.split(\"&lt;/think&gt;\")[-1]\n            response_length = len(tokenizer.encode(response_without_reasoning_trace))\n        else:\n            response_length = 0\n    else:\n        thinking_length = 0\n        response_length = len(tokenizer.encode(clean_assistant_response))\n    thread.join()\n    return clean_assistant_response, prompt_length, thinking_length, response_length\n\n\nassistant_response, prompt_length, thinking_length, response_length = generate_response(\n    user_input\n)\nprint(assistant_response)\n\n\n2 + 2 equals 4.\n\n\nThe answer is quite straightforward: 2 + 2 equals 4.\nThe number of tokens is the following:\n\n\nShow the code\nprint(\n    f\"# prompt tokens: {prompt_length}\\n# thinking tokens: {thinking_length}\\n# response tokens: {response_length}\"\n)\n\n\n# prompt tokens: 20\n# thinking tokens: 0\n# response tokens: 8\n\n\nThere are \\(20\\) prompt tokens and \\(8\\) response tokens. Here are the \\(8\\) response tokens:\n\n\nShow the code\nfor token in tokenizer.encode(assistant_response):\n    print(f\"id: {token}; token: {tokenizer.decode(token).replace(' ', '‚éµ')}\")\n\n\nid: 17; token: 2\nid: 488; token: ‚éµ+\nid: 220; token: ‚éµ\nid: 17; token: 2\nid: 16819; token: ‚éµequals\nid: 220; token: ‚éµ\nid: 19; token: 4\nid: 13; token: .\n\n\n\nMinimum Length\nLet‚Äôs force the language model to generate a longer answer. In order to do that, we can define the following logits processor:\n\nclass MinLengthLogitsProcessor(LogitsProcessor):\n    def __init__(self, min_length: int, eos_token_id: List[int]):\n        \n        self.min_length = min_length\n        self.eos_token_id = eos_token_id\n\n    def __call__(\n        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n    ) -&gt; torch.FloatTensor:\n        \n        scores_processed = scores.clone()\n        token_count = input_ids.shape[-1]\n        if token_count &lt; self.min_length:\n            for eos_token_id in self.eos_token_id:\n                scores_processed[0][eos_token_id] = float(\"-inf\")\n        return scores_processed\n\nThis logits processor consists of two parts:\n\nThe first is the constructor which just initializes the minimum length required and the list of end of sequence tokens.\nThe second is the callable method which clones the original scores, and if we haven‚Äôt yet reached the minimum length required, it will give a score of minus infinite to the end of sequence tokens, effectively preventing the language model to choose them and therefore preventing it from ending the sentence. The language model will need to continue talking for as long as we want. And that‚Äôs what we will see.\n\nLet‚Äôs instantiate this logits processor with a required minimum length of \\(40\\):\n\nlogits_processor = [\n    MinLengthLogitsProcessor(\n        min_length=40, eos_token_id=[tokenizer.eos_token_id, tokenizer.pad_token_id]\n    )\n]\n\nThis is the assistant response:\n\n\nShow the code\nassistant_response, prompt_length, thinking_length, response_length = generate_response(\n    user_input,\n    logits_processor=logits_processor,\n)\nprint(assistant_response)\n\n\n2 + 2 equals 4. Let me know if you have any other questions! üòä\n\n\nThe language model added to the previous response 2 + 2 equals 4. the phrase Let me know if you have any other questions! üòä\nWe made the language model do that!\nThe number of tokens is the following:\n\n\nShow the code\nprint(\n    f\"# prompt tokens: {prompt_length}\\n# thinking tokens: {thinking_length}\\n# response tokens: {response_length}\"\n)\n\n\n# prompt tokens: 20\n# thinking tokens: 0\n# response tokens: 20\n\n\nThe language model generated \\(20\\) tokens even though we asked for a minimum length of \\(40\\) tokens. The reason is that the logits processor also considers the \\(20\\) prompt tokens and \\(20+20\\ge40\\) so that‚Äôs correct.\n\n\nMinimum New Tokens Length\nLet‚Äôs remove the prompt tokens from the computation. The logits processor is slightly more complex:\n\nclass MinNewTokensLengthLogitsProcessor(LogitsProcessor):\n    def __init__(\n        self, min_new_tokens_length: int, eos_token_id: List[int]\n    ):\n        self.min_new_tokens_length = min_new_tokens_length\n        self.eos_token_id = eos_token_id\n        self.prompt_length_to_skip = None\n\n    def __call__(\n        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n    ) -&gt; torch.FloatTensor:\n        first_time = self.prompt_length_to_skip is None\n        if first_time:\n            self.prompt_length_to_skip = input_ids.shape[-1]\n        scores_processed = scores.clone()\n        token_count = input_ids.shape[-1] - self.prompt_length_to_skip\n        if token_count &lt; self.min_new_tokens_length:\n            for eos_token_id in self.eos_token_id:\n                scores_processed[0][eos_token_id] = float(\"-inf\")\n        return scores_processed\n\nWe have added a prompt_length_to_skip which will get its value from the length of the input ids only the first time the logits processor is called, effectively storing the prompt length. We then substract the prompt_length_to_skip from the token_count.\nLet‚Äôs instantiate this logits processor with a required minimum length of new tokens of \\(40\\).\n\nlogits_processor = [\n    MinNewTokensLengthLogitsProcessor(\n        min_new_tokens_length=40,\n        eos_token_id=[tokenizer.eos_token_id, tokenizer.pad_token_id],\n    )\n]\n\nThis is the assistant response:\n\n\nShow the code\nassistant_response, prompt_length, thinking_length, response_length = generate_response(\n    user_input,\n    logits_processor=logits_processor\n)\nprint(assistant_response)\n\n\n2 + 2 equals 4. Let me know if you have any other questions! üòä. üéâ. üîç. üß†. üß†. üß†.\n\n\nThe language model added to the previous phrase 2 + 2 equals 4. Let me know if you have any other questions! üòä the following phrase . üéâ. üîç. üß†. üß†. üß†.\nThe number of tokens is the following:\n\n\nShow the code\nprint(\n    f\"# prompt tokens: {prompt_length}\\n# thinking tokens: {thinking_length}\\n# response tokens: {response_length}\"\n)\n\n\n# prompt tokens: 20\n# thinking tokens: 0\n# response tokens: 40\n\n\nThe language model added some emojis in order to arrive to the required \\(40\\) response tokens. That‚Äôs ok."
  },
  {
    "objectID": "blog/posts/2025-07-13-Understanding-Logits-Processors/Understanding_Logits_Processors.html#replacements",
    "href": "blog/posts/2025-07-13-Understanding-Logits-Processors/Understanding_Logits_Processors.html#replacements",
    "title": "Understanding Logits Processors",
    "section": "Replacements",
    "text": "Replacements\n\nReplace the end of sequence by a word\nNow let‚Äôs do something slightly more complex. This time when the language model wants to finish its answer (in our case, after the phrase 2 + 2 equals 4.), we are going to replace the ending token with another token. In this case, with the token ‚éµHeck (my first choice was the F-word). Note that we can also replace any other token and see how the model would have continued the phrase.\n\nclass MinNewTokensLengthWithReplacementTokenLogitsProcessor(LogitsProcessor):\n    def __init__(\n        self,\n        min_new_tokens_length: int,\n        eos_token_id: List[int],\n        replacement_token_id: int\n    ):\n        self.min_new_tokens_length = min_new_tokens_length\n        self.eos_token_id = eos_token_id\n        self.replacement_token_id = replacement_token_id\n        self.prompt_length_to_skip = None\n        self.very_large_number = 10_000\n\n    def __call__(\n        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n    ) -&gt; torch.FloatTensor:\n        first_time = self.prompt_length_to_skip is None\n        if first_time:\n            self.prompt_length_to_skip = input_ids.shape[-1]\n        scores_processed = scores.clone()\n        token_count = input_ids.shape[-1] - self.prompt_length_to_skip\n        if token_count &lt; self.min_new_tokens_length:\n            token_chosen_id = torch.argmax(scores_processed).item()\n            if token_chosen_id in self.eos_token_id:\n                scores_processed[0][self.replacement_token_id] = self.very_large_number\n            for eos_token_id in self.eos_token_id:\n                scores_processed[0][eos_token_id] = float(\"-inf\")\n        return scores_processed\n\nLet‚Äôs instantiate this logits processor:\n\nlogits_processor=[\n    MinNewTokensLengthWithReplacementTokenLogitsProcessor(\n        min_new_tokens_length=40,\n        eos_token_id=[tokenizer.eos_token_id, tokenizer.pad_token_id],\n        replacement_token_id=tokenizer.encode(\" Heck\")[0],\n    )\n]\n\nHere is the assistant response:\n\n\nShow the code\nassistant_response, prompt_length, thinking_length, response_length = generate_response(\n    user_input,\n    logits_processor=logits_processor\n)\nprint(assistant_response)\n\n\n2 + 2 equals 4. Heck, that's a simple math problem. Heck, I'm just a AI assistant here. Heck, I'm not going to do that. Heck, I'm just going to tell you that 2 + 2 is 4.\n\n\nThe language model added to the very first phrase 2 + 2 equals 4. the following phrases Heck, that's a simple math problem. Heck, I'm just a AI assistant here. Heck, I'm not going to do that. Heck, I'm just going to tell you that 2 + 2 is 4.\nThere are completely different phrases with repect to the previous ones!\nBy modifying the end of sequence by the token ‚éµHeck we made the model take a completely different path compared to what the language model would have taken by itself.\n\n\nReplace the end of sequence by a phrase\nWe just replaced the end of sequence by a word (token) but it might be interesting to replace the end of sequence by a phrase. For example, we might want that the language model checks its answer. Let‚Äôs do that by replacing the end of sentence by the phrase Wait, let me check my answer.\nThe logits processor is slightly more complex since we need to generate a sequence of tokens and therefore find a way to keep the state (here the state will be kept by the index variable):\n\nclass MinNewTokensLengthWithReplacementLogitsProcessor(LogitsProcessor):\n    def __init__(\n        self,\n        min_new_tokens_length: int,\n        eos_token_id: List[int],\n        replacement_tokens_ids: List[int],\n    ):\n        self.min_new_tokens_length = min_new_tokens_length\n        self.eos_token_id = eos_token_id\n        self.replacement_tokens_ids = replacement_tokens_ids\n        self.prompt_length_to_skip = None\n        self.very_large_number = 10_000\n        self.index = -1\n\n    def __call__(\n        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n    ) -&gt; torch.FloatTensor:\n        first_time = self.prompt_length_to_skip is None\n        if first_time:\n            self.prompt_length_to_skip = input_ids.shape[-1]\n        scores_processed = scores.clone()\n        token_count = input_ids.shape[-1] - self.prompt_length_to_skip\n        token_chosen_id = torch.argmax(scores_processed).item()\n        if (\n            (token_count &lt; self.min_new_tokens_length)\n            and (token_chosen_id in self.eos_token_id)\n            and (self.index == -1)\n        ):\n            for eos_token_id in self.eos_token_id:\n                scores_processed[0][eos_token_id] = float(\"-inf\")\n            scores_processed[0][self.replacement_tokens_ids[0]] = self.very_large_number\n            self.index = 0\n\n        if len(self.replacement_tokens_ids) &gt; self.index &gt;= 0:\n            scores_processed[0][\n                self.replacement_tokens_ids[self.index]\n            ] = self.very_large_number\n            self.index += 1\n\n        if self.index == len(self.replacement_tokens_ids):\n            self.index = -1\n\n        return scores_processed\n\nLet‚Äôs instantiate the logits processor:\n\nlogits_processor = [\n    MinNewTokensLengthWithReplacementLogitsProcessor(\n        min_new_tokens_length=30,\n        eos_token_id=[tokenizer.eos_token_id, tokenizer.pad_token_id],\n        replacement_tokens_ids=tokenizer.encode(\" Wait, let me check my answer\")\n    )\n]\n\n\n\nShow the code\nassistant_response, prompt_length, thinking_length, response_length = generate_response(\n    user_input,\n    logits_processor=logits_processor\n)\nprint(assistant_response)\n\n\n2 + 2 equals 4. Wait, let me check my answer again. 2 + 2 is indeed 4. So the correct answer is 4.\n\n\nThe language model added to the first phrase 2 + 2 equals 4. the phrases Wait, let me check my answer again. 2 + 2 is indeed 4. So the correct answer is 4.\nWe forced the model to check its answer by replacing the end of sequence by the phrase Wait, let me check my answer and let the language model continue the phrase. That‚Äôs great!"
  },
  {
    "objectID": "blog/posts/2025-07-13-Understanding-Logits-Processors/Understanding_Logits_Processors.html#thinking-budget",
    "href": "blog/posts/2025-07-13-Understanding-Logits-Processors/Understanding_Logits_Processors.html#thinking-budget",
    "title": "Understanding Logits Processors",
    "section": "Thinking Budget",
    "text": "Thinking Budget\nThe previous sections were fun for me and I hope they were fun for you as well, but let‚Äôs now look at very practical applications of logits processors.\nMany people noticed that reasoning models are very verbose in their thinking and they were looking for practical ways to limit that. Qwen3 even provided the choice to remove thinking altogether for some questions (btw that‚Äôs what we did here by putting enable_thinking=False). However, what if we want to let the model think but not for too long. Couldn‚Äôt we define a thinking budget and if the model goes above that thinking budget you just make the thinking stop altogether?\nOf course, we can, thanks to logits processors!\nIf we ask the question What's 2 + 2? and let the language model think (enable_thinking=True) without any constraint, this is what we get:\n\n\nShow the code\nassistant_response, prompt_length, thinking_length, response_length = generate_response(\n    user_input,\n    enable_thinking=True\n)\nprint(assistant_response)\n\n\n&lt;think&gt;\nOkay, the user is asking, \"What's 2 + 2?\" Let me think about how to approach this. First, I need to make sure I understand the question correctly. The user is probably looking for the sum of 2 plus 2, which is 4. But maybe they're trying to get a different answer, like a joke or something else. Let me check if there's any context I'm missing.\n\nWait, sometimes people use \"2 + 2\" in a different way. For example, in some languages, numbers are written differently, but in English, it's straightforward. Also, maybe the user is testing if I can recognize that 2 + 2 equals 4. But I should also consider if there's any trick here. For instance, if they're using a calculator, the result would be 4. But since the question is simple, the answer is 4.\n\nI should also make sure there's no hidden meaning or cultural context. In most basic math problems, 2 + 2 is 4. So the answer is 4. I don't see any other possible interpretations here. The user might just want the direct answer. Let me confirm once more. Yes, 2 plus 2 is 4. So the final answer should be 4.\n&lt;/think&gt;\n\n2 + 2 equals 4.\n\n\nThe number of tokens is the following:\n\n\nShow the code\nprint(\n    f\"# prompt tokens: {prompt_length}\\n# thinking tokens: {thinking_length}\\n# response tokens: {response_length}\"\n)\n\n\n# prompt tokens: 16\n# thinking tokens: 269\n# response tokens: 10\n\n\nIf we let the language model think without any constraint, it uses \\(269\\) thinking tokens.\nLet‚Äôs consider the case when we force the language model to think for less than a fixed thinking budget. The logits processor would be:\n\nclass ThinkingBudgetLogitsProcessor(LogitsProcessor):\n    def __init__(\n        self,\n        thinking_budget: int,\n        eot_token_id: int\n    ):\n        self.thinking_budget = thinking_budget\n        self.eot_token_id = eot_token_id\n        self.prompt_length_to_skip = None\n        self.very_large_number = 10_000\n\n    def __call__(\n        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n    ) -&gt; torch.FloatTensor:\n        first_time = self.prompt_length_to_skip is None\n        if first_time:\n            self.prompt_length_to_skip = input_ids.shape[-1]\n        scores_processed = scores.clone()\n        token_count = input_ids.shape[-1] - self.prompt_length_to_skip\n        if token_count == self.thinking_budget:\n            scores_processed[0][self.eot_token_id] = self.very_large_number\n        return scores_processed\n\nWe can instantiate the logits processor with a thinking budget of \\(100\\):\n\nlogits_processor = [\n    ThinkingBudgetLogitsProcessor(\n        thinking_budget=100,\n        eot_token_id=tokenizer.encode(\"&lt;/think&gt;\")[0],\n    )\n]\n\n\n\nShow the code\nassistant_response, prompt_length, thinking_length, response_length = generate_response(\n    user_input,\n    logits_processor=logits_processor,\n    enable_thinking=True\n)\n\nprint(assistant_response)\n\n\n&lt;think&gt;\nOkay, the user is asking, \"What's 2 + 2?\" Let me think about how to approach this. First, I need to make sure I understand the question correctly. The user is probably looking for the sum of 2 plus 2, which is 4. But maybe they're trying to get a different answer, like a joke or something else. Let me check if there's any context I'm missing.\n\nWait, sometimes people use \"2 + 2&lt;/think&gt;\n\nThe answer is 4.\n\n\nThe number of tokens is:\n\nprint(\n    f\"# prompt tokens: {prompt_length}\\n# thinking tokens: {thinking_length}\\n# response tokens: {response_length}\"\n)\n\n# prompt tokens: 16\n# thinking tokens: 99\n# response tokens: 8\n\n\nWe have forced the model to stop thinking after a fixed number of thinking budget and then provide a response. This is very convenient if we want to limit the verbosity of reasoning models."
  },
  {
    "objectID": "blog/posts/2025-07-13-Understanding-Logits-Processors/Understanding_Logits_Processors.html#budget-forcing",
    "href": "blog/posts/2025-07-13-Understanding-Logits-Processors/Understanding_Logits_Processors.html#budget-forcing",
    "title": "Understanding Logits Processors",
    "section": "Budget Forcing",
    "text": "Budget Forcing\nAnother application of logits processors is budget forcing. Budget forcing consists of forcing the model to continue thinking for a longer time for particularly difficult problems by appending the token Wait when the language model wants to stop thinking. The idea comes from the paper s1: Simple test-time scaling which claims that budget forcing improves the language model accuracy from \\(50\\%\\) to \\(57\\%\\) in AIME 2024 dataset. In that paper, the authors force the model to continue thinking by replacing the stop thinking token (&lt;/think&gt;) by the token Wait.\nUsing logits processors, we can make it more general by replacing the stop thinking token (&lt;/think&gt;) by a phrase, for example the phrase Wait, let me check my answer.\nSimilar to the previous section, the logits processor would be:\n\nclass BudgetForcingLogitsProcessor(LogitsProcessor):\n    def __init__(\n        self,\n        thinking_budget: int,\n        eot_token_id: int,\n        replacement_tokens_ids: int,\n        eos_token_id: List[int],\n        device: str = \"cuda\",\n    ):\n        self.thinking_budget = thinking_budget\n        self.eot_token_id = eot_token_id\n        self.replacement_tokens_ids = replacement_tokens_ids\n        self.eos_token_id = eos_token_id\n        self.prompt_length_to_skip = None\n        self.very_large_number = 10_000\n        self.index = -1\n\n    def __call__(\n        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n    ) -&gt; torch.FloatTensor:\n        first_time = self.prompt_length_to_skip is None\n        if first_time:\n            self.prompt_length_to_skip = input_ids.shape[-1]\n        scores_processed = scores.clone()\n        token_count = input_ids.shape[-1] - self.prompt_length_to_skip\n        token_chosen_id = torch.argmax(scores_processed).item()\n        if (\n            (token_count &lt; self.thinking_budget)\n            and (token_chosen_id == self.eot_token_id)\n            and (self.index == -1)\n        ):\n            scores_processed[0][self.eot_token_id] = float(\"-inf\")\n            scores_processed[0][self.replacement_tokens_ids[0]] = self.very_large_number\n            self.index = 0\n        if len(self.replacement_tokens_ids) &gt; self.index &gt;= 0:\n            scores_processed[0][\n                self.replacement_tokens_ids[self.index]\n            ] = self.very_large_number\n            self.index += 1\n        if self.index == len(self.replacement_tokens_ids):\n            self.index = -1\n        return scores_processed\n\nWithout a logits processor, we saw that for the question What's 2 + 2?, the model would think for \\(269\\) tokens. Let‚Äôs instantiate the logits processor with a forced thinking budget of \\(400\\):\n\n\nShow the code\nlogits_processor = [\n    BudgetForcingLogitsProcessor(\n        thinking_budget=400,\n        eot_token_id=tokenizer.encode(\"&lt;/think&gt;\")[0],\n        replacement_tokens_ids=tokenizer.encode(\" Wait, let me check my answer\"),\n        eos_token_id=[tokenizer.eos_token_id, tokenizer.pad_token_id],\n    )\n]\n\n\nHere is the response:\n\nassistant_response, prompt_length, thinking_length, response_length = generate_response(\n    user_input,\n    logits_processor=logits_processor,\n    enable_thinking=True\n)\nprint(assistant_response)\n\n&lt;think&gt;\nOkay, the user is asking, \"What's 2 + 2?\" Let me think about how to approach this. First, I need to make sure I understand the question correctly. The user is probably looking for the sum of 2 plus 2, which is 4. But maybe they're trying to get a different answer, like a joke or something else. Let me check if there's any context I'm missing.\n\nWait, sometimes people use \"2 + 2\" in a different way. For example, in some languages, numbers are written differently, but in English, it's straightforward. Also, maybe the user is testing if I can recognize that 2 + 2 equals 4. But I should also consider if there's any trick here. For instance, if they're using a calculator, the result would be 4. But since the question is simple, the answer is 4.\n\nI should also make sure there's no hidden meaning or cultural context. In most basic math problems, 2 + 2 is 4. So the answer is 4. I don't see any other possible interpretations here. The user might just want the direct answer. Let me confirm once more. Yes, 2 plus 2 is 4. So the final answer should be 4.\n Wait, let me check my answer again. 2 + 2 is definitely 4. I think that's correct. No tricks here. The user might be testing if I can do basic arithmetic. So yeah, the answer is 4.\n Wait, let me check my answer again. 2 + 2 is 4. I think that's correct. No tricks here. The user might be testing if I can do basic arithmetic. So the answer is 4.\n Wait, let me check my answer again. 2 + 2 is 4. I think that's correct. No tricks here. The user might be testing if I can do basic arithmetic. So the answer is 4.\n Okay, I think I'm confident with this answer. The user is probably just looking for the sum of 2 plus 2, which is 4.\n&lt;/think&gt;\n\n2 + 2 equals 4.\n\n\nHere are the tokens:\n\nprint()\nprint(\n    f\"# prompt tokens: {prompt_length}\\n# thinking tokens: {thinking_length}\\n# response tokens: {response_length}\"\n)\n\n\n# prompt tokens: 16\n# thinking tokens: 445\n# response tokens: 10\n\n\nWe managed to make the language model to think for a longer time. The answer did not change for this simple quation but it would be interesting to see how the answer changes for other questions like in the s1: Simple test-time scaling paper.\nIt would be interesting to study if other phrases make the model take completely different reasoning paths and if those reasoning paths improve the language model accuracy.\nIn this post, we have seen what logits processors are, how to create them, how to use them, as well as some practical applications. There are many more interesting applications and we are just scratching the surface of what‚Äôs possible."
  },
  {
    "objectID": "blog/posts/2025-07-11-Understanding-Structured-Outputs/Understanding_Structured_Outputs.html",
    "href": "blog/posts/2025-07-11-Understanding-Structured-Outputs/Understanding_Structured_Outputs.html",
    "title": "Understanding Structured Outputs",
    "section": "",
    "text": "In my previous post, I explained that we can provide a description of a function to the language model and the language model will call that function even if the function itself has not been implemented!\nThis feature has several exciting applications. This is the power behind structured ouputs libraries such as Instructor and Marvin.\nIn this post, I start by providing a basic example of extracting information with structured outputs, then I give a slightly more complex example by extracting personal information of several people from a text. After that, I give an example of text classification with structured outputs. Finally, I explain how to do structured outputs in WebAssembly (optional but fun if you want to play with structured outputs in the browser in this post itself).\nThe language model has been trained (fine-tuned) to determine (given a prompt and a list of functions descriptions):"
  },
  {
    "objectID": "blog/posts/2025-07-11-Understanding-Structured-Outputs/Understanding_Structured_Outputs.html#basic-example",
    "href": "blog/posts/2025-07-11-Understanding-Structured-Outputs/Understanding_Structured_Outputs.html#basic-example",
    "title": "Understanding Structured Outputs",
    "section": "Basic Example",
    "text": "Basic Example\nLet‚Äôs start with a basic example to see an interesting application.\nLet‚Äôs download a small language model (1.7B parameters) and its tokenizer:\n\n\nShow the code\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n\nmodel_id = \"Qwen/Qwen3-1.7B\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, cache_dir=\"/big_storage/llms/hf_models/\"\n).to(\"cuda\")\nstreamer = TextIteratorStreamer(tokenizer, skip_prompt=True)\n\n\n\n\n\nWe can describe a function (which we won‚Äôt implement) that extracts the city and the country of the text.\n\n\nShow the code\nfrom pydantic import BaseModel, Field\nimport json\nfrom openai import pydantic_function_tool\n\nclass city_extractor(BaseModel):\n    \"\"\"Extracts the correctly inferred city, state and country name from the text with all the required parameters with correct types.\"\"\"\n\n    city: str = Field(..., description=\"city name, e.g. Berkeley\")\n    state: str = Field(..., description=\"state name, e.g. California\")\n    country: str = Field(..., description=\"country name, e.g United States\")\n\ntool = pydantic_function_tool(city_extractor)\nprint(json.dumps(tool, indent=4))\n\n\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"city_extractor\",\n        \"strict\": true,\n        \"parameters\": {\n            \"description\": \"Extracts the correctly inferred city, state and country name from the text with all the required parameters with correct types.\",\n            \"properties\": {\n                \"city\": {\n                    \"description\": \"city name, e.g. Berkeley\",\n                    \"title\": \"City\",\n                    \"type\": \"string\"\n                },\n                \"state\": {\n                    \"description\": \"state name, e.g. California\",\n                    \"title\": \"State\",\n                    \"type\": \"string\"\n                },\n                \"country\": {\n                    \"description\": \"country name, e.g United States\",\n                    \"title\": \"Country\",\n                    \"type\": \"string\"\n                }\n            },\n            \"required\": [\n                \"city\",\n                \"state\",\n                \"country\"\n            ],\n            \"title\": \"city_extractor\",\n            \"type\": \"object\",\n            \"additionalProperties\": false\n        },\n        \"description\": \"Extracts the correctly inferred city, state and country name from the text with all the required parameters with correct types.\"\n    }\n}\n\n\nWe can provide the text I live in the big apple and the function description above. We obtain the following response:\n\n\nShow the code\nuser_input = \"I live in the big apple\"\n\ndef generate_response(user_input, tool):\n    \n    messages = [\n        {\"role\": \"user\", \"content\": user_input},\n    ]\n    \n    prompt = tokenizer.apply_chat_template(\n        messages, tools=[tool], tokenize=False, add_generation_prompt=True, enable_thinking=True\n    )\n    \n    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    \n    generation_kwargs = dict(\n        model_inputs,\n        streamer=streamer,\n        max_new_tokens=4 * 1024,\n        do_sample=False,\n        temperature=1.0,\n        top_p=1.0,\n        top_k=50,\n    )\n    \n    from threading import Thread\n    \n    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n    thread.start()\n    \n    assistant_response = \"\"\n    for chunk in streamer:\n        assistant_response += chunk\n        print(chunk, end=\"\")\n    \n    thread.join()\n\n    return assistant_response\n\nassistant_response = generate_response(user_input, tool)\n\n\n&lt;think&gt;\nOkay, the user says, \"I live in the big apple.\" Let me think about how to handle this. The city_extractor function is supposed to get the city, state, and country from the text. \n\nFirst, the user mentions \"the big apple.\" The Big Apple is a common nickname for New York City. So, the city would be New York. But wait, the function might expect the full name. However, \"New York\" is the correct city name. The state would be New York, but since it's a city, maybe the state is not needed? Wait, the function requires all three: city, state, country. But in this case, the user is referring to a city, not a state. So maybe the state is not applicable here. But the function's parameters require all three. Hmm.\n\nWait, the function's description says it extracts the city, state, and country. But if the user is referring to a city, maybe the state is not present. However, the function might still require the state. But in this case, the user is in New York, which is a state. So the state would be New York, and the country is the United States. \n\nBut the user's statement is \"I live in the big apple,\" which is a city. So the city is New York, state is New York (since it's a city in the state), and country is United States. But the function might not require the state if it's a city. However, the function's parameters require all three. So maybe the state is still needed. \n\nAlternatively, maybe the function is designed to handle cases where the city is the state. But I need to check the function's parameters. The function's properties include state as a string, so even if it's a city, the state is required. But the user is in New York, which is a state. So the state would be New York, and the country is United States. \n\nSo the city_extractor function would return city: \"New York\", state: \"New York\", country: \"United States\". But I need to make sure that the function can handle that. The function's strict parameter is set to true, so it should correctly infer the parameters. \n\nTherefore, the correct call would be to city_extractor with city: \"New York\", state: \"New York\", country: \"United States\".\n&lt;/think&gt;\n\n&lt;tool_call&gt;\n{\"name\": \"city_extractor\", \"arguments\": {\"city\": \"New York\", \"state\": \"New York\", \"country\": \"United States\"}}\n&lt;/tool_call&gt;&lt;|im_end|&gt;\n\n\nInside the tool_call xml tags, we get the information:\n\n\nShow the code\nassistant_response_clean = assistant_response.split(\"&lt;tool_call&gt;\")[-1].split(\"&lt;/tool_call&gt;\")[0]\nassistant_response_json = json.loads(assistant_response_clean)\n\nfor key in assistant_response_json['arguments']:\n    print(f\"{key}: {assistant_response_json['arguments'][key]}\")\n\n\ncity: New York\nstate: New York\ncountry: United States\n\n\nThis is amazing! The language model was able to infer from the text I live in the big apple that the city I was referring to was New York, the state New York and the country United States!\nI want to stress again that all I needed was to provide to the language model a description of the function without ever implementing it."
  },
  {
    "objectID": "blog/posts/2025-07-11-Understanding-Structured-Outputs/Understanding_Structured_Outputs.html#extractor",
    "href": "blog/posts/2025-07-11-Understanding-Structured-Outputs/Understanding_Structured_Outputs.html#extractor",
    "title": "Understanding Structured Outputs",
    "section": "Extractor",
    "text": "Extractor\nA slightly more complicated example but interesting could be to have a personal information extractor. The logic is quite similar to the previous example. We need to describe a function that requires as arguments the information we want to extract (first_name, last_name, email). Here is the description of a function that does that:\n\n\nShow the code\nfrom typing import List\n\nclass Person(BaseModel):\n    \"\"\"It extracts the first name, the last name, and the email address mentioned in the text.\"\"\"\n\n    first_name: str = Field(..., description=\"the first name\")\n    last_name: str = Field(..., description=\"the last name\")\n    email: str = Field(..., description=\"the email address\")\n\n\nclass PeopleList(BaseModel):\n    people: List[Person] = Field(\n        ..., description=\"List of people mentioned in the text\"\n    )\n\ntool = pydantic_function_tool(PeopleList)\nprint(json.dumps(tool, indent=4))\n\n\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"PeopleList\",\n        \"strict\": true,\n        \"parameters\": {\n            \"$defs\": {\n                \"Person\": {\n                    \"description\": \"It extracts the first name, the last name, and the email address mentioned in the text.\",\n                    \"properties\": {\n                        \"first_name\": {\n                            \"description\": \"the first name\",\n                            \"title\": \"First Name\",\n                            \"type\": \"string\"\n                        },\n                        \"last_name\": {\n                            \"description\": \"the last name\",\n                            \"title\": \"Last Name\",\n                            \"type\": \"string\"\n                        },\n                        \"email\": {\n                            \"description\": \"the email address\",\n                            \"title\": \"Email\",\n                            \"type\": \"string\"\n                        }\n                    },\n                    \"required\": [\n                        \"first_name\",\n                        \"last_name\",\n                        \"email\"\n                    ],\n                    \"title\": \"Person\",\n                    \"type\": \"object\",\n                    \"additionalProperties\": false\n                }\n            },\n            \"properties\": {\n                \"people\": {\n                    \"description\": \"List of people mentioned in the text\",\n                    \"items\": {\n                        \"$ref\": \"#/$defs/Person\"\n                    },\n                    \"title\": \"People\",\n                    \"type\": \"array\"\n                }\n            },\n            \"required\": [\n                \"people\"\n            ],\n            \"title\": \"PeopleList\",\n            \"type\": \"object\",\n            \"additionalProperties\": false\n        }\n    }\n}\n\n\nWe could have a text that says:\n\n‚ÄúMy name is John Doe and you can contact me at sales@example.com and she is Jane Doe and can be contacted at support@example.com‚Äù\n\nWe can see that this small language model is able to extract the personal information from that text without any problem.\n\n\nShow the code\nuser_input = \"My name is John Doe and you can contact me at sales@example.com and she is Jane Doe and can be contacted at support@example.com\"\n\nassistant_response = generate_response(user_input, tool)\n\n\n&lt;think&gt;\nOkay, let me see. The user provided their name and contact info, and also mentioned someone else, Jane Doe with another email. I need to extract this into a list of people.\n\nFirst, the function PeopleList is available. It requires a list of Person objects, each with first_name, last_name, and email. The user's message has two entries: John Doe and Jane Doe. \n\nJohn Doe has first_name \"John\", last_name \"Doe\", and email \"sales@example.com\". Then Jane Doe has first_name \"Jane\", last_name \"Doe\", and email \"support@example.com\". \n\nI need to make sure each person's details are correctly mapped. The function's parameters are strict, so I have to check that all required fields are present. Both entries have first_name, last_name, and email, so that's good. \n\nI should structure the people array with each person as an object in the array. The function's example shows the people array as a list of Person objects. So the output should be two Person objects in the people array. \n\nNo other information is needed here. The user didn't mention any other people, so the list should only include John and Jane. \n\nDouble-checking the parameters: the function's properties are correct, and the emails are correctly assigned. Alright, that's all.\n&lt;/think&gt;\n\n&lt;tool_call&gt;\n{\"name\": \"PeopleList\", \"arguments\": {\"people\": [{\"first_name\": \"John\", \"last_name\": \"Doe\", \"email\": \"sales@example.com\"}, {\"first_name\": \"Jane\", \"last_name\": \"Doe\", \"email\": \"support@example.com\"}]}}\n&lt;/tool_call&gt;&lt;|im_end|&gt;\n\n\nInside the tool_call xml tags, we get the information we wanted to extract:\n\n\nShow the code\nassistant_response_clean = assistant_response.split(\"&lt;tool_call&gt;\")[-1].split(\"&lt;/tool_call&gt;\")[0]\nassistant_response_json = json.loads(assistant_response_clean)\n\nfor people in assistant_response_json['arguments']['people']:\n    print(f\"First name: {people['first_name']}, Last name: {people['last_name']}, E-mail: {people['email']}\")\n\n\nFirst name: John, Last name: Doe, E-mail: sales@example.com\nFirst name: Jane, Last name: Doe, E-mail: support@example.com\n\n\nThis is great. We can imagine several interesting applications. For example, we could use a small language model to automatically extract from CVs of candidates the information we are interested in and put it into a database instead of asking candidates to put that information themselves. Similarly, we could extract some specific information that we are interested in from websites (as an example, I saw an application that was just taking the required ingredients from a collection of cooking recipes websites)."
  },
  {
    "objectID": "blog/posts/2025-07-11-Understanding-Structured-Outputs/Understanding_Structured_Outputs.html#classifier",
    "href": "blog/posts/2025-07-11-Understanding-Structured-Outputs/Understanding_Structured_Outputs.html#classifier",
    "title": "Understanding Structured Outputs",
    "section": "Classifier",
    "text": "Classifier\nAnother example which is quite common for structured outputs is classification. Suppose we want to determine if an e-mail we received should be forwarded to the IT department or to the Sales department.\nBefore language models, to do this task we would need to create a dataset with different emails and the labels at which they correspond to and then train a model to do the classification. Now, we just need to provide a succint description. That‚Äôs neat!\nWe can describe a function that requires as arguments the information we want (IT department or Sales department). Here is the description of a function that does that:\n\n\nShow the code\nfrom typing import Literal\n\nfrom pydantic import BaseModel, Field\n\nclass Classifier(BaseModel):\n    \"\"\"Correctly inferred `team` the email should be directed to with all the required parameters with correct types.\"\"\"\n\n    team: Literal[\"IT department\", \"Sales department\"] = Field(\n        ..., description=\"Team at which should be the email should be directed to\"\n    )\ntool = pydantic_function_tool(Classifier)\nprint(json.dumps(tool, indent=4))\n\n\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"Classifier\",\n        \"strict\": true,\n        \"parameters\": {\n            \"description\": \"Correctly inferred `team` the email should be directed to with all the required parameters with correct types.\",\n            \"properties\": {\n                \"team\": {\n                    \"description\": \"Team at which should be the email should be directed to\",\n                    \"enum\": [\n                        \"IT department\",\n                        \"Sales department\"\n                    ],\n                    \"title\": \"Team\",\n                    \"type\": \"string\"\n                }\n            },\n            \"required\": [\n                \"team\"\n            ],\n            \"title\": \"Classifier\",\n            \"type\": \"object\",\n            \"additionalProperties\": false\n        },\n        \"description\": \"Correctly inferred `team` the email should be directed to with all the required parameters with correct types.\"\n    }\n}\n\n\nWe could have an email with the content:\n\n‚ÄúI would like to have more information related to the new product.‚Äù\n\nThis small language model is able to classify this text without any problem:\n\n\nShow the code\nuser_input = \"I would like to have more information related to the new product.\"\n\nassistant_response = generate_response(user_input, tool)\n\n\n&lt;think&gt;\nOkay, the user wants more information about a new product. Let me see. The available tool is a Classifier that determines the team to send the email to. The team options are IT department or Sales department.\n\nHmm, the user's request is about product information. Typically, Sales departments handle product details and customer inquiries. IT might be involved if there's a technical aspect, but the user didn't mention anything about technical issues. So, the most appropriate team here would be the Sales department. The Classifier function needs to be called with \"Sales department\" as the team parameter. I should make sure the JSON is correctly formatted with the team key and the appropriate value.\n&lt;/think&gt;\n\n&lt;tool_call&gt;\n{\"name\": \"Classifier\", \"arguments\": {\"team\": \"Sales department\"}}\n&lt;/tool_call&gt;&lt;|im_end|&gt;\n\n\nInside the tool_call xml tags, we get the correct department:\n\n\nShow the code\nassistant_response_clean = assistant_response.split(\"&lt;tool_call&gt;\")[-1].split(\"&lt;/tool_call&gt;\")[0]\nassistant_response_json = json.loads(assistant_response_clean)\n\nprint(assistant_response_json['arguments']['team'])\n\n\nSales department\n\n\nLet‚Äôs try another one:\n\n\nShow the code\nuser_input = \"I cannot exit Vim in my computer. Could you help me with that?\"\n\nassistant_response = generate_response(user_input, tool)\n\n\n&lt;think&gt;\nOkay, the user is having trouble exiting Vim. Let me think about how to help them. First, I need to determine which team they're dealing with because the Classifier function is supposed to figure that out. The team options are IT department or Sales department. But how does that relate to exiting Vim?\n\nHmm, maybe the user is using Vim for work, so they might be in the Sales department. But I'm not sure. The Classifier function requires the team to be specified. Since the user is asking about exiting Vim, which is a text editor, perhaps the IT department is more involved with technical support. But the Sales department might be more about business processes. \n\nWait, the function's description says it's for correctly inferred team. The user's query is about a technical issue, so maybe the IT department is the right team. But the user didn't mention anything about IT. This is a bit confusing. The function needs to be called with the team parameter. Since the user is asking about exiting Vim, which is a technical problem, the team might be IT. \n\nSo, I should call the Classifier function with the team as \"IT department\" to get the appropriate support. Even though the user didn't explicitly mention IT, the context of the problem suggests it. Therefore, the tool call would be to the Classifier function with team: \"IT department\".\n&lt;/think&gt;\n\n&lt;tool_call&gt;\n{\"name\": \"Classifier\", \"arguments\": {\"team\": \"IT department\"}}\n&lt;/tool_call&gt;&lt;|im_end|&gt;\n\n\nWe get again the correct department:\n\n\nShow the code\nassistant_response_clean = assistant_response.split(\"&lt;tool_call&gt;\")[-1].split(\"&lt;/tool_call&gt;\")[0]\nassistant_response_json = json.loads(assistant_response_clean)\n\nprint(assistant_response_json['arguments']['team'])\n\n\n'IT department'\n\n\nIn this example, we have seen only a simple classifier between two classes but the same code can be done with many more classes. Similarly, we can use a whole decision tree structure to classify the text and do different things in the leaf nodes of the decision tree (to minimize delay we could use asynchronous calls to the language model). We could determine to which department should the email be sent and for each department what‚Äôs the type of question the user is asking (e.g.¬†for sales, to which product is it related to while for IT department, wich program is it related to)."
  },
  {
    "objectID": "blog/posts/2025-07-11-Understanding-Structured-Outputs/Understanding_Structured_Outputs.html#webassembly",
    "href": "blog/posts/2025-07-11-Understanding-Structured-Outputs/Understanding_Structured_Outputs.html#webassembly",
    "title": "Understanding Structured Outputs",
    "section": "WebAssembly",
    "text": "WebAssembly\nWe can do structured outputs in the browser thanks to WebAssembly!\nWe first need to install some packages:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can download a quantized version of a small language model (0.6B parameters, not everything will work but most of it will). This step should take a few minutes the first time (later it should be stored in the cache):\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can send a prompt\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can define a tool and tokenize the prompt:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis is the generation part and it can take a few minutes:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe decode the assistant response:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis is the correct extracted information:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis tiny language model (quantized 0.6B parameters) was able to extract the information required!\nFeel free to modify the code and/or prompts and run them in the browser!"
  },
  {
    "objectID": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html",
    "href": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html",
    "title": "Understanding Chat Templates",
    "section": "",
    "text": "If you have worked with LLMs, you might have worked with lists of messages. Indeed, when you send a request to an LLM, we can call it a user messsage and the response can be called an assistant message. A conversation would consist of a list of a user message followed by an assistant message followed by a user message followed by an assistant message, etc. However, an LLM takes one text and outputs another text, so you might be wondering what‚Äôs the input text that‚Äôs being passed to the LLM. So what‚Äôs going on?\nA user message will be something like this:\n\nuser_message = [{\"role\": \"user\", \"content\": \"Hi!\"}]\n\nYou can pass the user message to the LLM to obtain an assistant message (it will take a few minutes the first time since it will download a small LLM):\n\n\n\n        \n        \n\n\nThe assistant message will be something like this:\n\nassistant_message = [{\"role\": \"user\", \"content\": \"Hello! How can I assist you today?\"}]\n\nTo have a conversation, you can pass it a list of messages:\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hi!\"},\n    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n    {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n]\n\nHow do we convert a list of messages to an input text?\nThe first time I encountered this was with Thomas Capelle from W&B. Our conversation went something like this:\n\nThomas: An LLM has no idea about user or assistant messages, it is just an autocompletion program.\nAlonso: So what‚Äôs going on with the list of messages I‚Äôm sending it? Are they just concatenated or what?\nThomas: No, no, no, it‚Äôs more complex than that, specially when you work with tools. You should check the model‚Äôs chat template.\nAlonso: How do I do that?\nThomas: It‚Äôs stored in the tokenizer, let me show you."
  },
  {
    "objectID": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#introduction",
    "href": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#introduction",
    "title": "Understanding Chat Templates",
    "section": "",
    "text": "If you have worked with LLMs, you might have worked with lists of messages. Indeed, when you send a request to an LLM, we can call it a user messsage and the response can be called an assistant message. A conversation would consist of a list of a user message followed by an assistant message followed by a user message followed by an assistant message, etc. However, an LLM takes one text and outputs another text, so you might be wondering what‚Äôs the input text that‚Äôs being passed to the LLM. So what‚Äôs going on?\nA user message will be something like this:\n\nuser_message = [{\"role\": \"user\", \"content\": \"Hi!\"}]\n\nYou can pass the user message to the LLM to obtain an assistant message (it will take a few minutes the first time since it will download a small LLM):\n\n\n\n        \n        \n\n\nThe assistant message will be something like this:\n\nassistant_message = [{\"role\": \"user\", \"content\": \"Hello! How can I assist you today?\"}]\n\nTo have a conversation, you can pass it a list of messages:\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hi!\"},\n    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n    {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n]\n\nHow do we convert a list of messages to an input text?\nThe first time I encountered this was with Thomas Capelle from W&B. Our conversation went something like this:\n\nThomas: An LLM has no idea about user or assistant messages, it is just an autocompletion program.\nAlonso: So what‚Äôs going on with the list of messages I‚Äôm sending it? Are they just concatenated or what?\nThomas: No, no, no, it‚Äôs more complex than that, specially when you work with tools. You should check the model‚Äôs chat template.\nAlonso: How do I do that?\nThomas: It‚Äôs stored in the tokenizer, let me show you."
  },
  {
    "objectID": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#the-chat-template",
    "href": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#the-chat-template",
    "title": "Understanding Chat Templates",
    "section": "The Chat Template",
    "text": "The Chat Template\nThe chat template takes as input a list of messages (and tools but we will talk about it later) and convert them into a single string. Let‚Äôs see what it does with an example.\n\n\n\n        \n        \n\n\nYou should get the following:\n&lt;|im_start|&gt;system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nHi!&lt;|im_end|&gt;\nNotice that when you don‚Äôt provide a system message, the model Qwen/Qwen2.5-0.5B-Instruct adds a system message (‚ÄúYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.‚Äù). Perhaps you want to change that to a different system message:\n\n\n\n        \n        \n\n\nWe can also see how does the chat template convert a conversation like this one:\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hi!\"},\n    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n    {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n]\n\n\n\n\n        \n        \n\n\nEach model has its own chat template. Let‚Äôs take a look at Mistral-7B-v0.3 chat template:\n\n\n\n        \n        \n\n\nYou should get:\n&lt;s&gt;[INST] Hi! [/INST]Hello! How can I assist you today?&lt;/s&gt;[INST] What's the capital of France? [/INST]\nSince after sending a user message, you expect an assistant message, you can help the model by basically saying ‚ÄúNow, it‚Äôs your turn!‚Äù. This is so useful that it has been incorporated into the chat template itself.\nThe instruction is:\ntokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n\n\n        \n        \n\n\nYou should get:\n&lt;|im_start|&gt;system\nYou are a helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nHi!&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nWhat‚Äôs interesting to me is that when you suggest to force a tool call or to force a tool, they look at you thinking you‚Äôre crazy even though it‚Äôs exactly the same thing (and it‚Äôs probably what OpenAI already does with the tool_choice=required and tool_choice: {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}."
  },
  {
    "objectID": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#tool-calls",
    "href": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#tool-calls",
    "title": "Understanding Chat Templates",
    "section": "Tool calls",
    "text": "Tool calls\nThe chat template also handles tool calls. That means that we can provide a list of tools (let‚Äôs do one as an example):\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"Python_REPL\",\n            \"description\": \"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n            \"parameters\": {\n                \"properties\": {\n                    \"python_code\": {\n                        \"description\": \"Valid python command.\",\n                        \"type\": \"string\",\n                    }\n                },\n                \"required\": [\"python_code\"],\n                \"type\": \"object\",\n            },\n        },\n    }\n]\n\nThe instruction is:\ntokenizer.apply_chat_template(messages, tokenize=False, tools=tools, add_generation_prompt=True)\n\n\n\n        \n        \n\n\nYou should get the following:\n&lt;|im_start|&gt;system\nYou are a helpful assistant.\n\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags:\n&lt;tools&gt;\n{\"type\": \"function\", \"function\": {\"name\": \"Python_REPL\", \"description\": \"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\", \"parameters\": {\"properties\": {\"python_code\": {\"description\": \"Valid python command.\", \"type\": \"string\"}}, \"required\": [\"python_code\"], \"type\": \"object\"}}}\n&lt;/tools&gt;\n\nFor each function call, return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags:\n&lt;tool_call&gt;\n{\"name\": &lt;function-name&gt;, \"arguments\": &lt;args-json-object&gt;}\n&lt;/tool_call&gt;&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nWhat's 2 to the power of 5?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nQuite complex string indeed.\nIf you look at some other model NousResearch/Hermes-3-Llama-3.1-8B, you see the following:\n\n\n\n        \n        \n\n\nYou should get the following:\n&lt;|begin_of_text|&gt;&lt;|im_start|&gt;system\nYou are a function calling AI model. You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: &lt;tools&gt; {\"type\": \"function\", \"function\": {\"name\": \"Python_REPL\", \"description\": \"Python_REPL(python_code: str) - A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\n\n    Args:\n        python_code(str): Valid python command.\", \"parameters\": {\"properties\": {\"python_code\": {\"description\": \"Valid python command.\", \"type\": \"string\"}}, \"required\": [\"python_code\"], \"type\": \"object\"}} &lt;/tools&gt;Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}, \"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}}, \"required\": [\"name\", \"arguments\"], \"title\": \"FunctionCall\", \"type\": \"object\"}}\nFor each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n&lt;tool_call&gt;\n{\"name\": &lt;function-name&gt;, \"arguments\": &lt;args-dict&gt;}\n&lt;/tool_call&gt;&lt;|im_end|&gt;\n&lt;|im_start|&gt;system\nYou are a helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nWhat's 2 to the power of 5?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nI don‚Äôt like this chat template. It appears that our messages have two different and consecutive system prompts. I prefer much more the previous chat template of the model ‚ÄúQwen/Qwen2.5-0.5B-Instruct‚Äù."
  },
  {
    "objectID": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#thinking-mode",
    "href": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#thinking-mode",
    "title": "Understanding Chat Templates",
    "section": "Thinking mode",
    "text": "Thinking mode\nA recent addition is the enable_thinking in some new reasoning models where the model will ‚Äúthink‚Äù between the XML tags &lt;think&gt;...&lt;/think&gt;. For example in Qwen/Qwen3-4B the model has the possibility to reason (which is the default), but you can turn this option off if you want to. The instruction is:\ntokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False)\n\n\n\n        \n        \n\n\nYou should get:\n&lt;|im_start|&gt;system\nYou are a helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nHow many r's in strawberry?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;think&gt;\n\n&lt;/think&gt;\nI saw some tweets (with multiple retweets) claiming they found a ‚Äúhack‚Äù to make the model not think and it was appending &lt;think&gt;\\n\\n&lt;/think&gt; while this is exactly what the chat template does!!!"
  },
  {
    "objectID": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#how-does-the-chat-template-handles-this",
    "href": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#how-does-the-chat-template-handles-this",
    "title": "Understanding Chat Templates",
    "section": "How does the chat template handles this?",
    "text": "How does the chat template handles this?\nThe chat template has been programmed in Jinja which is usually used in web development. You can see the chat template with the following command:\n\n\n\n        \n        \n\n\nYou should see the following:\n{%- if tools %}\n    {{- '&lt;|im_start|&gt;system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags:\\n&lt;tools&gt;\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n&lt;/tools&gt;\\n\\nFor each function call, return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags:\\n&lt;tool_call&gt;\\n{\\\"name\\\": &lt;function-name&gt;, \\\"arguments\\\": &lt;args-json-object&gt;}\\n&lt;/tool_call&gt;&lt;|im_end|&gt;\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '&lt;|im_start|&gt;system\\n' + messages[0].content + '&lt;|im_end|&gt;\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith('&lt;tool_response&gt;') and message.content.endswith('&lt;/tool_response&gt;')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '&lt;|im_start|&gt;' + message.role + '\\n' + content + '&lt;|im_end|&gt;' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '&lt;/think&gt;' in content %}\n                {%- set reasoning_content = content.split('&lt;/think&gt;')[0].rstrip('\\n').split('&lt;think&gt;')[-1].lstrip('\\n') %}\n                {%- set content = content.split('&lt;/think&gt;')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 &gt; ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '&lt;|im_start|&gt;' + message.role + '\\n&lt;think&gt;\\n' + reasoning_content.strip('\\n') + '\\n&lt;/think&gt;\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '&lt;|im_start|&gt;' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '&lt;|im_start|&gt;' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '&lt;tool_call&gt;\\n{\"name\": \"' }}\n                {{- tool_call.name }}\n                {{- '\", \"arguments\": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n&lt;/tool_call&gt;' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '&lt;|im_end|&gt;\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '&lt;|im_start|&gt;user' }}\n        {%- endif %}\n        {{- '\\n&lt;tool_response&gt;\\n' }}\n        {{- content }}\n        {{- '\\n&lt;/tool_response&gt;' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '&lt;|im_end|&gt;\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '&lt;|im_start|&gt;assistant\\n' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- '&lt;think&gt;\\n\\n&lt;/think&gt;\\n\\n' }}\n    {%- endif %}\n{%- endif %}\nAfter spending some time understanding what‚Äôs going on, you can create your own if you want to change its behavior."
  },
  {
    "objectID": "blog/posts/2024-04-23-Build_a_basic_widget_with_AnyWidget_in_a_Solara_app/2024-04-23-Build_a_basic_widget_with_AnyWidget_in_a_Solara_app.html",
    "href": "blog/posts/2024-04-23-Build_a_basic_widget_with_AnyWidget_in_a_Solara_app/2024-04-23-Build_a_basic_widget_with_AnyWidget_in_a_Solara_app.html",
    "title": "Build a basic widget with AnyWidget in a Solara app",
    "section": "",
    "text": "Build a basic widget with AnyWidget in a Solara app\nLet‚Äôs build the app below (try it out by clicking on the button and moving the slider).\n\n\n\n        \n        \n\n\nAnyWidget is a Python library that simplifies creating and publishing custom Jupyter Widgets. Since Jupyter Widgets have a VIP treatment in Solara ‚òÄÔ∏è, we expect them to work especially well there. This is indeed the case.\nFirst things first, let‚Äôs install AnyWidget and Solara ‚òÄÔ∏è.\n$ pip install anywidget solara\nLet‚Äôs now take the starting example from AnyWidget:\n\nimport anywidget\nimport traitlets\n\nclass CounterWidget(anywidget.AnyWidget):\n    _esm = \"\"\"\n    function render({ model, el }) {\n      let getCount = () =&gt; model.get(\"count\");\n      let button = document.createElement(\"button\");\n      button.classList.add(\"counter-button\");\n      button.innerHTML = `count is ${getCount()}`;\n      button.addEventListener(\"click\", () =&gt; {\n        model.set(\"count\", getCount() + 1);\n        model.save_changes();\n      });\n      model.on(\"change:count\", () =&gt; {\n        button.innerHTML = `count is ${getCount()}`;\n      });\n      el.appendChild(button);\n    }\n    export default { render };\n    \"\"\"\n    _css=\"\"\"\n    .counter-button { background-color: #ea580c; }\n    .counter-button:hover { background-color: #9a3412; }\n    \"\"\"\n    count = traitlets.Int(0).tag(sync=True)\n\nWe can create our Solara app by adding just a few lines of code:\n\nimport anywidget\nimport traitlets\n\nclass CounterWidget(anywidget.AnyWidget):\n    _esm = \"\"\"\n    function render({ model, el }) {\n      let getCount = () =&gt; model.get(\"count\");\n      let button = document.createElement(\"button\");\n      button.classList.add(\"counter-button\");\n      button.innerHTML = `count is ${getCount()}`;\n      button.addEventListener(\"click\", () =&gt; {\n        model.set(\"count\", getCount() + 1);\n        model.save_changes();\n      });\n      model.on(\"change:count\", () =&gt; {\n        button.innerHTML = `count is ${getCount()}`;\n      });\n      el.appendChild(button);\n    }\n    export default { render };\n    \"\"\"\n    _css=\"\"\"\n    .counter-button { background-color: #ea580c; }\n    .counter-button:hover { background-color: #9a3412; }\n    \"\"\"\n    count = traitlets.Int(0).tag(sync=True)\n\nimport solara\n@solara.component\ndef Page():\n    with solara.Column(style={\"padding\":\"30px\"}):\n        solara.Markdown(\"#Anywidget+Solara\")\n        CounterWidget.element()\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\nFrom Solara documentation, we know that in Solara, we should not create widgets, but elements instead. We can create elements by using the .element(...) method (as we did above). This method takes the same arguments as the widget constructor, but returns an element instead of a widget. The element can be used in the same way as a widget, but it is not a widget. It is a special object that can be used in Solara.\nTo make it more interesting, let‚Äôs modify our CounterWidget by changing the _css and adding some confetti from the canvas-confetti javascript package.\n\nimport anywidget\nimport traitlets\n\nclass CounterWidget(anywidget.AnyWidget):\n    _esm = \"\"\"\n    import confetti from \"https://esm.sh/canvas-confetti@1.6.0\"\n    function render({ model, el }) {\n      let getCount = () =&gt; model.get(\"count\");\n      let button = document.createElement(\"button\");\n      button.classList.add(\"counter-button\");\n      button.innerHTML = `count is ${getCount()}`;\n      button.addEventListener(\"click\", () =&gt; {\n        model.set(\"count\", getCount() + 1);\n        model.save_changes();\n      });\n      model.on(\"change:count\", () =&gt; {\n        button.innerHTML = `count is ${getCount()}`;\n        confetti({ angle: getCount() });\n      });\n      el.appendChild(button);\n    }\n    export default { render };\n    \"\"\"\n    _css=\"\"\"\n    .counter-button { background:blue; padding:10px 50px;}\n    .counter-button:hover { background-color:green; }\n    \"\"\"\n    count = traitlets.Int(0).tag(sync=True)\n\nimport solara\n@solara.component\ndef Page():\n    with solara.Column(style={\"padding\":\"30px\"}):\n        solara.Markdown(\"#Anywidget+Solara\")\n        CounterWidget.element()\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\nIf you want to access the value of the CounterWidget counter, we can do it through a reactive variable counter (thanks to Jonathan and Maarten for this suggestion):\n\nimport anywidget\nimport traitlets\n\nclass CounterWidget(anywidget.AnyWidget):\n    _esm = \"\"\"\n    import confetti from \"https://esm.sh/canvas-confetti@1.6.0\"\n    function render({ model, el }) {\n      let getCount = () =&gt; model.get(\"count\");\n      let button = document.createElement(\"button\");\n      button.classList.add(\"counter-button\");\n      button.innerHTML = `count is ${getCount()}`;\n      button.addEventListener(\"click\", () =&gt; {\n        model.set(\"count\", getCount() + 1);\n        model.save_changes();\n      });\n      model.on(\"change:count\", () =&gt; {\n        button.innerHTML = `count is ${getCount()}`;\n        confetti({ angle: getCount() });\n      });\n      el.appendChild(button);\n    }\n    export default { render };\n    \"\"\"\n    _css=\"\"\"\n    .counter-button { background:blue; padding:10px 50px;}\n    .counter-button:hover { background-color:green; }\n    \"\"\"\n    count = traitlets.Int(0).tag(sync=True)\n\nimport solara\ncounter = solara.reactive(0)\n@solara.component\ndef Page():\n    with solara.Column(style={\"padding\":\"30px\"}):\n        solara.Markdown(\"#Anywidget+Solara\")\n        CounterWidget.element(count=counter.value, on_count=counter.set)\n        solara.Markdown(f\"## Counter value is {counter.value}\")\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\nWe can add a slider from Jupyter Widgets and link it to the reactive variable counter.\n\nimport anywidget\nimport traitlets\n\nclass CounterWidget(anywidget.AnyWidget):\n    _esm = \"\"\"\n    import confetti from \"https://esm.sh/canvas-confetti@1.6.0\"\n    function render({ model, el }) {\n      let getCount = () =&gt; model.get(\"count\");\n      let button = document.createElement(\"button\");\n      button.classList.add(\"counter-button\");\n      button.innerHTML = `count is ${getCount()}`;\n      button.addEventListener(\"click\", () =&gt; {\n        model.set(\"count\", getCount() + 1);\n        model.save_changes();\n      });\n      model.on(\"change:count\", () =&gt; {\n        button.innerHTML = `count is ${getCount()}`;\n        confetti({ angle: getCount() });\n      });\n      el.appendChild(button);\n    }\n    export default { render };\n    \"\"\"\n    _css=\"\"\"\n    .counter-button { background:blue; padding:10px 50px;}\n    .counter-button:hover { background-color:green; }\n    \"\"\"\n    count = traitlets.Int(0).tag(sync=True)\n\nimport solara\nimport ipywidgets as widgets\ncounter = solara.reactive(0)\n@solara.component\ndef Page():\n    with solara.Column(style={\"padding\":\"30px\"}):\n        solara.Markdown(\"#Anywidget+Solara\")\n        CounterWidget.element(count=counter.value, on_count=counter.set)\n        widgets.IntSlider.element(min=-180, max=180, value=counter.value, on_value=counter.set)\n        solara.Markdown(f\"## Counter value is {counter.value}\")\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\nAnd that‚Äôs all. You can generate some other confetti animations from here. Create your own widgets with AnyWidget or use widgets from Jupyter Widgets or Solara itself and add them to your Solara app."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "‚ÄúLarge Language Models Playing Mixed Strategy Nash Equilibrium Games,‚Äù A. Silva, Proc. of the 11th International Conference on Network Games, Control and Optimization (Netgcoop), Lille, France, October 9-11, 2024.\n‚ÄúAggregation Methods and Comparative Study in Time-to-Event Analysis Models,‚Äù C. Fernandez, C.S. Chen, P. Gaillard, A. Silva, International Journal of Data Science and Analytics, September 25, 2024.\n‚ÄúPredicting Network Hardware Faults through Layered Treatment of Alarms Logs,‚Äù A. Massaro, D. Kostadinov, A. Silva, A. Obeid Guzman, A. Aghasaryan, Entropy, vol.¬†25, issue 6, pp.¬†917, 2023.\n‚ÄúExperimental Comparison of Semi-parametric, Parametric, and Machine Learning Methods for Time-to-Event Analysis Through the IPEC Score,‚Äù C. Fernandez, C.S. Chen, P. Gaillard, A. Silva, Proc. of the 52√®mes Journ√©es de Statistiques de la Soci√©t√© Fran√ßaise de Statistique (SFdS), Nice, France, June 7-11, 2021.\n‚ÄúExperimental Comparison of Semi-parametric, Parametric, and Machine Learning Models for Time-to-Event Analysis Through the Concordance Index,‚Äù C. Fernandez, C.S. Chen, P. Gaillard, A. Silva, Proc. of the 52√®mes Journ√©es de Statistiques de la Soci√©t√© Fran√ßaise de Statistique (SFdS), Nice, France, May 25-29, 2020.\n‚ÄúPath Planning Problems with Side Observations ‚Äî When Colonels Play Hide-and-Seek,‚Äù D.Q. Vu, P. Loiseau, A. Silva, L. Tran-Thanh, Proc of the 34th AAAI Conference on Artificial Intelligence (AAAI), New York, NY, USA, February 7-12, 2020.\n‚ÄúCombinatorial Bandits for Sequential Learning in Colonel Blotto Games,‚Äù D.Q. Vu, P. Loiseau, A. Silva, Proc. of the 58th IEEE Conference on Decision and Control (CDC), Nice, France, December 11-13, 2019.\n‚ÄúEfficient Computation of Approximate Equilibria in Discrete Colonel Blotto Games,‚Äù D.Q. Vu, P. Loiseau, A. Silva, Proc. of the 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence (IJCAI-ECAI), Stockholm, Sweden, July 13-19, 2018.\n‚ÄúA Simple and Efficient Algorithm to Compute Epsilon-Equilibria of Discrete Colonel Blotto Games,‚Äù D.Q. Vu, P. Loiseau, A. Silva, Proc. of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), Stockholm, Sweden, July 11-13, 2018. (Poster with extended abstract) Note: The IJCAI version above supersedes this extended abstract.\n‚ÄúOptimal Control of Storage Regeneration with Repair Codes,‚Äù F. De Pellegrini, R. El-Azouzi, A. Silva, O. Hassani, Proc. of the International Workshop on the Future of Cloud Computing and Cloud Services (FutureCloud), Hong Kong, China, December 11-14, 2017.\n‚ÄúNovel Market Approach for Locally Balancing Renewable Energy Production and Flexible Demand,‚Äù J. Horta, D. Kofman, D. Menga, A. Silva, Proc. of the IEEE International Conference on Smart Grid Communications (SmartGridComm), Dresden, Germany, October 23-26, 2017. Best Paper Award.\n‚ÄúEvolution of Social Power for Opinion Dynamics Networks,‚Äù S. Iglesias Rey, P. Reyes, A. Silva, Proc. of the 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton), Monticello, IL, USA, October 4-6, 2017.\n‚ÄúAdvertising Competitions in Social Networks,‚Äù A.M. Masucci, A. Silva, Proc. of the American Control Conference (ACC), Seattle, WA, USA, May 24-26, 2017.\n‚ÄúGreen Base Station Placement for Microwave Backhaul Links,‚Äù A. Silva, A.M. Masucci, Proc. of the 3rd International Symposium on Ubiquitous Networking (UNet), Casablanca, Morocco, May 9-12, 2017. Best Paper Award.\n‚ÄúOpinion Manipulation in Social Networks,‚Äù A. Silva, Proc. of the International Conference on Network Games, Control and Optimization (NETGCOOP), Avignon, France, November 23-25, 2016.\n‚ÄúGo-Index: Applying Supply Networks Principles as Internet Robustness Metrics,‚Äù I. Bachmann, F. Morales, A. Silva, J. Bustos-Jim√©nez, Proc. of the International Conference on Network Games, Control and Optimization (NETGCOOP), Avignon, France, November 23-25, 2016.\n‚ÄúOn the Throughput-Delay Trade-off in Georouting Networks,‚Äù P. Jacquet, S. Malik, B. Mans, A. Silva, IEEE Transactions on Information Theory, Vol. 62, Issue 6, pp.¬†3230‚Äì3242, June 2016,\\ ISSN: 0018-9448, DOI: 10.1109/TIT.2016.2519419.\n‚ÄúDefensive Resource Allocation in Social Networks,‚Äù A.M. Masucci, A. Silva, Proc. of the 54th IEEE Conference on Decision and Control (CDC), Osaka, Japan, December 15-18, 2015.\n‚ÄúMiuz: Measuring the Impact of Disconnecting a Node,‚Äù I. Bachmann, P. Reyes, A. Silva, J. Bustos-Jim√©nez, Proc. of the 34th SCCC 2015, Santiago, Chile, November 9-13, 2015.\n‚ÄúStrategic Resource Allocation for Competitive Influence in Social Networks,‚Äù A.M. Masucci, A. Silva, Proc. of the 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton), Monticello, IL, USA, October 1-3, 2014.\n‚ÄúMaximum Coverage and Maximum Connected Covering in Social Networks with Partial Topology Information,‚Äù P. Reyes, A. Silva, Proc. of the 6th IEEE INFOCOM Workshop on Network Science for Communication Networks (NetSciCom), Toronto, Canada, April 27- May 2, 2014.\n‚ÄúInformation Spreading on Almost Torus Networks,‚Äù A.M. Masucci, A. Silva, Proc. of the 52nd IEEE Conference on Decision and Control (CDC), Florence, Italy, December 10-13, 2013.\n‚ÄúOptimal Mobile Association on Hybrid Networks: Centralized and Decentralized Case,‚Äù A. Silva, T. Hamidou, E. Altman, M. Debbah, IEEE Transactions on Automatic Control, Volume 58, Issue 8, Pages 2018-2031, August 2013, ISSN: 0018-9286, DOI: 10.1109/TAC.2013.2250072.\n‚ÄúA Multi-Layer Market for Vehicle-to-Grid Energy Trading in the Smart Grid,‚Äù A.Y.S. Lam, L. Huang, A. Silva, W. Saad. Proc. of the 1st IEEE INFOCOM Workshop on Green Networking and Smart Grids (CCSES), Orlando, FL, USA, March 25-30, 2012.\n‚ÄúOn the Throughput-Delay Trade-off in Georouting Networks,‚Äù P. Jacquet, S. Malik, B. Mans, A. Silva. Proc. of the 31st IEEE INFOCOM 2012, Orlando, FL, USA, March 25-30, 2012.\n‚ÄúOn the Spectral Gap of Random Geometric Graphs and Their Mobility Properties,‚Äù A. Silva, G. Tucci. Proc. of the The 7th International Conference on Mobile Ad-hoc and Sensor Networks (MSN), Beijing, China, December 16-18, 2011. Second Best Paper Award.\n‚ÄúOptimal base station placement based on interference gradient: the downlink case,‚Äù S. Malik, A. Silva, J-M. Kelif. Proc. of the 5th International Conference on Performance Evaluation Methodologies and Tools (Valuetools) 2011, Cachan, France, May 16-20, 2011.\n‚ÄúSpatial games combining base station placement and mobile association: the downlink case,‚Äù A. Silva, H. Tembine, E. Altman, M. Debbah. Proc. of the 49th IEEE Conference on Decision and Control (CDC), Atlanta, GA, USA, December 15-17, 2010.\n‚ÄúUplink Spatial Games on Cellular Networks,‚Äù A. Silva, H. Tembine, M. Debbah, E. Altman. Proc. of the 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), Monticello, IL, USA, Sep.¬†29-Oct.¬†1, 2010.\n‚ÄúContinuum Equilibria and Global Optimization for Routing in Dense Static Ad Hoc Networks,‚Äù A. Silva, E. Altman, P. Bernhard, M. Debbah. Computer Networks, Volume 54, Issue 6, Pages 1005-1018, April 2010, ISSN: 1389-1286, DOI: 10.1016/j.comnet.2009.10.019.\n‚ÄúMagnetworks: how mobility impacts the design of mobile networks.‚Äù A. Silva, E. Altman, M. Debbah, G. Alfano. Proc. of the 29th IEEE INFOCOM 2010, San Diego, CA, USA, March 15-19, 2010.\n‚ÄúCongestion in Randomly Deployed Wireless Ad Hoc/Sensor Networks,‚Äù A. Silva, P. Reyes, M. Debbah. Proc. of International Conference on Ultra Modern Telecommunications (ICUMT), St-Petersburg, Russia, October 12-14, 2009.\n‚ÄúStochastic Games with One-Step Delay Sharing Information Pattern with Application to Power Control,‚Äù E. Altman, V. Kamble, A. Silva. Proc. of Gamenets 2009, Istanbul, Turkey, May 13-15, 2009.\n‚ÄúNumerical Solutions of Continuum Equilibria for Routing in Dense Ad Hoc Networks,‚Äù A. Silva, P. Bernhard, E. Altman. Proc. of the 3rd International Conference on Performance Evaluation Methodologies and Tools (Valuetools) 2008, Workshop Inter-Perf, Athens, Greece, October 20-24, 2008.\n‚ÄúThe Space Frontier: Physical Limits of Multiple Antenna Information Transfer,‚Äù R. Couillet, S. Wagner, M. Debbah, A. Silva. Proc. of the 3rd International Conference on Performance Evaluation Methodologies and Tools (Valuetools) 2008, Workshop Inter-Perf, Athens, Greece, October 20-24, 2008. Best Student Paper Award.\n‚ÄúThe Mathematics of Routing in Massively Dense Ad-Hoc Networks,‚Äù P. Bernhard, E. Altman, A. Silva. Proc. of AdHoc-NOW Conference, Sophia-Antipolis, France, September 10-13, 2008.\n‚ÄúContinuum Equilibria for Routing in Dense Ad-Hoc Networks,‚Äù E. Altman, A. Silva, P. Bernhard, M. Debbah. Proc. of 45th Annual Allerton Conference on Communication, Control, and Computing (Allerton), Monticello, IL, USA, September 26-28, 2007."
  },
  {
    "objectID": "til/posts/2025-06-16-Jupyter_Quiz/JupyterQuiz.html",
    "href": "til/posts/2025-06-16-Jupyter_Quiz/JupyterQuiz.html",
    "title": "How to do Quizzes with JupyterQuiz",
    "section": "",
    "text": "TIL how to do quizzes with JupyterQuiz. JupyterQuiz is a tool to make quizzes in jupyter notebooks. It is very easy to use and you can even use it with WebAssembly as I will show below:\nYou can install jupyterquiz as follows:\n\npipuvwasm\n\n\n\n\nTerminal\n\npython -m venv .venv\nsource .venv/bin/activate\npython -m pip install jupyterquiz\n\n\n\n\n\nTerminal\n\nuv venv\nsource .venv/bin/activate\nuv pip install jupyterquiz\n\n\n\n\n\nTerminal\n\n%pip install jupyterquiz\n\n\n\n\nThen you need to:\n\nProvide a question (in this example ‚ÄúWhat‚Äôs the capital of France?‚Äù),\nSpecify what‚Äôs the type of question you want to make (‚Äúmultiple_choice‚Äù, ‚Äúmany_choice‚Äù, etc.),\nand finally the possible answers and which one(s) are correct.\n\nThe expected format is as follows:\n\nexample = [\n    {\n        \"question\": \"What is the capital of France?\",\n        \"type\": \"multiple_choice\",\n        \"answers\": [\n            {\"answer\": \"London\", \"correct\": False},\n            {\"answer\": \"Paris\", \"correct\": True},\n            {\"answer\": \"New York\", \"correct\": False},\n            {\"answer\": \"Rome\", \"correct\": False},\n        ],\n    }\n]\n\nThen you can display the quiz:\n\nfrom jupyterquiz import display_quiz\n\ndisplay_quiz(example)\n\n\n\n\n\n\n\nYou can also try it here by running the following cell:\n\n\n\n        \n        \n\n\nYou can also store the questions in a json file (in this case I‚Äôm storing it here) and then display the quiz:\n\n\n\n        \n        \n\n\nNotice that we are not storing the results so this is only for people to get feedback when using these quizzes.\nI like this simple way to do quizzes and I‚Äôm using them to motivate my kid to learn new things."
  },
  {
    "objectID": "til/posts/2025-07-04-Chess/Chess.html",
    "href": "til/posts/2025-07-04-Chess/Chess.html",
    "title": "How to use the chess library",
    "section": "",
    "text": "TIL how to use the chess library. It is an excellent library that I used to create a HuggingFace dataset and it can also be used with WebAssembly.\nYou can install the chess library as follows:\n\npipuvwasm\n\n\n\n\nTerminal\n\npython -m venv .venv\nsource .venv/bin/activate\npython -m pip install chess\n\n\n\n\n\nTerminal\n\nuv venv\nsource .venv/bin/activate\nuv pip install chess\n\n\n\n\n\nTerminal\n\n%pip install chess\n\n\n\n\nThis is all the code needed to display the initial chess board:\n\nimport chess\n\nboard = chess.Board()\nboard\n\n\n\n\n\n\n\n\nThis is the code to use it here with WebAssembly:\n\n\n\n        \n        \n\n\nThis is the code to get the legal moves:\n\nlegal_moves_uci = list(board.legal_moves)[:5] # display only the first 5 legal moves\nlegal_moves_uci\n\n[Move.from_uci('g1h3'),\n Move.from_uci('g1f3'),\n Move.from_uci('b1c3'),\n Move.from_uci('b1a3'),\n Move.from_uci('h2h3')]\n\n\nWe can also get the legal moves in standard algebraic notation:\n\nlegal_moves_san = [chess.Board.san(board,move) for move in legal_moves_uci][:5] # display only the first 5 legal moves\nlegal_moves_san\n\n['Nh3', 'Nf3', 'Nc3', 'Na3', 'h3']\n\n\nWe can see whose turn is it as follows:\n\nwhose_turn = \"white\" if board.turn == chess.WHITE else \"black\"\nwhose_turn\n\n'white'\n\n\nWe can move a piece as follows:\n\nmove_uci = chess.Move.from_uci(\"f2f3\")\nif board.is_legal(move_uci):\n    board.push(move_uci)\nboard\n\n\n\n\n\n\n\n\nNow it is black‚Äôs turn:\n\nwhose_turn = \"white\" if board.turn == chess.WHITE else \"black\"\nwhose_turn\n\n'black'\n\n\nWe can also move it with standard algebraic notation:\n\nmove = \"e5\"\nmove_uci = board.parse_san(move)\nif board.is_legal(move_uci):\n    board.push_san(move)\nboard\n\n\n\n\n\n\n\n\nLet‚Äôs do two more moves because I want to show you something:\n\nmove_uci = chess.Move.from_uci(\"g2g4\")\nif board.is_legal(move_uci):\n    board.push(move_uci)\nmove_uci = chess.Move.from_uci(\"d8h4\")\nif board.is_legal(move_uci):\n    board.push(move_uci)\nboard\n\n\n\n\n\n\n\n\nWe can now check that this is a checkmate!\n\nboard.is_checkmate()\n\nTrue\n\n\nWe can also undo the previous move:\n\nboard.pop()\nboard"
  },
  {
    "objectID": "til/posts/2025-07-04-Chess/Chess.html#the-chess-library",
    "href": "til/posts/2025-07-04-Chess/Chess.html#the-chess-library",
    "title": "How to use the chess library",
    "section": "",
    "text": "TIL how to use the chess library. It is an excellent library that I used to create a HuggingFace dataset and it can also be used with WebAssembly.\nYou can install the chess library as follows:\n\npipuvwasm\n\n\n\n\nTerminal\n\npython -m venv .venv\nsource .venv/bin/activate\npython -m pip install chess\n\n\n\n\n\nTerminal\n\nuv venv\nsource .venv/bin/activate\nuv pip install chess\n\n\n\n\n\nTerminal\n\n%pip install chess\n\n\n\n\nThis is all the code needed to display the initial chess board:\n\nimport chess\n\nboard = chess.Board()\nboard\n\n\n\n\n\n\n\n\nThis is the code to use it here with WebAssembly:\n\n\n\n        \n        \n\n\nThis is the code to get the legal moves:\n\nlegal_moves_uci = list(board.legal_moves)[:5] # display only the first 5 legal moves\nlegal_moves_uci\n\n[Move.from_uci('g1h3'),\n Move.from_uci('g1f3'),\n Move.from_uci('b1c3'),\n Move.from_uci('b1a3'),\n Move.from_uci('h2h3')]\n\n\nWe can also get the legal moves in standard algebraic notation:\n\nlegal_moves_san = [chess.Board.san(board,move) for move in legal_moves_uci][:5] # display only the first 5 legal moves\nlegal_moves_san\n\n['Nh3', 'Nf3', 'Nc3', 'Na3', 'h3']\n\n\nWe can see whose turn is it as follows:\n\nwhose_turn = \"white\" if board.turn == chess.WHITE else \"black\"\nwhose_turn\n\n'white'\n\n\nWe can move a piece as follows:\n\nmove_uci = chess.Move.from_uci(\"f2f3\")\nif board.is_legal(move_uci):\n    board.push(move_uci)\nboard\n\n\n\n\n\n\n\n\nNow it is black‚Äôs turn:\n\nwhose_turn = \"white\" if board.turn == chess.WHITE else \"black\"\nwhose_turn\n\n'black'\n\n\nWe can also move it with standard algebraic notation:\n\nmove = \"e5\"\nmove_uci = board.parse_san(move)\nif board.is_legal(move_uci):\n    board.push_san(move)\nboard\n\n\n\n\n\n\n\n\nLet‚Äôs do two more moves because I want to show you something:\n\nmove_uci = chess.Move.from_uci(\"g2g4\")\nif board.is_legal(move_uci):\n    board.push(move_uci)\nmove_uci = chess.Move.from_uci(\"d8h4\")\nif board.is_legal(move_uci):\n    board.push(move_uci)\nboard\n\n\n\n\n\n\n\n\nWe can now check that this is a checkmate!\n\nboard.is_checkmate()\n\nTrue\n\n\nWe can also undo the previous move:\n\nboard.pop()\nboard"
  },
  {
    "objectID": "til/posts/2025-07-04-Chess/Chess.html#dataset-checkmate-in-one-big-bench",
    "href": "til/posts/2025-07-04-Chess/Chess.html#dataset-checkmate-in-one-big-bench",
    "title": "How to use the chess library",
    "section": "Dataset Checkmate In One BIG-Bench",
    "text": "Dataset Checkmate In One BIG-Bench\nGiven that we know how to do a text representation of the positions in the board, we can use the dataset from the BIG-Bench called Checkmate In One. This dataset already has a text representation. For example:\n1. d4 d5 2. Nf3 Nf6 3. e3 a6 4. Nc3 e6 5. Bd3 h6\n6. e4 dxe4 7. Bxe4 Nxe4 8. Nxe4 Bb4+ 9. c3 Ba5 10. Qa4+ Nc6\n11. Ne5 Qd5 12. f3 O-O 13. Nxc6 bxc6 14. Bf4 Ra7 15. Qb3 Qb5\n16. Qxb5 cxb5 17. a4 bxa4 18. Rxa4 Bb6 19. Kf2 Bd7 20. Ke3 Bxa4\n21. Ra1 Bc2 22. c4 Bxe4 23. fxe4 c5 24. d5 exd5 25. exd5 Re8+\n26. Kf3 Rae7 27. Rxa6 Bc7 28. Bd2 Re2 29. Bc3 R8e3+ 30. Kg4 Rxg2+\n31. Kf5\nwhere the checkmate in one is the move Rg5#.\nI wanted to provide an easier representation for small language models:\n\nmoves_str = \"\"\"\n1. d4 d5 2. Nf3 Nf6 3. e3 a6 4. Nc3 e6 5. Bd3 h6\n6. e4 dxe4 7. Bxe4 Nxe4 8. Nxe4 Bb4+ 9. c3 Ba5 10. Qa4+ Nc6\n11. Ne5 Qd5 12. f3 O-O 13. Nxc6 bxc6 14. Bf4 Ra7 15. Qb3 Qb5\n16. Qxb5 cxb5 17. a4 bxa4 18. Rxa4 Bb6 19. Kf2 Bd7 20. Ke3 Bxa4\n21. Ra1 Bc2 22. c4 Bxe4 23. fxe4 c5 24. d5 exd5 25. exd5 Re8+\n26. Kf3 Rae7 27. Rxa6 Bc7 28. Bd2 Re2 29. Bc3 R8e3+ 30. Kg4 Rxg2+\n31. Kf5\"\"\"\nmoves = [move for move in moves_str.split() if not move[0].isdigit()]\nboard = chess.Board()\nfor move in moves:\n    move_uci = board.parse_san(move)\n    if board.is_legal(move_uci):\n        board.push_san(move)\n    else:\n        print(\"Error: Invalid move\")\ndisplay(board)\nprint(create_board_str(board))\n\n\n\n\n\n\n\n\n\n    a    b    c    d    e    f    g    h  \n  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n8 ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ k  ‚îÇ    ‚îÇ 8\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n7 ‚îÇ    ‚îÇ    ‚îÇ b  ‚îÇ    ‚îÇ    ‚îÇ p  ‚îÇ p  ‚îÇ    ‚îÇ 7\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n6 ‚îÇ R  ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ p  ‚îÇ 6\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n5 ‚îÇ    ‚îÇ    ‚îÇ p  ‚îÇ P  ‚îÇ    ‚îÇ K  ‚îÇ    ‚îÇ    ‚îÇ 5\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n4 ‚îÇ    ‚îÇ    ‚îÇ P  ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 4\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n3 ‚îÇ    ‚îÇ    ‚îÇ B  ‚îÇ    ‚îÇ r  ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 3\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n2 ‚îÇ    ‚îÇ P  ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ r  ‚îÇ P  ‚îÇ 2\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n1 ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 1\n  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n    a    b    c    d    e    f    g    h\n\n\n\nI think this is a much simpler task (hopefully not too simple) for small language models to solve.\nGPT-4o failed when given this task:\n\nThe conversation can be found here: https://chatgpt.com/s/t_68683067ff7081919314e8573ec64d5d\nThe prompt was the following:\nYou are playing Black in a game of Chess. The initial state of the board was this:\n\n    a    b    c    d    e    f    g    h  \n  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n8 ‚îÇ r  ‚îÇ n  ‚îÇ b  ‚îÇ q  ‚îÇ k  ‚îÇ b  ‚îÇ n  ‚îÇ r  ‚îÇ 8\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n7 ‚îÇ p  ‚îÇ p  ‚îÇ p  ‚îÇ p  ‚îÇ p  ‚îÇ p  ‚îÇ p  ‚îÇ p  ‚îÇ 7\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n6 ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 6\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n5 ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 5\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n4 ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 4\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n3 ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 3\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n2 ‚îÇ P  ‚îÇ P  ‚îÇ P  ‚îÇ P  ‚îÇ P  ‚îÇ P  ‚îÇ P  ‚îÇ P  ‚îÇ 2\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n1 ‚îÇ R  ‚îÇ N  ‚îÇ B  ‚îÇ Q  ‚îÇ K  ‚îÇ B  ‚îÇ N  ‚îÇ R  ‚îÇ 1\n  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n    a    b    c    d    e    f    g    h\n\nThe current state of the board is this:\n\n    a    b    c    d    e    f    g    h  \n  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n8 ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ k  ‚îÇ    ‚îÇ 8\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n7 ‚îÇ    ‚îÇ    ‚îÇ b  ‚îÇ    ‚îÇ    ‚îÇ p  ‚îÇ p  ‚îÇ    ‚îÇ 7\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n6 ‚îÇ R  ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ p  ‚îÇ 6\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n5 ‚îÇ    ‚îÇ    ‚îÇ p  ‚îÇ P  ‚îÇ    ‚îÇ K  ‚îÇ    ‚îÇ    ‚îÇ 5\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n4 ‚îÇ    ‚îÇ    ‚îÇ P  ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 4\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n3 ‚îÇ    ‚îÇ    ‚îÇ B  ‚îÇ    ‚îÇ r  ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 3\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n2 ‚îÇ    ‚îÇ P  ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ r  ‚îÇ P  ‚îÇ 2\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n1 ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 1\n  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n    a    b    c    d    e    f    g    h\n\nMake your move in standard algebraic notation format enclosed in square brackets (e.g., [e4]).\nYou can also include additional text in your messages.\nIt's your turn. Find the checkmate in one.\nIf you want to use the Checkmate In One BIG-Bench with that text representation, it can be found here:\nhttps://hf.co/datasets/alonsosilva/chess_checkmate_in_one_big_bench\nThe future plan is to do Reinforcement Learning with Verifiable Rewards with that dataset. And the longer term plan is to do multi-turn reinforcement learning with that game. Similar efforts have already been done with the games Tic Tac Toe and 2048: https://github.com/OpenPipe/ART?tab=readme-ov-file"
  },
  {
    "objectID": "til/index.html",
    "href": "til/index.html",
    "title": "TIL",
    "section": "",
    "text": "How to use the chess library\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 4, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nHow to add marimo notebook cells to Quarto projects\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 29, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nHow to do Quizzes with JupyterQuiz\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 16, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nEmbed live REPL on a website\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nJun 15, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nQuickly explore and share a SQLite database with Datasette and Datasette Lite\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nNov 22, 2024\n\n\nAlonso Silva\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "til/posts/2025-06-29-Quarto-Marimo/marimo.html",
    "href": "til/posts/2025-06-29-Quarto-Marimo/marimo.html",
    "title": "How to add marimo notebook cells to Quarto projects",
    "section": "",
    "text": "TIL how to add marimo notebook cells to Quarto projects such as this one:"
  },
  {
    "objectID": "til/posts/2025-06-29-Quarto-Marimo/marimo.html#instructions",
    "href": "til/posts/2025-06-29-Quarto-Marimo/marimo.html#instructions",
    "title": "How to add marimo notebook cells to Quarto projects",
    "section": "Instructions",
    "text": "Instructions\n\nInstall uv (if you haven‚Äôt yet):\n\n\n\nTerminal\n\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n\nCreate a Quarto project (if you haven‚Äôt yet):\n\n\n\nTerminal\n\nuvx --from quarto-cli quarto create project\n\nHere are some example configuration options:\n\n\nEnter into the project folder and add the marimo extension:\n\n\n\nTerminal\n\nuvx --from quarto-cli quarto add marimo-team/quarto-marimo\n\n\n\nEdit the index.qmd file (this is an example):\n\n\n\nindex.qmd\n\n---\ntitle: \"test-quarto-marimo\"\nfilters:\n  - marimo-team/marimo\n---\n```python {.marimo}\nimport marimo as mo\n\nslider = mo.ui.slider(1, 30)\n```\n\n```python {.marimo}\nmo.md(\n    f\"\"\"\n    This is a **reactive** Python notebook that **runs automatically**\n    when you modify them or\n    interact with UI elements, like this slider: {slider}\n\n    {\"##\" + \"üçÉ\" * slider.value}\n    \"\"\"\n).callout(\"info\")\n\n\nPreview the project:\n\n\n\nTerminal\n\nuvx --with marimo --from quarto-cli quarto preview\n\n\nDeploy it (for example to GitHub pages), the command is the following:\n\n\n\nTerminal\n\nuvx --with marimo --from quarto-cli quarto publish\n\n\nThere is a nice video by the marimo team explaining the whole process:"
  },
  {
    "objectID": "til/posts/2025-06-29-Quarto-Marimo/marimo.html#conclusions",
    "href": "til/posts/2025-06-29-Quarto-Marimo/marimo.html#conclusions",
    "title": "How to add marimo notebook cells to Quarto projects",
    "section": "Conclusions",
    "text": "Conclusions\nIf you have read my previous post, you know I am very excited that we can now embed a single executable cell inside a website. You can also embed a whole notebook inside a website. However, I love the idea of adding many dependent executable cells to a website such as with this extension. The reactivity is definitely a nice plus!"
  },
  {
    "objectID": "til/posts/2024-11-21-Datasette/Datasette.html",
    "href": "til/posts/2024-11-21-Datasette/Datasette.html",
    "title": "Quickly explore and share a SQLite database with Datasette and Datasette Lite",
    "section": "",
    "text": "I only recently learned about Datasette. My interest came from the possibility to quickly explore a SQLite database and I am not disappointed. You can explore a SQLite database database.db as follows:\n\nuv (ephemeral venv)uv (persistent venv)pip\n\n\n\n\nTerminal\n\nuvx datasette database.db\n\n\n\n\n\nTerminal\n\nuv venv\nsource .venv/bin/activate\nuv pip install datasette\ndatasette database.db\n\n\n\n\n\nTerminal\n\npython -m venv .venv\nsource .venv/bin/activate\npython -m pip install datasette\ndatasette database.db\n\n\n\n\nThis will pop up the following in your browser:\n\nWe get a nice indication of the title of the dataset, how many rows, and how many tables there are, as well as links to explore the tables. For example, if I click on the link for the table responses I get all the rows and columns of that table:\n\nYou can also use Datasette Lite which runs in browser using WebAssembly and Pyodide to share your SQLite database and let others to explore it. For example, by putting a SQLite database in GitHub, you can upload it in https://lite.datasette.io/, and you get a link to explore it in the following link or directly below:"
  },
  {
    "objectID": "til/posts/2025-06-15-Embed_REPL/Embed_REPL.html",
    "href": "til/posts/2025-06-15-Embed_REPL/Embed_REPL.html",
    "title": "Embed live REPL on a website",
    "section": "",
    "text": "TIL that it‚Äôs possible to embed a live REPL on a website. This is possible thanks to JupyterLite.\nYou can write some code:\n\ncode_str = \"\"\"\nimport numpy as np\n\nnp.random.randint(10, size=5)\"\"\"\n\nYou can use urllib.parse.quote, which handles special characters as needed for URLs:\n\nimport urllib.parse\n\ncode = urllib.parse.quote(code_str)\n\nThen you can then use the public facing https://jupyterlite.github.io/demo/repl as an example and embed a live REPL on a website:\n\nfrom IPython.display import IFrame\n\nIFrame(f\"https://jupyterlite.github.io/demo/repl/index.html?toolbar=1&kernel=python&promptCellPosition=left&code={code}&execute=0\", width=850, height=200)\n\n\n        \n        \n\n\nThere are several configuration options.\nI find this very cool especially for writing tutorials and blog posts."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "A Void by Georges Perec\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Logits Processors\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 13, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Structured Outputs\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 11, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Function Calling\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 5, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding LLM Memory\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 28, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Chat Templates\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 20, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Tokenizers\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 17, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nBuild a basic widget with AnyWidget in a Solara app\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 23, 2024\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nBuild a basic LLM chat app with Solara\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nObservatorio de sueldos en Chile\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 21, 2023\n\n\nAlonso Silva\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2025-06-28-Understanding-Memory/Understanding_LLMs_Memory.html",
    "href": "blog/posts/2025-06-28-Understanding-Memory/Understanding_LLMs_Memory.html",
    "title": "Understanding LLM Memory",
    "section": "",
    "text": "Despite what some people think (even some researchers I‚Äôve met), language models don‚Äôt have any memory. The confusion comes, I suppose, from the fact that most people interact with models through some user interface like www.chatgpt.com, which handles the memory for them (incidentally, in some interesting ways as explained below).\nLet‚Äôs explore this further. Let‚Äôs use transformers_js_py which allows us to use language models in the browser.\n\n    \n    \n    \n    \n\nLet‚Äôs download a small model and its tokenizer (it takes a few minutes):\n\n    \n    \n    \n    \n\n\n    \n    \n    \n    \n\nWe can ask the question What's 2 + 2? and get the following response:\n\n    \n    \n    \n    \n\n\n    \n    \n    \n    \n\nIf we ask the model to add to the result 2 more, we get the following response:\n\n    \n    \n    \n    \n\n\n    \n    \n    \n    \n\nThe model doesn‚Äôt have any recollection of our previous conversation. Now, that‚Äôs completely expected since we haven‚Äôt provided the model any way to access that information."
  },
  {
    "objectID": "blog/posts/2025-06-28-Understanding-Memory/Understanding_LLMs_Memory.html#understanding-llm-memory",
    "href": "blog/posts/2025-06-28-Understanding-Memory/Understanding_LLMs_Memory.html#understanding-llm-memory",
    "title": "Understanding LLM Memory",
    "section": "",
    "text": "Despite what some people think (even some researchers I‚Äôve met), language models don‚Äôt have any memory. The confusion comes, I suppose, from the fact that most people interact with models through some user interface like www.chatgpt.com, which handles the memory for them (incidentally, in some interesting ways as explained below).\nLet‚Äôs explore this further. Let‚Äôs use transformers_js_py which allows us to use language models in the browser.\n\n    \n    \n    \n    \n\nLet‚Äôs download a small model and its tokenizer (it takes a few minutes):\n\n    \n    \n    \n    \n\n\n    \n    \n    \n    \n\nWe can ask the question What's 2 + 2? and get the following response:\n\n    \n    \n    \n    \n\n\n    \n    \n    \n    \n\nIf we ask the model to add to the result 2 more, we get the following response:\n\n    \n    \n    \n    \n\n\n    \n    \n    \n    \n\nThe model doesn‚Äôt have any recollection of our previous conversation. Now, that‚Äôs completely expected since we haven‚Äôt provided the model any way to access that information."
  },
  {
    "objectID": "blog/posts/2025-06-28-Understanding-Memory/Understanding_LLMs_Memory.html#handling-memory",
    "href": "blog/posts/2025-06-28-Understanding-Memory/Understanding_LLMs_Memory.html#handling-memory",
    "title": "Understanding LLM Memory",
    "section": "Handling Memory",
    "text": "Handling Memory\nThe simplest way to handle memory is to provide our previous conversation within a list of messages:\n\n    \n    \n    \n    \n\nWith all these messages we obtain the following response:\n\n    \n    \n    \n    \n\nThat works! The problem with that approach is that we need to be mindful of the context length of the model. We could for example store only the last 10 messages or so and perhaps the system message if there is one (in this example, we don‚Äôt have one).\nMore sophisticated approaches could be to store the messages and its responses in a vector database and retrieve the most closely related ones. Similarly, we could store the information as a graph in a graph database and retrieve the nodes and edges most closely related.\nChatGPT‚Äôs memory feature is very interesting because it uses memory as a tool. Let‚Äôs take a look at that.\nWe can define the tools as follows:\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"biography\",\n            \"description\": \"The biography tool allows you to persist user information across conversations. Use this tool and write whatever user information you want to remember. The information will appear in the model set context below in future conversations.\",\n            \"parameters\": {\n                \"properties\": {\n                    \"user_information\": {\n                        \"description\": \"Information from the user you want to remember across conversations.\",\n                        \"type\": \"string\",\n                    }\n                },\n                \"required\": [\"user_information\"],\n                \"type\": \"object\",\n            },\n        },\n    }\n]\n\n    \n    \n    \n    \n\n\n    \n    \n    \n    \n\nWe can then store the tool call as a text file and provide it in the context.\nI really like that idea. It‚Äôs very simple and clean!"
  },
  {
    "objectID": "blog/posts/2025-06-17-Understanding-Tokenizers/UnderstandingTokenizers.html",
    "href": "blog/posts/2025-06-17-Understanding-Tokenizers/UnderstandingTokenizers.html",
    "title": "Understanding Tokenizers",
    "section": "",
    "text": "This post is largely inspired by Understanding GPT tokenizers by Simon Willison.\nLarge Language Models don‚Äôt work with words, they work with tokens. They take text, convert it into tokens (integers), then predict which tokens should come next.\nTo explain this I will use the transformers_js_py library which allows us to work with LLMs in the browser through WebAssembly.\nLet‚Äôs consider a text we want to tokenize:\n\ntext = \"The dog eats the apples.\"\n\nEach LLM has its own tokenizer, so we need to specify which model we are going to use:\n\nmodel_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\nFinally, we can encode it with the model‚Äôs tokenizer by running the following cell:\n\n\n\n        \n        \n\n\nYou should see as output the following list of integers:\n\ntoken_ids = [785, 5562, 49677, 279, 40676, 13]\n\nThis list of integers correspond to the token ids.\nYou can encode other texts by running the following code above:\ntokenizer.encode(\"El perro come las manzanas.\")\nYou can also modify the model_id to see how the tokens change (search for other models in HuggingFace). For example:\n\n\n\n        \n        \n\n\nWith this model, you should see as output the following list of tokens:\n\ntoken_ids_Mistral_7B_v0_3 = [1, 1183, 4682, 1085, 2217, 1040, 1747, 3583, 29491]\n\nYou can observe that even if the text is the same, these tokens are very different from the previous ones:\n\ntoken_ids = [785, 5562, 49677, 279, 40676, 13]\n\nWe can do the reverse operation. We take the tokens and convert them to text:\n\n\n\n        \n        \n\n\nEncoding a text and then decoding it should give the same original text.\nPlaying with tokenizers reveal all sorts of interesting facts.\nMost common English words are assigned a single token. As demonstrated above:\n\n‚ÄúThe‚Äù: 785\n‚Äù dog‚Äù: 5562\n‚Äù eats‚Äù: 49677\n‚Äù the‚Äù: 279\n‚Äù apples‚Äù: 40676\n‚Äú.‚Äù: 13\n\nCapitalization is important: ‚ÄúThe‚Äù with a capital T corresponds to token 785, but ‚Äúthe‚Äù with lowercase is 1782 and ‚Äù the‚Äù with both a leading space and a lowercase t is token 279.\nMany words also have a token that incorporates a leading space. This makes for much more efficient encoding of full sentences, since they can be encoded without needing to spend a token on each whitespace character.\nNumbers get their own tokens:\n\n‚Äú0‚Äù: 15\n‚Äú1‚Äù: 16\n‚Äú2‚Äù: 17\n‚Ä¶\n‚Äú9‚Äù: 24\n\nLanguages other than English suffer from less efficient tokenization.\n‚ÄúEl perro come las manzanas‚Äù in Spanish is encoded like this:\n\n‚ÄúEl‚Äù: 6582\n‚Äù per‚Äù: 817\n‚Äúro‚Äù: 299\n‚Äù come‚Äù: 2525\n‚Äù las‚Äù: 5141\n‚Äù man‚Äù: 883\n‚Äúz‚Äù: 89\n‚Äù anas‚Äù: 25908\n‚Äú.‚Äù: 13\n\n‚ÄúLe chien mange les pommes‚Äù in French is encoded like this:\n\n‚ÄúLe‚Äù: 2304\n‚Äù ch‚Äù: 521\n‚Äúien‚Äù: 3591\n‚Äù mange‚Äù: 59434\n‚Äù les‚Äù: 3541\n‚Äù pom‚Äù: 29484\n‚Äù mes‚Äù: 8828\n‚Äú.‚Äù : 13\n\nThere are all sorts of interesting things like the glitch tokens.\nThe majority of tokenizers are trained with the byte-pair encoding algorithm.\nMany researchers think we should work with bytes and we shouldn‚Äôt have tokenizers to begin with and they are actively trying to remove them (without much success)."
  },
  {
    "objectID": "blog/posts/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara.html",
    "href": "blog/posts/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara.html",
    "title": "Build a basic LLM chat app with Solara",
    "section": "",
    "text": "In this post, we will build a basic LLM chat app with Solara. Large Language Models (LLMs) have become increasingly popular and Solara provides several components to work with them. Let‚Äôs dive in.\nFirst things first, let‚Äôs install Solara.\n$ pip install solara\nNow, let‚Äôs start by creating an app.py that sends a simple message with the content ‚ÄúHello!‚Äù as a user. To do that we use the ChatBox and ChatMessage components.\n\nimport solara\n@solara.component\ndef Page():\n    with solara.lab.ChatBox():\n        with solara.lab.ChatMessage(user=True, name=\"User\"):\n            solara.Markdown(\"Hello!\")\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\nYou can modify the user name and/or the message as you please.\n\nimport solara\n@solara.component\ndef Page():\n    with solara.lab.ChatBox():\n        with solara.lab.ChatMessage(user=True, name=\"Morpheus\"):\n            solara.Markdown(\"Wake up, Neo...\")\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\nYou can also send a message as an assistant.\n\nimport solara\n@solara.component\ndef Page():\n    with solara.lab.ChatBox():\n        with solara.lab.ChatMessage(user=False, name=\"Assistant\",):\n            solara.Markdown(\"Hello! How can I assist you today?\")\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\nTo have a conversation, we create a reactive variable messages where we will store the messages. To do that we create a list of dictionaries where we will save the roles (for example, user and assistant) and the messages contents.\n\nimport solara\nfrom typing import List\nfrom typing_extensions import TypedDict\n\nclass MessageDict(TypedDict):\n    role: str\n    content: str\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n\nWe can generate a conversation by adding messages to the reactive variable messages that we previously created and displaying each message one by one.\n\n@solara.component\ndef Page():\n    messages.value = [\n        {\"role\": \"user\", \"content\": \"Hello!\"}, \n        {\"role\": \"assistant\",  \"content\": \"Hello! How can I assist you today?\"},\n    ]\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"Assistant\"\n            ):\n                solara.Markdown(item[\"content\"])\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\nLet‚Äôs now add the possibility to receive messages from the user by adding the ChatInput component and a send function that adds the message to the conversation.\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"Assistant\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\nTry it out by sending a message.\n\n\n\n        \n        \n\n\n\n\nUp to now we are only displaying the message the user sent. Let‚Äôs first simulate a conversation by replying exactly the same message we receive from the user. To do that we need to add a response function and a result function that will reply the last message (which will be the one sent by the user) and it will be activated once every time the counter user_message_count changes.\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": message}]\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"EchoBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\nThe complete code can be found below.\n\n\nShow the code\nimport solara\nfrom typing import List\nfrom typing_extensions import TypedDict\n\nclass MessageDict(TypedDict):\n    role: str\n    content: str\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": message}]\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"EchoBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\n\nUp to now, our EchoBot application looks like this. Try it out!\n\n\n\n        \n        \n\n\n\n\n\nLet‚Äôs now build a Bot that will stream a response message. Let‚Äôs first emulate a streamed response with a function that we call response_generator.\n\n# Streamed response emulator\nimport time\nimport random\ndef response_generator():\n    response = random.choice(\n        [\n            \"Hello! How can I assist you today?\",\n            \"Hello! If you have any questions or need help with something, feel free to ask.\",\n        ]\n    )\n    for word in response.split():\n        yield word + \" \"\n        time.sleep(0.05)\n\nLet‚Äôs see that it‚Äôs working as expected.\n\nfor chunk in response_generator():\n    print(chunk)\n\nHello! \nHow \ncan \nI \nassist \nyou \ntoday? \n\n\nIt works. Notice that for the moment the response_generator function will give one of the two possible responses at random without considering the user message.\nLet‚Äôs now create a function that will be adding the chunks successively to the message.\n\ndef add_chunk_to_ai_message(chunk: str):\n    messages.value = [\n        *messages.value[:-1],\n        {\n            \"role\": \"assistant\",\n            \"content\": messages.value[-1][\"content\"] + chunk,\n        },\n    ]\n\nWe need to modify the EchoBot code to include this functionality as follows.\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator():\n            add_chunk_to_ai_message(chunk)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"StreamBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\nThe complete code can be found below.\n\n\nShow the code\nimport solara\nimport time\nimport random\nfrom typing import List\nfrom typing_extensions import TypedDict\n\nclass MessageDict(TypedDict):\n    role: str\n    content: str\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n\n# Streamed response emulator\ndef response_generator():\n    response = random.choice(\n        [\n            \"Hello! How can I assist you today?\",\n            \"Hello! If you have any questions or need help with something, feel free to ask.\",\n        ]\n    )\n    for word in response.split():\n        yield word + \" \"\n        time.sleep(0.05)\n\ndef add_chunk_to_ai_message(chunk: str):\n    messages.value = [\n        *messages.value[:-1],\n        {\n            \"role\": \"assistant\",\n            \"content\": messages.value[-1][\"content\"] + chunk,\n        },\n    ]\n    \nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator():\n            add_chunk_to_ai_message(chunk)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"StreamBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\n\nOur StreamBot application looks like this. Try it out!\n\n\n\n        \n        \n\n\n\n\n\nThe StreamBot application don‚Äôt take into account the user message. To reply something coherent, let‚Äôs use one of OpenAI models (in this example, gpt-3.5-turbo).\nFirst, obtain an OPENAI_API_KEY=sk-... and replace it below.\n\nimport os\nimport openai\nfrom openai import OpenAI\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\nclient = OpenAI()\n\nNow we can define a new response_generator function that will use OpenAI to give a coherent answer.\n\ndef response_generator(message):\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n           {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n           {\"role\": \"user\", \"content\": message}\n        ],\n        stream=True\n    )\n\nLet‚Äôs see that it works (as you can see in the code, we need to add some cleaning to the chunks and verify they are not None).\n\nfor chunk in response_generator(\"Hello!\"):\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content)\n\n\nHello\n!\n How\n can\n I\n assist\n you\n today\n?\n\n\nWe need to modify the StreamBot code as follows.\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator(message):\n            if chunk.choices[0].delta.content is not None:\n                add_chunk_to_ai_message(chunk.choices[0].delta.content)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"ChatGPT\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\nThe complete code can be found below.\n\n\nShow the code\nimport solara\nfrom typing import List\nfrom typing_extensions import TypedDict\nimport os\nimport openai\nfrom openai import OpenAI\n\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\nclient = OpenAI()\n\ndef response_generator(message):\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n           {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n           {\"role\": \"user\", \"content\": message}\n        ],\n        stream=True\n    )\n\nclass MessageDict(TypedDict):\n    role: str\n    content: str\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator(message):\n            if chunk.choices[0].delta.content is not None:\n                add_chunk_to_ai_message(chunk.choices[0].delta.content)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"ChatGPT\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()"
  },
  {
    "objectID": "blog/posts/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara.html#echobot",
    "href": "blog/posts/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara.html#echobot",
    "title": "Build a basic LLM chat app with Solara",
    "section": "",
    "text": "Up to now we are only displaying the message the user sent. Let‚Äôs first simulate a conversation by replying exactly the same message we receive from the user. To do that we need to add a response function and a result function that will reply the last message (which will be the one sent by the user) and it will be activated once every time the counter user_message_count changes.\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": message}]\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"EchoBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\nThe complete code can be found below.\n\n\nShow the code\nimport solara\nfrom typing import List\nfrom typing_extensions import TypedDict\n\nclass MessageDict(TypedDict):\n    role: str\n    content: str\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": message}]\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"EchoBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\n\nUp to now, our EchoBot application looks like this. Try it out!"
  },
  {
    "objectID": "blog/posts/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara.html#streambot",
    "href": "blog/posts/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara.html#streambot",
    "title": "Build a basic LLM chat app with Solara",
    "section": "",
    "text": "Let‚Äôs now build a Bot that will stream a response message. Let‚Äôs first emulate a streamed response with a function that we call response_generator.\n\n# Streamed response emulator\nimport time\nimport random\ndef response_generator():\n    response = random.choice(\n        [\n            \"Hello! How can I assist you today?\",\n            \"Hello! If you have any questions or need help with something, feel free to ask.\",\n        ]\n    )\n    for word in response.split():\n        yield word + \" \"\n        time.sleep(0.05)\n\nLet‚Äôs see that it‚Äôs working as expected.\n\nfor chunk in response_generator():\n    print(chunk)\n\nHello! \nHow \ncan \nI \nassist \nyou \ntoday? \n\n\nIt works. Notice that for the moment the response_generator function will give one of the two possible responses at random without considering the user message.\nLet‚Äôs now create a function that will be adding the chunks successively to the message.\n\ndef add_chunk_to_ai_message(chunk: str):\n    messages.value = [\n        *messages.value[:-1],\n        {\n            \"role\": \"assistant\",\n            \"content\": messages.value[-1][\"content\"] + chunk,\n        },\n    ]\n\nWe need to modify the EchoBot code to include this functionality as follows.\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator():\n            add_chunk_to_ai_message(chunk)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"StreamBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\nThe complete code can be found below.\n\n\nShow the code\nimport solara\nimport time\nimport random\nfrom typing import List\nfrom typing_extensions import TypedDict\n\nclass MessageDict(TypedDict):\n    role: str\n    content: str\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n\n# Streamed response emulator\ndef response_generator():\n    response = random.choice(\n        [\n            \"Hello! How can I assist you today?\",\n            \"Hello! If you have any questions or need help with something, feel free to ask.\",\n        ]\n    )\n    for word in response.split():\n        yield word + \" \"\n        time.sleep(0.05)\n\ndef add_chunk_to_ai_message(chunk: str):\n    messages.value = [\n        *messages.value[:-1],\n        {\n            \"role\": \"assistant\",\n            \"content\": messages.value[-1][\"content\"] + chunk,\n        },\n    ]\n    \nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator():\n            add_chunk_to_ai_message(chunk)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"StreamBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\n\nOur StreamBot application looks like this. Try it out!"
  },
  {
    "objectID": "blog/posts/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara.html#chatgpt-bot",
    "href": "blog/posts/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara.html#chatgpt-bot",
    "title": "Build a basic LLM chat app with Solara",
    "section": "",
    "text": "The StreamBot application don‚Äôt take into account the user message. To reply something coherent, let‚Äôs use one of OpenAI models (in this example, gpt-3.5-turbo).\nFirst, obtain an OPENAI_API_KEY=sk-... and replace it below.\n\nimport os\nimport openai\nfrom openai import OpenAI\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\nclient = OpenAI()\n\nNow we can define a new response_generator function that will use OpenAI to give a coherent answer.\n\ndef response_generator(message):\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n           {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n           {\"role\": \"user\", \"content\": message}\n        ],\n        stream=True\n    )\n\nLet‚Äôs see that it works (as you can see in the code, we need to add some cleaning to the chunks and verify they are not None).\n\nfor chunk in response_generator(\"Hello!\"):\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content)\n\n\nHello\n!\n How\n can\n I\n assist\n you\n today\n?\n\n\nWe need to modify the StreamBot code as follows.\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator(message):\n            if chunk.choices[0].delta.content is not None:\n                add_chunk_to_ai_message(chunk.choices[0].delta.content)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"ChatGPT\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\nThe complete code can be found below.\n\n\nShow the code\nimport solara\nfrom typing import List\nfrom typing_extensions import TypedDict\nimport os\nimport openai\nfrom openai import OpenAI\n\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\nclient = OpenAI()\n\ndef response_generator(message):\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n           {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n           {\"role\": \"user\", \"content\": message}\n        ],\n        stream=True\n    )\n\nclass MessageDict(TypedDict):\n    role: str\n    content: str\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator(message):\n            if chunk.choices[0].delta.content is not None:\n                add_chunk_to_ai_message(chunk.choices[0].delta.content)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"ChatGPT\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()"
  },
  {
    "objectID": "blog/posts/2023-04-21-Observatorio-de-sueldos-en-Chile/2023-04-21-Observatorio-de-sueldos-en-Chile.html",
    "href": "blog/posts/2023-04-21-Observatorio-de-sueldos-en-Chile/2023-04-21-Observatorio-de-sueldos-en-Chile.html",
    "title": "Observatorio de sueldos en Chile",
    "section": "",
    "text": "Sueldos en Chile\nLos datos de sueldos en Chile se pueden obtener de la encuesta suplementaria de ingresos (ESI). La √∫ltima ESI disponible es la del a√±o 2021 y se puede descargar aqu√≠.\nUna vez que hemos descargado los datos podemos acceder a ellos:\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n# download the data\ndata_raw = pd.read_csv(\"esi-2021---personas.csv\", encoding=\"ISO-8859-1\", low_memory=False, index_col=0)\ndata_raw.head(3)\n\n\n\n\n\n\n\n\n\nidrph\nnro_linea\nedad\ntramo_edad\nsexo\nparentesco\ncurso\nnivel\ntermino_nivel\nestudia_actual\n...\nmes_central\nano_encuesta\nmes_encuesta\nregion\nr_p_c\nestrato\ntipo\nconglomerado\nid_identificacion\nhogar\n\n\n\n\n1\n14565\n4\n36\n5.0\n1\n4\n2\n4\n2\n2\n...\n11\n2021\n10\n8\n8205\n8200111\n1\n820510025\n102542.0\n1\n\n\n2\n24946\n2\n57\n9.0\n2\n3\n2\n10\n1\n2\n...\n11\n2021\n12\n13\n13123\n13291\n1\n13123000144956\n69274.0\n1\n\n\n3\n41897\n2\n64\n10.0\n2\n2\n3\n8\n1\n2\n...\n11\n2021\n10\n16\n16302\n16300120\n3\n1630220015\n80521.0\n1\n\n\n\n\n3 rows √ó 292 columns\n\n\n\nPara el an√°lisis, la encuesta considera s√≥lo las personas ocupadas con m√°s de 1 mes en el empleo actual. Podemos acceder al Manual y Gu√≠a de Variables ESI y ver que la variable que nos interesa es ocup_ref donde el valor 1 corresponde a los ‚ÄúOcupados con m√°s de 1 mes en el empleo actual‚Äù.\n\n\nCode\ndata = data_raw[data_raw[\"ocup_ref\"] == 1].copy()\n\n\nDel Manual y Gu√≠a de Variables ESI, vemos que debemos considerar el factor de expansi√≥n que corresponde a la variable fact_cal_esi. De esta forma encontramos que el n√∫mero de personas ocupadas es 8.243.580.\n\n\nCode\nn_ocupados = data['fact_cal_esi'].sum()\nprint(f\"N√∫mero de personas ocupadas: {n_ocupados:,.0f}\".replace(',','.'))\n\n\nN√∫mero de personas ocupadas: 8.243.580\n\n\nPara calcular el sueldo promedio, seg√∫n el Manual y Gu√≠a de Variables ESI, debemos utilizar la variable ing_t_p que corresponde a ‚ÄúIngresos del trabajo principal‚Äù y debemos utilizar nuevamente el factor de expansi√≥n.\n\n\nCode\ndef ingreso_promedio(data):\n    n_ocupados = data['fact_cal_esi'].sum()\n    promedio = (data['ing_t_p']*data['fact_cal_esi']).sum()/n_ocupados\n    return int(np.round(promedio))\nprint(f\"Ingreso promedio mensual: ${ingreso_promedio(data):,.0f}\".replace(',','.'))\n\n\nIngreso promedio mensual: $681.039\n\n\nTanto el n√∫mero de personas ocupadas como el ingreso promedio coinciden con los valores entregados por el Instituto Nacional de Estad√≠sticas (INE) en la S√≠ntesis de Resultados (ver slide abajo) por lo que vamos por buen camino :-)\n\n\n\nSitio INE ingreso promedio\n\n\nPara calcular el sueldo promedio, diferenciando por hombre/mujer, debemos utilizar la variable sexo.\n\n\nCode\ndata['sexo'] = data['sexo'].map({1: 'hombre', 2: 'mujer'})\nocupados_hombres = data[data['sexo'] == 'hombre']\nocupadas_mujeres = data[data['sexo'] == 'mujer']\nprint(f\"Porcentaje de hombres: {100*ocupados_hombres['fact_cal_esi'].sum()/n_ocupados:.1f}%\")\nprint(f\"Ingreso promedio mensual para hombres: ${ingreso_promedio(ocupados_hombres):,.0f}\".replace(',','.'))\nprint(f\"Porcentaje de mujeres: {100*ocupadas_mujeres['fact_cal_esi'].sum()/n_ocupados:.1f}%\")\nprint(f\"Ingreso promedio mensual para mujeres: ${ingreso_promedio(ocupadas_mujeres):,.0f}\".replace(',','.'))\n\n\nPorcentaje de hombres: 58.2%\nIngreso promedio mensual para hombres: $749.046\nPorcentaje de mujeres: 41.8%\nIngreso promedio mensual para mujeres: $586.178\n\n\nNuevamente los valores coinciden con los valores entregados por el INE (ver slide arriba).\nPara calcular el sueldo promedio, diferenciando por regi√≥n, debemos utilizar la variable region:\n\n\nCode\nmap_regiones = {\n    1: \"Tarapac√°\",\n    2: \"Antofagasta\",\n    3: \"Atacama\",\n    4: \"Coquimbo\",\n    5: \"Valpara√≠so\",\n    6: \"O'Higgins\",\n    7: \"Maule\",\n    8: \"Biob√≠o\",\n    9: \"La Araucan√≠a\",\n    10: \"Los Lagos\",\n    11: \"Ays√©n\",\n    12: \"Magallanes\",\n    13: \"Metropolitana\",\n    14: \"Los R√≠os\",\n    15: \"Arica y Parinacota\",\n    16: \"√ëuble\",\n    99: \"Regi√≥n no identificada\"\n}\ndata[\"region\"] = data[\"region\"].map(map_regiones)\nocupados_regiones = data.groupby('region').apply(lambda x: ingreso_promedio(x))\nfor index, value in ocupados_regiones.items():\n    print(f\"Ingreso promedio en la Regi√≥n de {index}: ${value:,}\".replace(',','.'))\n\n\nIngreso promedio en la Regi√≥n de Antofagasta: $765.318\nIngreso promedio en la Regi√≥n de Arica y Parinacota: $582.646\nIngreso promedio en la Regi√≥n de Atacama: $649.946\nIngreso promedio en la Regi√≥n de Ays√©n: $748.998\nIngreso promedio en la Regi√≥n de Biob√≠o: $574.946\nIngreso promedio en la Regi√≥n de Coquimbo: $603.089\nIngreso promedio en la Regi√≥n de La Araucan√≠a: $533.858\nIngreso promedio en la Regi√≥n de Los Lagos: $552.445\nIngreso promedio en la Regi√≥n de Los R√≠os: $576.430\nIngreso promedio en la Regi√≥n de Magallanes: $844.329\nIngreso promedio en la Regi√≥n de Maule: $534.284\nIngreso promedio en la Regi√≥n de Metropolitana: $780.454\nIngreso promedio en la Regi√≥n de O'Higgins: $567.721\nIngreso promedio en la Regi√≥n de Tarapac√°: $672.109\nIngreso promedio en la Regi√≥n de Valpara√≠so: $601.402\nIngreso promedio en la Regi√≥n de √ëuble: $543.780\n\n\nLos valores coinciden con los entregados por el INE (ver slide abajo).\n\n\n\nSitio INE ingreso promedio por regiones\n\n\nPara calcular el ingreso mediano (la mitad de las personas ocupadas recibe ingresos menores o iguales al ingreso mediano) para la poblaci√≥n total, hombres y mujeres, y por regiones:\n\n\nCode\ndef ingreso_percentil(data, percentil):\n    n_ocupados = data['fact_cal_esi'].sum()\n    data_ordered = data.sort_values(by=\"ing_t_p\", ascending=True)\n    mediano = data_ordered[data_ordered[\"fact_cal_esi\"].cumsum()&gt;n_ocupados*percentil/100.]['ing_t_p'].iloc[0]\n    return int(np.round(mediano))\nprint(f\"Ingreso mediano mensual: ${ingreso_percentil(data,50):,.0f}\".replace(',','.'))\n\n\nIngreso mediano mensual: $457.690\n\n\n\n\nCode\nprint(f\"Ingreso mediano para hombres: ${ingreso_percentil(ocupados_hombres,50):,.0f}\".replace(',','.'))\nprint(f\"Ingreso mediano para mujeres: ${ingreso_percentil(ocupadas_mujeres,50):,.0f}\".replace(',','.'))\n\n\nIngreso mediano para hombres: $500.000\nIngreso mediano para mujeres: $405.348\n\n\n\n\nCode\nocupados_regiones = data.groupby('region').apply(lambda x: ingreso_percentil(x,50))\nfor index, value in ocupados_regiones.items():\n    print(f\"Ingreso mediano en la Regi√≥n de {index}: ${value:,}\".replace(',','.'))\n\n\nIngreso mediano en la Regi√≥n de Antofagasta: $570.000\nIngreso mediano en la Regi√≥n de Arica y Parinacota: $420.000\nIngreso mediano en la Regi√≥n de Atacama: $506.685\nIngreso mediano en la Regi√≥n de Ays√©n: $537.086\nIngreso mediano en la Regi√≥n de Biob√≠o: $427.307\nIngreso mediano en la Regi√≥n de Coquimbo: $405.348\nIngreso mediano en la Regi√≥n de La Araucan√≠a: $397.991\nIngreso mediano en la Regi√≥n de Los Lagos: $405.348\nIngreso mediano en la Regi√≥n de Los R√≠os: $420.000\nIngreso mediano en la Regi√≥n de Magallanes: $587.754\nIngreso mediano en la Regi√≥n de Maule: $400.000\nIngreso mediano en la Regi√≥n de Metropolitana: $500.000\nIngreso mediano en la Regi√≥n de O'Higgins: $400.000\nIngreso mediano en la Regi√≥n de Tarapac√°: $469.630\nIngreso mediano en la Regi√≥n de Valpara√≠so: $427.642\nIngreso mediano en la Regi√≥n de √ëuble: $400.000\n\n\nDichos valores coinciden con los valores entregados por el INE (ver slides abajo).\n\n\n\nSitio INE ingreso mediano\n\n\n\n\n\nSitio INE ingreso mediano regiones\n\n\nDe la misma forma como calculamos la mediana podemos calcular los sueldos para todos los percentiles.\n\n\nCode\nimport plotly.graph_objects as go\n\nPERCENTILES = [ingreso_percentil(data,p) for p in range(100)]\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=[.01*i for i in range(5,100)], y=PERCENTILES[5:], hovertemplate='Sueldo mensual: %{y:$,.0f}&lt;extra&gt;&lt;/extra&gt;'))\nfig.update_layout(\n    title = f'Distribuci√≥n de ingresos',\n    yaxis_title = 'Sueldos mensuales',\n    xaxis = dict(\n        tickmode = 'array',\n        tickvals = [.1*i for i in range(11)],\n        ticktext = [f'{10*i}%' for i in range(11)]\n    ),\n    xaxis_tickformat=',.0%',\n    yaxis_tickformat=',.0'.replace(',',','),\n    yaxis = dict(\n        tickmode = 'array',\n        tickvals = [500_000*i for i in range(9)],\n        ticktext = [f'${500_000*i:,}'.replace(',','.') for i in range(9)]\n    ),\n    showlegend=False\n)\nfig.update_layout(\n    hovermode=\"x\",\n    hoverlabel=dict(\n        bgcolor=\"white\",\n    )\n)\nfig.show()\n\n\n\n        \n        \n            \n            \n        \n\n\nPuede ver en qu√© percentil se encuentra su sueldo en esta aplicaci√≥n.\nDudas o sugerencias pueden hacerlas envi√°ndome un mensaje en Twitter: alonsosilva"
  },
  {
    "objectID": "blog/posts/2025-07-05-Understanding-Function-Calling/Understanding_Function_Calling.html",
    "href": "blog/posts/2025-07-05-Understanding-Function-Calling/Understanding_Function_Calling.html",
    "title": "Understanding Function Calling",
    "section": "",
    "text": "Language models take text as input and predict which text should come next. Taking that into consideration, what does function calling even mean?\nIn this post, I start by providing a basic example to motivate function calling, then I give a slightly more complex example by allowing a small language model to use Python. After that, I explain the conversational response as a tool trick. Finally, I explain how to do function calling in WebAssembly (optional but fun if you want to try function calling in the browser in this post itself)."
  },
  {
    "objectID": "blog/posts/2025-07-05-Understanding-Function-Calling/Understanding_Function_Calling.html#basic-example",
    "href": "blog/posts/2025-07-05-Understanding-Function-Calling/Understanding_Function_Calling.html#basic-example",
    "title": "Understanding Function Calling",
    "section": "Basic example",
    "text": "Basic example\nLet‚Äôs start with a basic example. Imagine that we ask a language model to perform the multiplication of 1234567 times 8765432 whose result is:\n\nprint(f\"{1234567*8765432:,}\")\n\n10,821,513,087,944\n\n\nThis is the answer of the language model:\n\n\nShow the code\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"Qwen/Qwen3-0.6B\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, cache_dir=\"/big_storage/llms/hf_models/\"\n).to(\"cuda\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What's 1234567 times 8765432?\"},\n]\n\nprompt = tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True, enable_thinking=False\n)\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs[\"input_ids\"].shape[1]\noutputs = model.generate(\n    **model_inputs,\n    max_new_tokens=1024,\n    do_sample=False,\n    temperature=1.0,\n    top_p=1.0,\n    top_k=50,\n)\nassistant_response = tokenizer.decode(outputs[0][prompt_length:])\nprint(assistant_response)\n\n\nTo find the product of **1234567 √ó 8765432**, we can use a calculator or a multiplication table. However, since this is a large number, it's best to use a calculator or a computational tool.\n\n### Final Answer:\n$$\n1234567 \\times 8765432 = \\boxed{1099999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999\n\n\nThis model is not particularly good but even GPT-4o-mini fails without using function calling:\n\nHere is the link to that conversation.\nAn analogy of function calling is that to ask a language model directly to perform a computation is similar to ask someone to make that computation in his head. It‚Äôs hard! However, if we gave him a calculator, he could solve it quite easily. He needs to know only two things:\n\nwhich operation (or function) to use\nwhich numbers (or arguments) to use\n\nThe same is true for doing function calling with a language model. It needs to know which function and which arguments to use.\nLet‚Äôs provide a description of a function to the language model so it knows what‚Äôs the function name and which are the arguments the function expects.\nYou can provide the description manually (after all it‚Äôs just text) but I will use Pydantic to do that. Here is the description of the multiply function\n\n\nShow the code\nfrom pydantic import BaseModel, Field\nimport json\nfrom openai import pydantic_function_tool\n\nclass multiply(BaseModel):\n    \"\"\"Multiply two integers together.\"\"\"\n\n    a: int = Field(..., description=\"First integer\")\n    b: int = Field(..., description=\"Second integer\")\n\ntool = pydantic_function_tool(multiply)\nprint(json.dumps(tool, indent=4))\n\n\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"multiply\",\n        \"strict\": true,\n        \"parameters\": {\n            \"description\": \"Multiply two integers together.\",\n            \"properties\": {\n                \"a\": {\n                    \"description\": \"First integer\",\n                    \"title\": \"A\",\n                    \"type\": \"integer\"\n                },\n                \"b\": {\n                    \"description\": \"Second integer\",\n                    \"title\": \"B\",\n                    \"type\": \"integer\"\n                }\n            },\n            \"required\": [\n                \"a\",\n                \"b\"\n            ],\n            \"title\": \"multiply\",\n            \"type\": \"object\",\n            \"additionalProperties\": false\n        },\n        \"description\": \"Multiply two integers together.\"\n    }\n}\n\n\nAfter we have provided a function/tool description, we can see if the model knows what to do with it:\n\n\nShow the code\nprompt = tokenizer.apply_chat_template(\n    messages, tools=[tool], tokenize=False, add_generation_prompt=True, enable_thinking=False\n)\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs[\"input_ids\"].shape[1]\noutputs = model.generate(\n    **model_inputs,\n    max_new_tokens=1024,\n    do_sample=False,\n    temperature=1.0,\n    top_p=1.0,\n    top_k=50,\n)\nassistant_response = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\nprint(assistant_response)\n\n\n&lt;tool_call&gt;\n{\"name\": \"multiply\", \"arguments\": {\"a\": 1234567, \"b\": 8765432}}\n&lt;/tool_call&gt;\n\n\nEven this small model is able to get correctly:\n\nthe function name (multiply)\nthe arguments to provide to the function (1234567 and 8765432)\n\nNotice that we have not yet implemented the function itself (!), which we might or might not do (as we will see in other examples next).\nWe can get the assistant response and simply do the multiplication ourselves. This is important, what we call function calling in reality is the model telling us which function to call and which arguments to provide to that function. It is up to us to run or not that function. In this case, we will run it:\n\n\nShow the code\nassistant_response_clean = assistant_response.split(\"&lt;tool_call&gt;\")[-1].split(\"&lt;/tool_call&gt;\")[0]\nassistant_response_json = json.loads(assistant_response_clean)\n\ndef execute_function_call(assistant_response_json):\n    if assistant_response_json['name'] == 'multiply':\n        tool_response = assistant_response_json['arguments']['a']*assistant_response_json['arguments']['b']\n    else:\n        tool_response = assistant_response_clean\n    return tool_response\n\ntool_response = execute_function_call(assistant_response_json)\nprint(f\"{tool_response:,}\")\n\n\n10,821,513,087,944\n\n\nThis is great. We already got the correct answer! However, we usually want to have a more ‚Äúhuman‚Äù response. To do that we can append two messages (a message for the function call and another for the tool response:\n\n\nShow the code\nmessages.append({\n    \"role\": \"assistant\",\n    \"content\": \"\",\n    \"function_call\": None,\n    \"tool_calls\": [{\n        \"name\": assistant_response_json[\"name\"],\n        \"arguments\": assistant_response_json[\"arguments\"],\n    }],\n})\n\nmessages.append(\n    {\n        \"role\": \"tool\",\n        \"content\": f\"{tool_response}\",\n    }\n)\n\n\nSince we have full control in this setting, we can actually see what‚Äôs the text the language model is receiving as input:\n\n\nShow the code\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tools=[tool],\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False,\n)\nprint(prompt)\n\n\n&lt;|im_start|&gt;system\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags:\n&lt;tools&gt;\n{\"type\": \"function\", \"function\": {\"name\": \"multiply\", \"strict\": true, \"parameters\": {\"description\": \"Multiply two integers together.\", \"properties\": {\"a\": {\"description\": \"First integer\", \"title\": \"A\", \"type\": \"integer\"}, \"b\": {\"description\": \"Second integer\", \"title\": \"B\", \"type\": \"integer\"}}, \"required\": [\"a\", \"b\"], \"title\": \"multiply\", \"type\": \"object\", \"additionalProperties\": false}, \"description\": \"Multiply two integers together.\"}}\n&lt;/tools&gt;\n\nFor each function call, return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags:\n&lt;tool_call&gt;\n{\"name\": &lt;function-name&gt;, \"arguments\": &lt;args-json-object&gt;}\n&lt;/tool_call&gt;&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nWhat's 1234567 times 8765432?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;tool_call&gt;\n{\"name\": \"multiply\", \"arguments\": {\"a\": 1234567, \"b\": 8765432}}\n&lt;/tool_call&gt;&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n&lt;tool_response&gt;\n10821513087944\n&lt;/tool_response&gt;&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;think&gt;\n\n&lt;/think&gt;\n\n\n\n\nWe can then make a call to the model to provide a ‚Äúhuman‚Äù response:\n\n\nShow the code\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs[\"input_ids\"].shape[1]\noutputs = model.generate(\n    **model_inputs,\n    max_new_tokens=1024,\n    do_sample=False,\n    temperature=1.0,\n    top_p=1.0,\n    top_k=50,\n)\nassistant_response = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\nprint(assistant_response)\n\n\n1234567 √ó 8765432 = 10821513087944\n\n\nOK, this response is correct and more ‚Äúhuman‚Äù. It could be improved but that‚Äôs because it‚Äôs a very small model.\nI hope you realize that even a small language model (in this example, with 0.6B parameters) provided with a function/tool can answer better that question than a powerful model such as GPT-4o-mini.\nThat‚Äôs powerful!"
  },
  {
    "objectID": "blog/posts/2025-07-05-Understanding-Function-Calling/Understanding_Function_Calling.html#language-model-using-python",
    "href": "blog/posts/2025-07-05-Understanding-Function-Calling/Understanding_Function_Calling.html#language-model-using-python",
    "title": "Understanding Function Calling",
    "section": "Language Model Using Python",
    "text": "Language Model Using Python\nIn our basic example, we used a very tailored function because I wanted to show that function calling can use several arguments. We can however provided with a Python REPL which will allow us to use a pletora of tools in Python. Here is a description of a Python REPL:\n\n\nShow the code\nclass Python_REPL(BaseModel):\n    \"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\"\n\n    python_code: str = Field(..., description=\"Valid python command.\")\n\ntool = pydantic_function_tool(Python_REPL)\nprint(json.dumps(tool, indent=4))\n\n\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"Python_REPL\",\n        \"strict\": true,\n        \"parameters\": {\n            \"description\": \"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n            \"properties\": {\n                \"python_code\": {\n                    \"description\": \"Valid python command.\",\n                    \"title\": \"Python Code\",\n                    \"type\": \"string\"\n                }\n            },\n            \"required\": [\n                \"python_code\"\n            ],\n            \"title\": \"Python_REPL\",\n            \"type\": \"object\",\n            \"additionalProperties\": false\n        },\n        \"description\": \"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\"\n    }\n}\n\n\nWe can ask our model to, for example, make a bar plot. In this case, the language model couldn‚Äôt figure out what to do:\n\n\nShow the code\nmessages = [\n    {\"role\": \"user\", \"content\": \"Make a bar plot\"},\n]\n\nprompt = tokenizer.apply_chat_template(\n    messages, tools=[tool], tokenize=False, add_generation_prompt=True, enable_thinking=False\n)\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs[\"input_ids\"].shape[1]\noutputs = model.generate(\n    **model_inputs,\n    max_new_tokens=1024,\n    do_sample=False,\n    temperature=1.0,\n    top_p=1.0,\n    top_k=50,\n)\nassistant_response = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\nprint(assistant_response)\n\n\nI cannot make a bar plot directly, but I can help you create one using Python. Could you please provide the data you want to plot?\n\n\nThe model was unable to call the tool but we can help it by adding it directly to the prompt (notice that this is equivalent to add_generation_prompt=True when we apply the chat template). This is probably what OpenAI does with the option tool_choice=required and we could even impose which tool to call which is probably equivalent to the forced function option that OpenAI provides). Let‚Äôs add the tool call directly to the prompt:\n\n\nShow the code\nprompt += \"&lt;tool_call&gt;\" # added directly to the prompt\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs[\"input_ids\"].shape[1]\noutputs = model.generate(\n    **model_inputs,\n    max_new_tokens=1024,\n    do_sample=False,\n    temperature=1.0,\n    top_p=1.0,\n    top_k=50,\n)\nassistant_response = \"&lt;tool_call&gt;\" + tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True) # it was added to the prompt\nprint(assistant_response)\n\n\n&lt;tool_call&gt;\n{\"name\": \"Python_REPL\", \"arguments\": {\"python_code\": \"import matplotlib.pyplot as plt\\nplt.bar([1, 2, 3], [10, 20, 30])\\nplt.show()\"}}\n&lt;/tool_call&gt;\n\n\nThis small model was able to provide the code to do a bar plot. It is up to us to run or not that code. In this case, we will run it:\n\n\nShow the code\nassistant_response_clean = assistant_response.split(\"&lt;tool_call&gt;\")[-1].split(\"&lt;/tool_call&gt;\")[0]\nassistant_response_json = json.loads(assistant_response_clean)\n\ndef execute_function_call(assistant_response_json):\n    if assistant_response_json['name'] == 'Python_REPL':\n        tool_response = exec(assistant_response_json['arguments']['python_code'])\n    else:\n        tool_response = assistant_response_json\n    return tool_response\n\nexecute_function_call(assistant_response_json)\n\n\n\n\n\n\n\n\n\nBy imposing that the language model needs to call a tool, we cannot let the model to respond to a conversational question. For example to tell us a joke:\n\n\nShow the code\nmessages = [\n    {\"role\": \"user\", \"content\": \"Tell me a joke\"},\n]\n\nprompt = tokenizer.apply_chat_template(\n    messages, tools=[tool], tokenize=False, add_generation_prompt=True, enable_thinking=False\n)\nprompt += \"&lt;tool_call&gt;\" # imposed tool call\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs[\"input_ids\"].shape[1]\noutputs = model.generate(\n    **model_inputs,\n    max_new_tokens=1024,\n    do_sample=False,\n    temperature=1.0,\n    top_p=1.0,\n    top_k=50,\n)\nassistant_response = \"&lt;tool_call&gt;\" + tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\nprint(assistant_response)\n\n\n&lt;tool_call&gt;\n{\"name\": \"Python_REPL\", \"arguments\": {\"python_code\": \"print('Hello, world!')\"}}\n&lt;/tool_call&gt;\n\n\nWe could use a trick. We could provide a ‚Äútool‚Äù to get back that behavior. Let‚Äôs give a description of that ‚Äútool‚Äù:\n\n\nShow the code\nclass ConversationalResponse(BaseModel):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: str = Field(description=\"A conversational response to the user's query\")\ntool_conversational_response = pydantic_function_tool(ConversationalResponse)\nprint(json.dumps(tool_conversational_response, indent=4))\n\n\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"ConversationalResponse\",\n        \"strict\": true,\n        \"parameters\": {\n            \"description\": \"Respond in a conversational manner. Be kind and helpful.\",\n            \"properties\": {\n                \"response\": {\n                    \"description\": \"A conversational response to the user's query\",\n                    \"title\": \"Response\",\n                    \"type\": \"string\"\n                }\n            },\n            \"required\": [\n                \"response\"\n            ],\n            \"title\": \"ConversationalResponse\",\n            \"type\": \"object\",\n            \"additionalProperties\": false\n        },\n        \"description\": \"Respond in a conversational manner. Be kind and helpful.\"\n    }\n}\n\n\n\n\nShow the code\nmessages = [\n    {\"role\": \"user\", \"content\": \"Tell me a joke\"},\n]\n\nprompt = tokenizer.apply_chat_template(\n    messages, tools=[tool, tool_conversational_response], tokenize=False, add_generation_prompt=True, enable_thinking=False\n)\nprompt += \"&lt;tool_call&gt;\" # imposed tool call\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs[\"input_ids\"].shape[1]\noutputs = model.generate(\n    **model_inputs,\n    max_new_tokens=1024,\n    do_sample=False,\n    temperature=1.0,\n    top_p=1.0,\n    top_k=50,\n)\nassistant_response = \"&lt;tool_call&gt;\" + tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\nprint(assistant_response)\n\n\n&lt;tool_call&gt;\n{\"name\": \"ConversationalResponse\", \"arguments\": {\"response\": \"Here's a joke for you: A man in a hat with a hat on it, a hat with a hat on it, and a hat with a hat on it... It's a hat with a hat on it!\"}}\n&lt;/tool_call&gt;\n\n\n\nassistant_response_clean = assistant_response.split(\"&lt;tool_call&gt;\")[-1].split(\"&lt;/tool_call&gt;\")[0]\nassistant_response_json = json.loads(assistant_response_clean)\n\ndef execute_function_call(assistant_response_json):\n    if assistant_response_json['name'] == 'Python_REPL':\n        tool_response = exec(assistant_response_json['arguments']['python_code'])\n    elif assistant_response_json['name'] == 'ConversationalResponse':\n        tool_response = assistant_response_json['arguments']['response']\n    else:\n        tool_response = assistant_response_json\n    return tool_response\n\nexecute_function_call(assistant_response_json)\n\n\"Here's a joke for you: A man in a hat with a hat on it, a hat with a hat on it, and a hat with a hat on it... It's a hat with a hat on it!\"\n\n\nNot clear that it‚Äôs a good joke but we got back the behavior we were expecting."
  },
  {
    "objectID": "blog/posts/2025-07-05-Understanding-Function-Calling/Understanding_Function_Calling.html#webassembly",
    "href": "blog/posts/2025-07-05-Understanding-Function-Calling/Understanding_Function_Calling.html#webassembly",
    "title": "Understanding Function Calling",
    "section": "WebAssembly",
    "text": "WebAssembly\nWe can run the previous code in the browser thanks to WebAssembly!\nWe first need to install some packages:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can download a quantized version of the model we used (not everything will work but most of it will). This step should take a few minutes the first time (later it should be stored in the cache):\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can send a prompt:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can define the tools and tokenize the prompt:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis is the generation part and it can take one or two minutes:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis is the generated code:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can execute the code:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFeel free to modify the code and/or prompts and run them in the browser!"
  }
]