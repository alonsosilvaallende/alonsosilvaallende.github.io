[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alonso Silva‚Äôs Homepage",
    "section": "",
    "text": "Hi üëã My name is Alonso Silva and I‚Äôm a Senior Researcher on Generative AI at Nokia Bell Labs.\nI received my Ph.D.¬†in Physics from the √âcole Sup√©rieure d‚Äô√âlectricit√© (Sup√©lec) in 2010. I did my Ph.D.¬†at INRIA Sophia-Antipolis under the direction of Professor Eitan Altman. Prior to my Ph.D., I received my Mathematical Engineering degree from the Department of Mathematical Engineering (DIM) at the Universidad de Chile in 2006. I have previously worked as a Researcher at Safran from September 2018 to September 2022, a Researcher at Bell Labs from September 2012 to August 2018, Postdoctoral Researcher in the Department of EECS at the University of California, Berkeley, from September 2011 to August 2012 at INRIA Paris Rocquencourt from September 2010 to August 2011 and as Reseach Consultant/Intern at Bell Labs Headquarters in New Jersey from March to May 2010. I have received the Best Paper Award at IEEE SmartGridComm‚Äô17, UNet‚Äô17, the Second Best Paper Award at MSN‚Äô11 and the Best Student Paper Award at Valuetools‚Äô08."
  },
  {
    "objectID": "index.html#short-bio",
    "href": "index.html#short-bio",
    "title": "Alonso Silva‚Äôs Homepage",
    "section": "",
    "text": "Hi üëã My name is Alonso Silva and I‚Äôm a Senior Researcher on Generative AI at Nokia Bell Labs.\nI received my Ph.D.¬†in Physics from the √âcole Sup√©rieure d‚Äô√âlectricit√© (Sup√©lec) in 2010. I did my Ph.D.¬†at INRIA Sophia-Antipolis under the direction of Professor Eitan Altman. Prior to my Ph.D., I received my Mathematical Engineering degree from the Department of Mathematical Engineering (DIM) at the Universidad de Chile in 2006. I have previously worked as a Researcher at Safran from September 2018 to September 2022, a Researcher at Bell Labs from September 2012 to August 2018, Postdoctoral Researcher in the Department of EECS at the University of California, Berkeley, from September 2011 to August 2012 at INRIA Paris Rocquencourt from September 2010 to August 2011 and as Reseach Consultant/Intern at Bell Labs Headquarters in New Jersey from March to May 2010. I have received the Best Paper Award at IEEE SmartGridComm‚Äô17, UNet‚Äô17, the Second Best Paper Award at MSN‚Äô11 and the Best Student Paper Award at Valuetools‚Äô08."
  },
  {
    "objectID": "index.html#recent-talks",
    "href": "index.html#recent-talks",
    "title": "Alonso Silva‚Äôs Homepage",
    "section": "Recent talks",
    "text": "Recent talks\n\nBuilding Knowledge Graph-Based Agents with Structured Text Generation and Open-Weights Models, A. Silva, PyData Global, December 3-5, 2024 [slides] [code] \nEnhancing RAG-based apps by constructing and leveraging knowledge graphs with open-weights LLMs, A. Silva, PyData Paris, September 25-26, 2024 [slides] [code] \nHow to create reproducible notebooks (uv+marimo), A. Silva, Presentation at LINCS, November 8, 2024 [slides] [code] \n‚ÄúLarge Language Models Playing Mixed Strategy Nash Equilibrium Games,‚Äù A. Silva, NETGCOOP, Oct 9-11, 2024 [paper] [slides][code] \n‚ÄúLarge Language Models Playing Mixed Strategy Nash Equilibrium Games,‚Äù A. Silva, LINCS Annual Workshop, June 27, 2024 [paper] [slides][code] \nHow to create pure Python web apps (solara+anywidget) A. Silva, Presentation at LINCS, May 22, 2024 [slides][code] \nKeynote: Catching up on the weird world of LLMs, A. Silva, PyCon Chile, November 24-26, 2023 [slides] [code] \nWhen we cease to understand the world, A. Silva, Intervention at the European Parliament, November 16, 2023. \nKeynote: ‚ÄúImproving Retrieval Augmented Generation (RAG) with Hybrid RAG,‚Äù A. Silva, Summer school in computational intelligence (EVIC), December 11-13, 2024.\nKeynote: ‚ÄúImproving Retrieval Augmented Generation (RAG) with Hybrid RAG,‚Äù A. Silva, IADevs, Dec 9, 2024.\n‚ÄúImprove (almost) any retrieval augmented generation (RAG) with hybrid RAG,‚Äù A. Silva, PyCon Chile, November 30-December 1, 2024."
  },
  {
    "objectID": "blog/posts/2023-04-21-Observatorio-de-sueldos-en-Chile/2023-04-21-Observatorio-de-sueldos-en-Chile.html",
    "href": "blog/posts/2023-04-21-Observatorio-de-sueldos-en-Chile/2023-04-21-Observatorio-de-sueldos-en-Chile.html",
    "title": "Observatorio de sueldos en Chile",
    "section": "",
    "text": "Sueldos en Chile\nLos datos de sueldos en Chile se pueden obtener de la encuesta suplementaria de ingresos (ESI). La √∫ltima ESI disponible es la del a√±o 2021 y se puede descargar aqu√≠.\nUna vez que hemos descargado los datos podemos acceder a ellos:\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n# download the data\ndata_raw = pd.read_csv(\"esi-2021---personas.csv\", encoding=\"ISO-8859-1\", low_memory=False, index_col=0)\ndata_raw.head(3)\n\n\n\n\n\n\n\n\n\nidrph\nnro_linea\nedad\ntramo_edad\nsexo\nparentesco\ncurso\nnivel\ntermino_nivel\nestudia_actual\n...\nmes_central\nano_encuesta\nmes_encuesta\nregion\nr_p_c\nestrato\ntipo\nconglomerado\nid_identificacion\nhogar\n\n\n\n\n1\n14565\n4\n36\n5.0\n1\n4\n2\n4\n2\n2\n...\n11\n2021\n10\n8\n8205\n8200111\n1\n820510025\n102542.0\n1\n\n\n2\n24946\n2\n57\n9.0\n2\n3\n2\n10\n1\n2\n...\n11\n2021\n12\n13\n13123\n13291\n1\n13123000144956\n69274.0\n1\n\n\n3\n41897\n2\n64\n10.0\n2\n2\n3\n8\n1\n2\n...\n11\n2021\n10\n16\n16302\n16300120\n3\n1630220015\n80521.0\n1\n\n\n\n\n3 rows √ó 292 columns\n\n\n\nPara el an√°lisis, la encuesta considera s√≥lo las personas ocupadas con m√°s de 1 mes en el empleo actual. Podemos acceder al Manual y Gu√≠a de Variables ESI y ver que la variable que nos interesa es ocup_ref donde el valor 1 corresponde a los ‚ÄúOcupados con m√°s de 1 mes en el empleo actual‚Äù.\n\n\nCode\ndata = data_raw[data_raw[\"ocup_ref\"] == 1].copy()\n\n\nDel Manual y Gu√≠a de Variables ESI, vemos que debemos considerar el factor de expansi√≥n que corresponde a la variable fact_cal_esi. De esta forma encontramos que el n√∫mero de personas ocupadas es 8.243.580.\n\n\nCode\nn_ocupados = data['fact_cal_esi'].sum()\nprint(f\"N√∫mero de personas ocupadas: {n_ocupados:,.0f}\".replace(',','.'))\n\n\nN√∫mero de personas ocupadas: 8.243.580\n\n\nPara calcular el sueldo promedio, seg√∫n el Manual y Gu√≠a de Variables ESI, debemos utilizar la variable ing_t_p que corresponde a ‚ÄúIngresos del trabajo principal‚Äù y debemos utilizar nuevamente el factor de expansi√≥n.\n\n\nCode\ndef ingreso_promedio(data):\n    n_ocupados = data['fact_cal_esi'].sum()\n    promedio = (data['ing_t_p']*data['fact_cal_esi']).sum()/n_ocupados\n    return int(np.round(promedio))\nprint(f\"Ingreso promedio mensual: ${ingreso_promedio(data):,.0f}\".replace(',','.'))\n\n\nIngreso promedio mensual: $681.039\n\n\nTanto el n√∫mero de personas ocupadas como el ingreso promedio coinciden con los valores entregados por el Instituto Nacional de Estad√≠sticas (INE) en la S√≠ntesis de Resultados (ver slide abajo) por lo que vamos por buen camino :-)\n\n\n\nSitio INE ingreso promedio\n\n\nPara calcular el sueldo promedio, diferenciando por hombre/mujer, debemos utilizar la variable sexo.\n\n\nCode\ndata['sexo'] = data['sexo'].map({1: 'hombre', 2: 'mujer'})\nocupados_hombres = data[data['sexo'] == 'hombre']\nocupadas_mujeres = data[data['sexo'] == 'mujer']\nprint(f\"Porcentaje de hombres: {100*ocupados_hombres['fact_cal_esi'].sum()/n_ocupados:.1f}%\")\nprint(f\"Ingreso promedio mensual para hombres: ${ingreso_promedio(ocupados_hombres):,.0f}\".replace(',','.'))\nprint(f\"Porcentaje de mujeres: {100*ocupadas_mujeres['fact_cal_esi'].sum()/n_ocupados:.1f}%\")\nprint(f\"Ingreso promedio mensual para mujeres: ${ingreso_promedio(ocupadas_mujeres):,.0f}\".replace(',','.'))\n\n\nPorcentaje de hombres: 58.2%\nIngreso promedio mensual para hombres: $749.046\nPorcentaje de mujeres: 41.8%\nIngreso promedio mensual para mujeres: $586.178\n\n\nNuevamente los valores coinciden con los valores entregados por el INE (ver slide arriba).\nPara calcular el sueldo promedio, diferenciando por regi√≥n, debemos utilizar la variable region:\n\n\nCode\nmap_regiones = {\n    1: \"Tarapac√°\",\n    2: \"Antofagasta\",\n    3: \"Atacama\",\n    4: \"Coquimbo\",\n    5: \"Valpara√≠so\",\n    6: \"O'Higgins\",\n    7: \"Maule\",\n    8: \"Biob√≠o\",\n    9: \"La Araucan√≠a\",\n    10: \"Los Lagos\",\n    11: \"Ays√©n\",\n    12: \"Magallanes\",\n    13: \"Metropolitana\",\n    14: \"Los R√≠os\",\n    15: \"Arica y Parinacota\",\n    16: \"√ëuble\",\n    99: \"Regi√≥n no identificada\"\n}\ndata[\"region\"] = data[\"region\"].map(map_regiones)\nocupados_regiones = data.groupby('region').apply(lambda x: ingreso_promedio(x))\nfor index, value in ocupados_regiones.items():\n    print(f\"Ingreso promedio en la Regi√≥n de {index}: ${value:,}\".replace(',','.'))\n\n\nIngreso promedio en la Regi√≥n de Antofagasta: $765.318\nIngreso promedio en la Regi√≥n de Arica y Parinacota: $582.646\nIngreso promedio en la Regi√≥n de Atacama: $649.946\nIngreso promedio en la Regi√≥n de Ays√©n: $748.998\nIngreso promedio en la Regi√≥n de Biob√≠o: $574.946\nIngreso promedio en la Regi√≥n de Coquimbo: $603.089\nIngreso promedio en la Regi√≥n de La Araucan√≠a: $533.858\nIngreso promedio en la Regi√≥n de Los Lagos: $552.445\nIngreso promedio en la Regi√≥n de Los R√≠os: $576.430\nIngreso promedio en la Regi√≥n de Magallanes: $844.329\nIngreso promedio en la Regi√≥n de Maule: $534.284\nIngreso promedio en la Regi√≥n de Metropolitana: $780.454\nIngreso promedio en la Regi√≥n de O'Higgins: $567.721\nIngreso promedio en la Regi√≥n de Tarapac√°: $672.109\nIngreso promedio en la Regi√≥n de Valpara√≠so: $601.402\nIngreso promedio en la Regi√≥n de √ëuble: $543.780\n\n\nLos valores coinciden con los entregados por el INE (ver slide abajo).\n\n\n\nSitio INE ingreso promedio por regiones\n\n\nPara calcular el ingreso mediano (la mitad de las personas ocupadas recibe ingresos menores o iguales al ingreso mediano) para la poblaci√≥n total, hombres y mujeres, y por regiones:\n\n\nCode\ndef ingreso_percentil(data, percentil):\n    n_ocupados = data['fact_cal_esi'].sum()\n    data_ordered = data.sort_values(by=\"ing_t_p\", ascending=True)\n    mediano = data_ordered[data_ordered[\"fact_cal_esi\"].cumsum()&gt;n_ocupados*percentil/100.]['ing_t_p'].iloc[0]\n    return int(np.round(mediano))\nprint(f\"Ingreso mediano mensual: ${ingreso_percentil(data,50):,.0f}\".replace(',','.'))\n\n\nIngreso mediano mensual: $457.690\n\n\n\n\nCode\nprint(f\"Ingreso mediano para hombres: ${ingreso_percentil(ocupados_hombres,50):,.0f}\".replace(',','.'))\nprint(f\"Ingreso mediano para mujeres: ${ingreso_percentil(ocupadas_mujeres,50):,.0f}\".replace(',','.'))\n\n\nIngreso mediano para hombres: $500.000\nIngreso mediano para mujeres: $405.348\n\n\n\n\nCode\nocupados_regiones = data.groupby('region').apply(lambda x: ingreso_percentil(x,50))\nfor index, value in ocupados_regiones.items():\n    print(f\"Ingreso mediano en la Regi√≥n de {index}: ${value:,}\".replace(',','.'))\n\n\nIngreso mediano en la Regi√≥n de Antofagasta: $570.000\nIngreso mediano en la Regi√≥n de Arica y Parinacota: $420.000\nIngreso mediano en la Regi√≥n de Atacama: $506.685\nIngreso mediano en la Regi√≥n de Ays√©n: $537.086\nIngreso mediano en la Regi√≥n de Biob√≠o: $427.307\nIngreso mediano en la Regi√≥n de Coquimbo: $405.348\nIngreso mediano en la Regi√≥n de La Araucan√≠a: $397.991\nIngreso mediano en la Regi√≥n de Los Lagos: $405.348\nIngreso mediano en la Regi√≥n de Los R√≠os: $420.000\nIngreso mediano en la Regi√≥n de Magallanes: $587.754\nIngreso mediano en la Regi√≥n de Maule: $400.000\nIngreso mediano en la Regi√≥n de Metropolitana: $500.000\nIngreso mediano en la Regi√≥n de O'Higgins: $400.000\nIngreso mediano en la Regi√≥n de Tarapac√°: $469.630\nIngreso mediano en la Regi√≥n de Valpara√≠so: $427.642\nIngreso mediano en la Regi√≥n de √ëuble: $400.000\n\n\nDichos valores coinciden con los valores entregados por el INE (ver slides abajo).\n\n\n\nSitio INE ingreso mediano\n\n\n\n\n\nSitio INE ingreso mediano regiones\n\n\nDe la misma forma como calculamos la mediana podemos calcular los sueldos para todos los percentiles.\n\n\nCode\nimport plotly.graph_objects as go\n\nPERCENTILES = [ingreso_percentil(data,p) for p in range(100)]\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=[.01*i for i in range(5,100)], y=PERCENTILES[5:], hovertemplate='Sueldo mensual: %{y:$,.0f}&lt;extra&gt;&lt;/extra&gt;'))\nfig.update_layout(\n    title = f'Distribuci√≥n de ingresos',\n    yaxis_title = 'Sueldos mensuales',\n    xaxis = dict(\n        tickmode = 'array',\n        tickvals = [.1*i for i in range(11)],\n        ticktext = [f'{10*i}%' for i in range(11)]\n    ),\n    xaxis_tickformat=',.0%',\n    yaxis_tickformat=',.0'.replace(',',','),\n    yaxis = dict(\n        tickmode = 'array',\n        tickvals = [500_000*i for i in range(9)],\n        ticktext = [f'${500_000*i:,}'.replace(',','.') for i in range(9)]\n    ),\n    showlegend=False\n)\nfig.update_layout(\n    hovermode=\"x\",\n    hoverlabel=dict(\n        bgcolor=\"white\",\n    )\n)\nfig.show()\n\n\n\n        \n        \n            \n            \n        \n\n\nPuede ver en qu√© percentil se encuentra su sueldo en esta aplicaci√≥n.\nDudas o sugerencias pueden hacerlas envi√°ndome un mensaje en Twitter: alonsosilva"
  },
  {
    "objectID": "blog/posts/2025-06-17-Understanding-Tokenizers/UnderstandingTokenizers.html",
    "href": "blog/posts/2025-06-17-Understanding-Tokenizers/UnderstandingTokenizers.html",
    "title": "Understanding Tokenizers",
    "section": "",
    "text": "This post is largely inspired by Understanding GPT tokenizers by Simon Willison.\nLarge Language Models don‚Äôt work with words, they work with tokens. They take text, convert it into tokens (integers), then predict which tokens should come next.\nTo explain this I will use the transformers_js_py library which allows us to work with LLMs in the browser through WebAssembly.\nLet‚Äôs consider a text we want to tokenize:\n\ntext = \"The dog eats the apples.\"\n\nEach LLM has its own tokenizer, so we need to specify which model we are going to use:\n\nmodel_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\nFinally, we can encode it with the model‚Äôs tokenizer by running the following cell:\n\n\n\n        \n        \n\n\nYou should see as output the following list of integers:\n\ntoken_ids = [785, 5562, 49677, 279, 40676, 13]\n\nThis list of integers correspond to the token ids.\nYou can encode other texts by running the following code above:\ntokenizer.encode(\"El perro come las manzanas.\")\nYou can also modify the model_id to see how the tokens change (search for other models in HuggingFace). For example:\n\n\n\n        \n        \n\n\nWith this model, you should see as output the following list of tokens:\n\ntoken_ids_Mistral_7B_v0_3 = [1, 1183, 4682, 1085, 2217, 1040, 1747, 3583, 29491]\n\nYou can observe that even if the text is the same, these tokens are very different from the previous ones:\n\ntoken_ids = [785, 5562, 49677, 279, 40676, 13]\n\nWe can do the reverse operation. We take the tokens and convert them to text:\n\n\n\n        \n        \n\n\nEncoding a text and then decoding it should give the same original text.\nPlaying with tokenizers reveal all sorts of interesting facts.\nMost common English words are assigned a single token. As demonstrated above:\n\n‚ÄúThe‚Äù: 785\n‚Äù dog‚Äù: 5562\n‚Äù eats‚Äù: 49677\n‚Äù the‚Äù: 279\n‚Äù apples‚Äù: 40676\n‚Äú.‚Äù: 13\n\nCapitalization is important: ‚ÄúThe‚Äù with a capital T corresponds to token 785, but ‚Äúthe‚Äù with lowercase is 1782 and ‚Äù the‚Äù with both a leading space and a lowercase t is token 279.\nMany words also have a token that incorporates a leading space. This makes for much more efficient encoding of full sentences, since they can be encoded without needing to spend a token on each whitespace character.\nNumbers get their own tokens:\n\n‚Äú0‚Äù: 15\n‚Äú1‚Äù: 16\n‚Äú2‚Äù: 17\n‚Ä¶\n‚Äú9‚Äù: 24\n\nLanguages other than English suffer from less efficient tokenization.\n‚ÄúEl perro come las manzanas‚Äù in Spanish is encoded like this:\n\n‚ÄúEl‚Äù: 6582\n‚Äù per‚Äù: 817\n‚Äúro‚Äù: 299\n‚Äù come‚Äù: 2525\n‚Äù las‚Äù: 5141\n‚Äù man‚Äù: 883\n‚Äúz‚Äù: 89\n‚Äù anas‚Äù: 25908\n‚Äú.‚Äù: 13\n\n‚ÄúLe chien mange les pommes‚Äù in French is encoded like this:\n\n‚ÄúLe‚Äù: 2304\n‚Äù ch‚Äù: 521\n‚Äúien‚Äù: 3591\n‚Äù mange‚Äù: 59434\n‚Äù les‚Äù: 3541\n‚Äù pom‚Äù: 29484\n‚Äù mes‚Äù: 8828\n‚Äú.‚Äù : 13\n\nThere are all sorts of interesting things like the glitch tokens.\nThe majority of tokenizers are trained with the byte-pair encoding algorithm.\nMany researchers think we should work with bytes and we shouldn‚Äôt have tokenizers to begin with and they are actively trying to remove them (without much success)."
  },
  {
    "objectID": "blog/posts/2025-06-28-Understanding-Memory/Understanding_LLMs_Memory.html",
    "href": "blog/posts/2025-06-28-Understanding-Memory/Understanding_LLMs_Memory.html",
    "title": "Understanding LLM Memory",
    "section": "",
    "text": "Despite what some people think (even some researchers I‚Äôve met), language models don‚Äôt have any memory. The confusion comes, I suppose, from the fact that most people interact with models through some user interface like www.chatgpt.com, which handles the memory for them (incidentally, in some interesting ways as explained below).\nLet‚Äôs explore this further. Let‚Äôs use transformers_js_py which allows us to use language models in the browser.\n\n    \n    \n    \n    \n\nLet‚Äôs download a small model and its tokenizer (it takes a few minutes):\n\n    \n    \n    \n    \n\n\n    \n    \n    \n    \n\nWe can ask the question What's 2 + 2? and get the following response:\n\n    \n    \n    \n    \n\n\n    \n    \n    \n    \n\nIf we ask the model to add to the result 2 more, we get the following response:\n\n    \n    \n    \n    \n\n\n    \n    \n    \n    \n\nThe model doesn‚Äôt have any recollection of our previous conversation. Now, that‚Äôs completely expected since we haven‚Äôt provided the model any way to access that information."
  },
  {
    "objectID": "blog/posts/2025-06-28-Understanding-Memory/Understanding_LLMs_Memory.html#understanding-llm-memory",
    "href": "blog/posts/2025-06-28-Understanding-Memory/Understanding_LLMs_Memory.html#understanding-llm-memory",
    "title": "Understanding LLM Memory",
    "section": "",
    "text": "Despite what some people think (even some researchers I‚Äôve met), language models don‚Äôt have any memory. The confusion comes, I suppose, from the fact that most people interact with models through some user interface like www.chatgpt.com, which handles the memory for them (incidentally, in some interesting ways as explained below).\nLet‚Äôs explore this further. Let‚Äôs use transformers_js_py which allows us to use language models in the browser.\n\n    \n    \n    \n    \n\nLet‚Äôs download a small model and its tokenizer (it takes a few minutes):\n\n    \n    \n    \n    \n\n\n    \n    \n    \n    \n\nWe can ask the question What's 2 + 2? and get the following response:\n\n    \n    \n    \n    \n\n\n    \n    \n    \n    \n\nIf we ask the model to add to the result 2 more, we get the following response:\n\n    \n    \n    \n    \n\n\n    \n    \n    \n    \n\nThe model doesn‚Äôt have any recollection of our previous conversation. Now, that‚Äôs completely expected since we haven‚Äôt provided the model any way to access that information."
  },
  {
    "objectID": "blog/posts/2025-06-28-Understanding-Memory/Understanding_LLMs_Memory.html#handling-memory",
    "href": "blog/posts/2025-06-28-Understanding-Memory/Understanding_LLMs_Memory.html#handling-memory",
    "title": "Understanding LLM Memory",
    "section": "Handling Memory",
    "text": "Handling Memory\nThe simplest way to handle memory is to provide our previous conversation within a list of messages:\n\n    \n    \n    \n    \n\nWith all these messages we obtain the following response:\n\n    \n    \n    \n    \n\nThat works! The problem with that approach is that we need to be mindful of the context length of the model. We could for example store only the last 10 messages or so and perhaps the system message if there is one (in this example, we don‚Äôt have one).\nMore sophisticated approaches could be to store the messages and its responses in a vector database and retrieve the most closely related ones. Similarly, we could store the information as a graph in a graph database and retrieve the nodes and edges most closely related.\nChatGPT‚Äôs memory feature is very interesting because it uses memory as a tool. Let‚Äôs take a look at that.\nWe can define the tools as follows:\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"biography\",\n            \"description\": \"The biography tool allows you to persist user information across conversations. Use this tool and write whatever user information you want to remember. The information will appear in the model set context below in future conversations.\",\n            \"parameters\": {\n                \"properties\": {\n                    \"user_information\": {\n                        \"description\": \"Information from the user you want to remember across conversations.\",\n                        \"type\": \"string\",\n                    }\n                },\n                \"required\": [\"user_information\"],\n                \"type\": \"object\",\n            },\n        },\n    }\n]\n\n    \n    \n    \n    \n\n\n    \n    \n    \n    \n\nWe can then store the tool call as a text file and provide it in the context.\nI really like that idea. It‚Äôs very simple and clean!"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Understanding Function Calling\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 5, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding LLM Memory\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 28, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Chat Templates\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 20, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Tokenizers\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 17, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nBuild a basic widget with AnyWidget in a Solara app\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 23, 2024\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nBuild a basic LLM chat app with Solara\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nObservatorio de sueldos en Chile\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 21, 2023\n\n\nAlonso Silva\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "til/posts/2025-06-15-Embed_REPL/Embed_REPL.html",
    "href": "til/posts/2025-06-15-Embed_REPL/Embed_REPL.html",
    "title": "Embed live REPL on a website",
    "section": "",
    "text": "TIL that it‚Äôs possible to embed a live REPL on a website. This is possible thanks to JupyterLite.\nYou can write some code:\n\ncode_str = \"\"\"\nimport numpy as np\n\nnp.random.randint(10, size=5)\"\"\"\n\nYou can use urllib.parse.quote, which handles special characters as needed for URLs:\n\nimport urllib.parse\n\ncode = urllib.parse.quote(code_str)\n\nThen you can then use the public facing https://jupyterlite.github.io/demo/repl as an example and embed a live REPL on a website:\n\nfrom IPython.display import IFrame\n\nIFrame(f\"https://jupyterlite.github.io/demo/repl/index.html?toolbar=1&kernel=python&promptCellPosition=left&code={code}&execute=0\", width=850, height=200)\n\n\n        \n        \n\n\nThere are several configuration options.\nI find this very cool especially for writing tutorials and blog posts."
  },
  {
    "objectID": "til/posts/2024-11-21-Datasette/Datasette.html",
    "href": "til/posts/2024-11-21-Datasette/Datasette.html",
    "title": "Quickly explore and share a SQLite database with Datasette and Datasette Lite",
    "section": "",
    "text": "I only recently learned about Datasette. My interest came from the possibility to quickly explore a SQLite database and I am not disappointed. You can explore a SQLite database database.db as follows:\n\nuv (ephemeral venv)uv (persistent venv)pip\n\n\n\n\nTerminal\n\nuvx datasette database.db\n\n\n\n\n\nTerminal\n\nuv venv\nsource .venv/bin/activate\nuv pip install datasette\ndatasette database.db\n\n\n\n\n\nTerminal\n\npython -m venv .venv\nsource .venv/bin/activate\npython -m pip install datasette\ndatasette database.db\n\n\n\n\nThis will pop up the following in your browser:\n\nWe get a nice indication of the title of the dataset, how many rows, and how many tables there are, as well as links to explore the tables. For example, if I click on the link for the table responses I get all the rows and columns of that table:\n\nYou can also use Datasette Lite which runs in browser using WebAssembly and Pyodide to share your SQLite database and let others to explore it. For example, by putting a SQLite database in GitHub, you can upload it in https://lite.datasette.io/, and you get a link to explore it in the following link or directly below:"
  },
  {
    "objectID": "til/posts/2025-06-29-Quarto-Marimo/marimo.html",
    "href": "til/posts/2025-06-29-Quarto-Marimo/marimo.html",
    "title": "How to add marimo notebook cells to Quarto projects",
    "section": "",
    "text": "TIL how to add marimo notebook cells to Quarto projects such as this one:"
  },
  {
    "objectID": "til/posts/2025-06-29-Quarto-Marimo/marimo.html#instructions",
    "href": "til/posts/2025-06-29-Quarto-Marimo/marimo.html#instructions",
    "title": "How to add marimo notebook cells to Quarto projects",
    "section": "Instructions",
    "text": "Instructions\n\nInstall uv (if you haven‚Äôt yet):\n\n\n\nTerminal\n\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n\nCreate a Quarto project (if you haven‚Äôt yet):\n\n\n\nTerminal\n\nuvx --from quarto-cli quarto create project\n\nHere are some example configuration options:\n\n\nEnter into the project folder and add the marimo extension:\n\n\n\nTerminal\n\nuvx --from quarto-cli quarto add marimo-team/quarto-marimo\n\n\n\nEdit the index.qmd file (this is an example):\n\n\n\nindex.qmd\n\n---\ntitle: \"test-quarto-marimo\"\nfilters:\n  - marimo-team/marimo\n---\n```python {.marimo}\nimport marimo as mo\n\nslider = mo.ui.slider(1, 30)\n```\n\n```python {.marimo}\nmo.md(\n    f\"\"\"\n    This is a **reactive** Python notebook that **runs automatically**\n    when you modify them or\n    interact with UI elements, like this slider: {slider}\n\n    {\"##\" + \"üçÉ\" * slider.value}\n    \"\"\"\n).callout(\"info\")\n\n\nPreview the project:\n\n\n\nTerminal\n\nuvx --with marimo --from quarto-cli quarto preview\n\n\nDeploy it (for example to GitHub pages), the command is the following:\n\n\n\nTerminal\n\nuvx --with marimo --from quarto-cli quarto publish\n\n\nThere is a nice video by the marimo team explaining the whole process:"
  },
  {
    "objectID": "til/posts/2025-06-29-Quarto-Marimo/marimo.html#conclusions",
    "href": "til/posts/2025-06-29-Quarto-Marimo/marimo.html#conclusions",
    "title": "How to add marimo notebook cells to Quarto projects",
    "section": "Conclusions",
    "text": "Conclusions\nIf you have read my previous post, you know I am very excited that we can now embed a single executable cell inside a website. You can also embed a whole notebook inside a website. However, I love the idea of adding many dependent executable cells to a website such as with this extension. The reactivity is definitely a nice plus!"
  },
  {
    "objectID": "til/index.html",
    "href": "til/index.html",
    "title": "TIL",
    "section": "",
    "text": "How to use the chess library\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 4, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nHow to add marimo notebook cells to Quarto projects\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 29, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nHow to do Quizzes with JupyterQuiz\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 16, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nEmbed live REPL on a website\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nJun 15, 2025\n\n\nAlonso Silva\n\n\n\n\n\n\n\n\n\n\n\n\nQuickly explore and share a SQLite database with Datasette and Datasette Lite\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nNov 22, 2024\n\n\nAlonso Silva\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "til/posts/2025-07-04-Chess/Chess.html",
    "href": "til/posts/2025-07-04-Chess/Chess.html",
    "title": "How to use the chess library",
    "section": "",
    "text": "TIL how to use the chess library. It is an excellent library that I used to create a HuggingFace dataset and it can also be used with WebAssembly.\nYou can install the chess library as follows:\n\npipuvwasm\n\n\n\n\nTerminal\n\npython -m venv .venv\nsource .venv/bin/activate\npython -m pip install chess\n\n\n\n\n\nTerminal\n\nuv venv\nsource .venv/bin/activate\nuv pip install chess\n\n\n\n\n\nTerminal\n\n%pip install chess\n\n\n\n\nThis is all the code needed to display the initial chess board:\n\nimport chess\n\nboard = chess.Board()\nboard\n\n\n\n\n\n\n\n\nThis is the code to use it here with WebAssembly:\n\n\n\n        \n        \n\n\nThis is the code to get the legal moves:\n\nlegal_moves_uci = list(board.legal_moves)[:5] # display only the first 5 legal moves\nlegal_moves_uci\n\n[Move.from_uci('g1h3'),\n Move.from_uci('g1f3'),\n Move.from_uci('b1c3'),\n Move.from_uci('b1a3'),\n Move.from_uci('h2h3')]\n\n\nWe can also get the legal moves in standard algebraic notation:\n\nlegal_moves_san = [chess.Board.san(board,move) for move in legal_moves_uci][:5] # display only the first 5 legal moves\nlegal_moves_san\n\n['Nh3', 'Nf3', 'Nc3', 'Na3', 'h3']\n\n\nWe can see whose turn is it as follows:\n\nwhose_turn = \"white\" if board.turn == chess.WHITE else \"black\"\nwhose_turn\n\n'white'\n\n\nWe can move a piece as follows:\n\nmove_uci = chess.Move.from_uci(\"f2f3\")\nif board.is_legal(move_uci):\n    board.push(move_uci)\nboard\n\n\n\n\n\n\n\n\nNow it is black‚Äôs turn:\n\nwhose_turn = \"white\" if board.turn == chess.WHITE else \"black\"\nwhose_turn\n\n'black'\n\n\nWe can also move it with standard algebraic notation:\n\nmove = \"e5\"\nmove_uci = board.parse_san(move)\nif board.is_legal(move_uci):\n    board.push_san(move)\nboard\n\n\n\n\n\n\n\n\nLet‚Äôs do two more moves because I want to show you something:\n\nmove_uci = chess.Move.from_uci(\"g2g4\")\nif board.is_legal(move_uci):\n    board.push(move_uci)\nmove_uci = chess.Move.from_uci(\"d8h4\")\nif board.is_legal(move_uci):\n    board.push(move_uci)\nboard\n\n\n\n\n\n\n\n\nWe can now check that this is a checkmate!\n\nboard.is_checkmate()\n\nTrue\n\n\nWe can also undo the previous move:\n\nboard.pop()\nboard"
  },
  {
    "objectID": "til/posts/2025-07-04-Chess/Chess.html#the-chess-library",
    "href": "til/posts/2025-07-04-Chess/Chess.html#the-chess-library",
    "title": "How to use the chess library",
    "section": "",
    "text": "TIL how to use the chess library. It is an excellent library that I used to create a HuggingFace dataset and it can also be used with WebAssembly.\nYou can install the chess library as follows:\n\npipuvwasm\n\n\n\n\nTerminal\n\npython -m venv .venv\nsource .venv/bin/activate\npython -m pip install chess\n\n\n\n\n\nTerminal\n\nuv venv\nsource .venv/bin/activate\nuv pip install chess\n\n\n\n\n\nTerminal\n\n%pip install chess\n\n\n\n\nThis is all the code needed to display the initial chess board:\n\nimport chess\n\nboard = chess.Board()\nboard\n\n\n\n\n\n\n\n\nThis is the code to use it here with WebAssembly:\n\n\n\n        \n        \n\n\nThis is the code to get the legal moves:\n\nlegal_moves_uci = list(board.legal_moves)[:5] # display only the first 5 legal moves\nlegal_moves_uci\n\n[Move.from_uci('g1h3'),\n Move.from_uci('g1f3'),\n Move.from_uci('b1c3'),\n Move.from_uci('b1a3'),\n Move.from_uci('h2h3')]\n\n\nWe can also get the legal moves in standard algebraic notation:\n\nlegal_moves_san = [chess.Board.san(board,move) for move in legal_moves_uci][:5] # display only the first 5 legal moves\nlegal_moves_san\n\n['Nh3', 'Nf3', 'Nc3', 'Na3', 'h3']\n\n\nWe can see whose turn is it as follows:\n\nwhose_turn = \"white\" if board.turn == chess.WHITE else \"black\"\nwhose_turn\n\n'white'\n\n\nWe can move a piece as follows:\n\nmove_uci = chess.Move.from_uci(\"f2f3\")\nif board.is_legal(move_uci):\n    board.push(move_uci)\nboard\n\n\n\n\n\n\n\n\nNow it is black‚Äôs turn:\n\nwhose_turn = \"white\" if board.turn == chess.WHITE else \"black\"\nwhose_turn\n\n'black'\n\n\nWe can also move it with standard algebraic notation:\n\nmove = \"e5\"\nmove_uci = board.parse_san(move)\nif board.is_legal(move_uci):\n    board.push_san(move)\nboard\n\n\n\n\n\n\n\n\nLet‚Äôs do two more moves because I want to show you something:\n\nmove_uci = chess.Move.from_uci(\"g2g4\")\nif board.is_legal(move_uci):\n    board.push(move_uci)\nmove_uci = chess.Move.from_uci(\"d8h4\")\nif board.is_legal(move_uci):\n    board.push(move_uci)\nboard\n\n\n\n\n\n\n\n\nWe can now check that this is a checkmate!\n\nboard.is_checkmate()\n\nTrue\n\n\nWe can also undo the previous move:\n\nboard.pop()\nboard"
  },
  {
    "objectID": "til/posts/2025-07-04-Chess/Chess.html#dataset-checkmate-in-one-big-bench",
    "href": "til/posts/2025-07-04-Chess/Chess.html#dataset-checkmate-in-one-big-bench",
    "title": "How to use the chess library",
    "section": "Dataset Checkmate In One BIG-Bench",
    "text": "Dataset Checkmate In One BIG-Bench\nGiven that we know how to do a text representation of the positions in the board, we can use the dataset from the BIG-Bench called Checkmate In One. This dataset already has a text representation. For example:\n1. d4 d5 2. Nf3 Nf6 3. e3 a6 4. Nc3 e6 5. Bd3 h6\n6. e4 dxe4 7. Bxe4 Nxe4 8. Nxe4 Bb4+ 9. c3 Ba5 10. Qa4+ Nc6\n11. Ne5 Qd5 12. f3 O-O 13. Nxc6 bxc6 14. Bf4 Ra7 15. Qb3 Qb5\n16. Qxb5 cxb5 17. a4 bxa4 18. Rxa4 Bb6 19. Kf2 Bd7 20. Ke3 Bxa4\n21. Ra1 Bc2 22. c4 Bxe4 23. fxe4 c5 24. d5 exd5 25. exd5 Re8+\n26. Kf3 Rae7 27. Rxa6 Bc7 28. Bd2 Re2 29. Bc3 R8e3+ 30. Kg4 Rxg2+\n31. Kf5\nwhere the checkmate in one is the move Rg5#.\nI wanted to provide an easier representation for small language models:\n\nmoves_str = \"\"\"\n1. d4 d5 2. Nf3 Nf6 3. e3 a6 4. Nc3 e6 5. Bd3 h6\n6. e4 dxe4 7. Bxe4 Nxe4 8. Nxe4 Bb4+ 9. c3 Ba5 10. Qa4+ Nc6\n11. Ne5 Qd5 12. f3 O-O 13. Nxc6 bxc6 14. Bf4 Ra7 15. Qb3 Qb5\n16. Qxb5 cxb5 17. a4 bxa4 18. Rxa4 Bb6 19. Kf2 Bd7 20. Ke3 Bxa4\n21. Ra1 Bc2 22. c4 Bxe4 23. fxe4 c5 24. d5 exd5 25. exd5 Re8+\n26. Kf3 Rae7 27. Rxa6 Bc7 28. Bd2 Re2 29. Bc3 R8e3+ 30. Kg4 Rxg2+\n31. Kf5\"\"\"\nmoves = [move for move in moves_str.split() if not move[0].isdigit()]\nboard = chess.Board()\nfor move in moves:\n    move_uci = board.parse_san(move)\n    if board.is_legal(move_uci):\n        board.push_san(move)\n    else:\n        print(\"Error: Invalid move\")\ndisplay(board)\nprint(create_board_str(board))\n\n\n\n\n\n\n\n\n\n    a    b    c    d    e    f    g    h  \n  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n8 ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ k  ‚îÇ    ‚îÇ 8\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n7 ‚îÇ    ‚îÇ    ‚îÇ b  ‚îÇ    ‚îÇ    ‚îÇ p  ‚îÇ p  ‚îÇ    ‚îÇ 7\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n6 ‚îÇ R  ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ p  ‚îÇ 6\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n5 ‚îÇ    ‚îÇ    ‚îÇ p  ‚îÇ P  ‚îÇ    ‚îÇ K  ‚îÇ    ‚îÇ    ‚îÇ 5\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n4 ‚îÇ    ‚îÇ    ‚îÇ P  ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 4\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n3 ‚îÇ    ‚îÇ    ‚îÇ B  ‚îÇ    ‚îÇ r  ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 3\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n2 ‚îÇ    ‚îÇ P  ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ r  ‚îÇ P  ‚îÇ 2\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n1 ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 1\n  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n    a    b    c    d    e    f    g    h\n\n\n\nI think this is a much simpler task (hopefully not too simple) for small language models to solve.\nGPT-4o failed when given this task:\n\nThe conversation can be found here: https://chatgpt.com/s/t_68683067ff7081919314e8573ec64d5d\nThe prompt was the following:\nYou are playing Black in a game of Chess. The initial state of the board was this:\n\n    a    b    c    d    e    f    g    h  \n  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n8 ‚îÇ r  ‚îÇ n  ‚îÇ b  ‚îÇ q  ‚îÇ k  ‚îÇ b  ‚îÇ n  ‚îÇ r  ‚îÇ 8\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n7 ‚îÇ p  ‚îÇ p  ‚îÇ p  ‚îÇ p  ‚îÇ p  ‚îÇ p  ‚îÇ p  ‚îÇ p  ‚îÇ 7\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n6 ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 6\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n5 ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 5\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n4 ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 4\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n3 ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 3\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n2 ‚îÇ P  ‚îÇ P  ‚îÇ P  ‚îÇ P  ‚îÇ P  ‚îÇ P  ‚îÇ P  ‚îÇ P  ‚îÇ 2\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n1 ‚îÇ R  ‚îÇ N  ‚îÇ B  ‚îÇ Q  ‚îÇ K  ‚îÇ B  ‚îÇ N  ‚îÇ R  ‚îÇ 1\n  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n    a    b    c    d    e    f    g    h\n\nThe current state of the board is this:\n\n    a    b    c    d    e    f    g    h  \n  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n8 ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ k  ‚îÇ    ‚îÇ 8\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n7 ‚îÇ    ‚îÇ    ‚îÇ b  ‚îÇ    ‚îÇ    ‚îÇ p  ‚îÇ p  ‚îÇ    ‚îÇ 7\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n6 ‚îÇ R  ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ p  ‚îÇ 6\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n5 ‚îÇ    ‚îÇ    ‚îÇ p  ‚îÇ P  ‚îÇ    ‚îÇ K  ‚îÇ    ‚îÇ    ‚îÇ 5\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n4 ‚îÇ    ‚îÇ    ‚îÇ P  ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 4\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n3 ‚îÇ    ‚îÇ    ‚îÇ B  ‚îÇ    ‚îÇ r  ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 3\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n2 ‚îÇ    ‚îÇ P  ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ r  ‚îÇ P  ‚îÇ 2\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n1 ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ 1\n  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n    a    b    c    d    e    f    g    h\n\nMake your move in standard algebraic notation format enclosed in square brackets (e.g., [e4]).\nYou can also include additional text in your messages.\nIt's your turn. Find the checkmate in one.\nIf you want to use the Checkmate In One BIG-Bench with that text representation, it can be found here:\nhttps://hf.co/datasets/alonsosilva/chess_checkmate_in_one_big_bench\nThe future plan is to do Reinforcement Learning with Verifiable Rewards with that dataset. And the longer term plan is to do multi-turn reinforcement learning with that game. Similar efforts have already been done with the games Tic Tac Toe and 2048: https://github.com/OpenPipe/ART?tab=readme-ov-file"
  },
  {
    "objectID": "til/posts/2025-06-16-Jupyter_Quiz/JupyterQuiz.html",
    "href": "til/posts/2025-06-16-Jupyter_Quiz/JupyterQuiz.html",
    "title": "How to do Quizzes with JupyterQuiz",
    "section": "",
    "text": "TIL how to do quizzes with JupyterQuiz. JupyterQuiz is a tool to make quizzes in jupyter notebooks. It is very easy to use and you can even use it with WebAssembly as I will show below:\nYou can install jupyterquiz as follows:\n\npipuvwasm\n\n\n\n\nTerminal\n\npython -m venv .venv\nsource .venv/bin/activate\npython -m pip install jupyterquiz\n\n\n\n\n\nTerminal\n\nuv venv\nsource .venv/bin/activate\nuv pip install jupyterquiz\n\n\n\n\n\nTerminal\n\n%pip install jupyterquiz\n\n\n\n\nThen you need to:\n\nProvide a question (in this example ‚ÄúWhat‚Äôs the capital of France?‚Äù),\nSpecify what‚Äôs the type of question you want to make (‚Äúmultiple_choice‚Äù, ‚Äúmany_choice‚Äù, etc.),\nand finally the possible answers and which one(s) are correct.\n\nThe expected format is as follows:\n\nexample = [\n    {\n        \"question\": \"What is the capital of France?\",\n        \"type\": \"multiple_choice\",\n        \"answers\": [\n            {\"answer\": \"London\", \"correct\": False},\n            {\"answer\": \"Paris\", \"correct\": True},\n            {\"answer\": \"New York\", \"correct\": False},\n            {\"answer\": \"Rome\", \"correct\": False},\n        ],\n    }\n]\n\nThen you can display the quiz:\n\nfrom jupyterquiz import display_quiz\n\ndisplay_quiz(example)\n\n\n\n\n\n\n\nYou can also try it here by running the following cell:\n\n\n\n        \n        \n\n\nYou can also store the questions in a json file (in this case I‚Äôm storing it here) and then display the quiz:\n\n\n\n        \n        \n\n\nNotice that we are not storing the results so this is only for people to get feedback when using these quizzes.\nI like this simple way to do quizzes and I‚Äôm using them to motivate my kid to learn new things."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "‚ÄúLarge Language Models Playing Mixed Strategy Nash Equilibrium Games,‚Äù A. Silva, Proc. of the 11th International Conference on Network Games, Control and Optimization (Netgcoop), Lille, France, October 9-11, 2024.\n‚ÄúAggregation Methods and Comparative Study in Time-to-Event Analysis Models,‚Äù C. Fernandez, C.S. Chen, P. Gaillard, A. Silva, International Journal of Data Science and Analytics, September 25, 2024.\n‚ÄúPredicting Network Hardware Faults through Layered Treatment of Alarms Logs,‚Äù A. Massaro, D. Kostadinov, A. Silva, A. Obeid Guzman, A. Aghasaryan, Entropy, vol.¬†25, issue 6, pp.¬†917, 2023.\n‚ÄúExperimental Comparison of Semi-parametric, Parametric, and Machine Learning Methods for Time-to-Event Analysis Through the IPEC Score,‚Äù C. Fernandez, C.S. Chen, P. Gaillard, A. Silva, Proc. of the 52√®mes Journ√©es de Statistiques de la Soci√©t√© Fran√ßaise de Statistique (SFdS), Nice, France, June 7-11, 2021.\n‚ÄúExperimental Comparison of Semi-parametric, Parametric, and Machine Learning Models for Time-to-Event Analysis Through the Concordance Index,‚Äù C. Fernandez, C.S. Chen, P. Gaillard, A. Silva, Proc. of the 52√®mes Journ√©es de Statistiques de la Soci√©t√© Fran√ßaise de Statistique (SFdS), Nice, France, May 25-29, 2020.\n‚ÄúPath Planning Problems with Side Observations ‚Äî When Colonels Play Hide-and-Seek,‚Äù D.Q. Vu, P. Loiseau, A. Silva, L. Tran-Thanh, Proc of the 34th AAAI Conference on Artificial Intelligence (AAAI), New York, NY, USA, February 7-12, 2020.\n‚ÄúCombinatorial Bandits for Sequential Learning in Colonel Blotto Games,‚Äù D.Q. Vu, P. Loiseau, A. Silva, Proc. of the 58th IEEE Conference on Decision and Control (CDC), Nice, France, December 11-13, 2019.\n‚ÄúEfficient Computation of Approximate Equilibria in Discrete Colonel Blotto Games,‚Äù D.Q. Vu, P. Loiseau, A. Silva, Proc. of the 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence (IJCAI-ECAI), Stockholm, Sweden, July 13-19, 2018.\n‚ÄúA Simple and Efficient Algorithm to Compute Epsilon-Equilibria of Discrete Colonel Blotto Games,‚Äù D.Q. Vu, P. Loiseau, A. Silva, Proc. of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), Stockholm, Sweden, July 11-13, 2018. (Poster with extended abstract) Note: The IJCAI version above supersedes this extended abstract.\n‚ÄúOptimal Control of Storage Regeneration with Repair Codes,‚Äù F. De Pellegrini, R. El-Azouzi, A. Silva, O. Hassani, Proc. of the International Workshop on the Future of Cloud Computing and Cloud Services (FutureCloud), Hong Kong, China, December 11-14, 2017.\n‚ÄúNovel Market Approach for Locally Balancing Renewable Energy Production and Flexible Demand,‚Äù J. Horta, D. Kofman, D. Menga, A. Silva, Proc. of the IEEE International Conference on Smart Grid Communications (SmartGridComm), Dresden, Germany, October 23-26, 2017. Best Paper Award.\n‚ÄúEvolution of Social Power for Opinion Dynamics Networks,‚Äù S. Iglesias Rey, P. Reyes, A. Silva, Proc. of the 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton), Monticello, IL, USA, October 4-6, 2017.\n‚ÄúAdvertising Competitions in Social Networks,‚Äù A.M. Masucci, A. Silva, Proc. of the American Control Conference (ACC), Seattle, WA, USA, May 24-26, 2017.\n‚ÄúGreen Base Station Placement for Microwave Backhaul Links,‚Äù A. Silva, A.M. Masucci, Proc. of the 3rd International Symposium on Ubiquitous Networking (UNet), Casablanca, Morocco, May 9-12, 2017. Best Paper Award.\n‚ÄúOpinion Manipulation in Social Networks,‚Äù A. Silva, Proc. of the International Conference on Network Games, Control and Optimization (NETGCOOP), Avignon, France, November 23-25, 2016.\n‚ÄúGo-Index: Applying Supply Networks Principles as Internet Robustness Metrics,‚Äù I. Bachmann, F. Morales, A. Silva, J. Bustos-Jim√©nez, Proc. of the International Conference on Network Games, Control and Optimization (NETGCOOP), Avignon, France, November 23-25, 2016.\n‚ÄúOn the Throughput-Delay Trade-off in Georouting Networks,‚Äù P. Jacquet, S. Malik, B. Mans, A. Silva, IEEE Transactions on Information Theory, Vol. 62, Issue 6, pp.¬†3230‚Äì3242, June 2016,\\ ISSN: 0018-9448, DOI: 10.1109/TIT.2016.2519419.\n‚ÄúDefensive Resource Allocation in Social Networks,‚Äù A.M. Masucci, A. Silva, Proc. of the 54th IEEE Conference on Decision and Control (CDC), Osaka, Japan, December 15-18, 2015.\n‚ÄúMiuz: Measuring the Impact of Disconnecting a Node,‚Äù I. Bachmann, P. Reyes, A. Silva, J. Bustos-Jim√©nez, Proc. of the 34th SCCC 2015, Santiago, Chile, November 9-13, 2015.\n‚ÄúStrategic Resource Allocation for Competitive Influence in Social Networks,‚Äù A.M. Masucci, A. Silva, Proc. of the 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton), Monticello, IL, USA, October 1-3, 2014.\n‚ÄúMaximum Coverage and Maximum Connected Covering in Social Networks with Partial Topology Information,‚Äù P. Reyes, A. Silva, Proc. of the 6th IEEE INFOCOM Workshop on Network Science for Communication Networks (NetSciCom), Toronto, Canada, April 27- May 2, 2014.\n‚ÄúInformation Spreading on Almost Torus Networks,‚Äù A.M. Masucci, A. Silva, Proc. of the 52nd IEEE Conference on Decision and Control (CDC), Florence, Italy, December 10-13, 2013.\n‚ÄúOptimal Mobile Association on Hybrid Networks: Centralized and Decentralized Case,‚Äù A. Silva, T. Hamidou, E. Altman, M. Debbah, IEEE Transactions on Automatic Control, Volume 58, Issue 8, Pages 2018-2031, August 2013, ISSN: 0018-9286, DOI: 10.1109/TAC.2013.2250072.\n‚ÄúA Multi-Layer Market for Vehicle-to-Grid Energy Trading in the Smart Grid,‚Äù A.Y.S. Lam, L. Huang, A. Silva, W. Saad. Proc. of the 1st IEEE INFOCOM Workshop on Green Networking and Smart Grids (CCSES), Orlando, FL, USA, March 25-30, 2012.\n‚ÄúOn the Throughput-Delay Trade-off in Georouting Networks,‚Äù P. Jacquet, S. Malik, B. Mans, A. Silva. Proc. of the 31st IEEE INFOCOM 2012, Orlando, FL, USA, March 25-30, 2012.\n‚ÄúOn the Spectral Gap of Random Geometric Graphs and Their Mobility Properties,‚Äù A. Silva, G. Tucci. Proc. of the The 7th International Conference on Mobile Ad-hoc and Sensor Networks (MSN), Beijing, China, December 16-18, 2011. Second Best Paper Award.\n‚ÄúOptimal base station placement based on interference gradient: the downlink case,‚Äù S. Malik, A. Silva, J-M. Kelif. Proc. of the 5th International Conference on Performance Evaluation Methodologies and Tools (Valuetools) 2011, Cachan, France, May 16-20, 2011.\n‚ÄúSpatial games combining base station placement and mobile association: the downlink case,‚Äù A. Silva, H. Tembine, E. Altman, M. Debbah. Proc. of the 49th IEEE Conference on Decision and Control (CDC), Atlanta, GA, USA, December 15-17, 2010.\n‚ÄúUplink Spatial Games on Cellular Networks,‚Äù A. Silva, H. Tembine, M. Debbah, E. Altman. Proc. of the 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), Monticello, IL, USA, Sep.¬†29-Oct.¬†1, 2010.\n‚ÄúContinuum Equilibria and Global Optimization for Routing in Dense Static Ad Hoc Networks,‚Äù A. Silva, E. Altman, P. Bernhard, M. Debbah. Computer Networks, Volume 54, Issue 6, Pages 1005-1018, April 2010, ISSN: 1389-1286, DOI: 10.1016/j.comnet.2009.10.019.\n‚ÄúMagnetworks: how mobility impacts the design of mobile networks.‚Äù A. Silva, E. Altman, M. Debbah, G. Alfano. Proc. of the 29th IEEE INFOCOM 2010, San Diego, CA, USA, March 15-19, 2010.\n‚ÄúCongestion in Randomly Deployed Wireless Ad Hoc/Sensor Networks,‚Äù A. Silva, P. Reyes, M. Debbah. Proc. of International Conference on Ultra Modern Telecommunications (ICUMT), St-Petersburg, Russia, October 12-14, 2009.\n‚ÄúStochastic Games with One-Step Delay Sharing Information Pattern with Application to Power Control,‚Äù E. Altman, V. Kamble, A. Silva. Proc. of Gamenets 2009, Istanbul, Turkey, May 13-15, 2009.\n‚ÄúNumerical Solutions of Continuum Equilibria for Routing in Dense Ad Hoc Networks,‚Äù A. Silva, P. Bernhard, E. Altman. Proc. of the 3rd International Conference on Performance Evaluation Methodologies and Tools (Valuetools) 2008, Workshop Inter-Perf, Athens, Greece, October 20-24, 2008.\n‚ÄúThe Space Frontier: Physical Limits of Multiple Antenna Information Transfer,‚Äù R. Couillet, S. Wagner, M. Debbah, A. Silva. Proc. of the 3rd International Conference on Performance Evaluation Methodologies and Tools (Valuetools) 2008, Workshop Inter-Perf, Athens, Greece, October 20-24, 2008. Best Student Paper Award.\n‚ÄúThe Mathematics of Routing in Massively Dense Ad-Hoc Networks,‚Äù P. Bernhard, E. Altman, A. Silva. Proc. of AdHoc-NOW Conference, Sophia-Antipolis, France, September 10-13, 2008.\n‚ÄúContinuum Equilibria for Routing in Dense Ad-Hoc Networks,‚Äù E. Altman, A. Silva, P. Bernhard, M. Debbah. Proc. of 45th Annual Allerton Conference on Communication, Control, and Computing (Allerton), Monticello, IL, USA, September 26-28, 2007."
  },
  {
    "objectID": "blog/posts/2024-04-23-Build_a_basic_widget_with_AnyWidget_in_a_Solara_app/2024-04-23-Build_a_basic_widget_with_AnyWidget_in_a_Solara_app.html",
    "href": "blog/posts/2024-04-23-Build_a_basic_widget_with_AnyWidget_in_a_Solara_app/2024-04-23-Build_a_basic_widget_with_AnyWidget_in_a_Solara_app.html",
    "title": "Build a basic widget with AnyWidget in a Solara app",
    "section": "",
    "text": "Build a basic widget with AnyWidget in a Solara app\nLet‚Äôs build the app below (try it out by clicking on the button and moving the slider).\n\n\n\n        \n        \n\n\nAnyWidget is a Python library that simplifies creating and publishing custom Jupyter Widgets. Since Jupyter Widgets have a VIP treatment in Solara ‚òÄÔ∏è, we expect them to work especially well there. This is indeed the case.\nFirst things first, let‚Äôs install AnyWidget and Solara ‚òÄÔ∏è.\n$ pip install anywidget solara\nLet‚Äôs now take the starting example from AnyWidget:\n\nimport anywidget\nimport traitlets\n\nclass CounterWidget(anywidget.AnyWidget):\n    _esm = \"\"\"\n    function render({ model, el }) {\n      let getCount = () =&gt; model.get(\"count\");\n      let button = document.createElement(\"button\");\n      button.classList.add(\"counter-button\");\n      button.innerHTML = `count is ${getCount()}`;\n      button.addEventListener(\"click\", () =&gt; {\n        model.set(\"count\", getCount() + 1);\n        model.save_changes();\n      });\n      model.on(\"change:count\", () =&gt; {\n        button.innerHTML = `count is ${getCount()}`;\n      });\n      el.appendChild(button);\n    }\n    export default { render };\n    \"\"\"\n    _css=\"\"\"\n    .counter-button { background-color: #ea580c; }\n    .counter-button:hover { background-color: #9a3412; }\n    \"\"\"\n    count = traitlets.Int(0).tag(sync=True)\n\nWe can create our Solara app by adding just a few lines of code:\n\nimport anywidget\nimport traitlets\n\nclass CounterWidget(anywidget.AnyWidget):\n    _esm = \"\"\"\n    function render({ model, el }) {\n      let getCount = () =&gt; model.get(\"count\");\n      let button = document.createElement(\"button\");\n      button.classList.add(\"counter-button\");\n      button.innerHTML = `count is ${getCount()}`;\n      button.addEventListener(\"click\", () =&gt; {\n        model.set(\"count\", getCount() + 1);\n        model.save_changes();\n      });\n      model.on(\"change:count\", () =&gt; {\n        button.innerHTML = `count is ${getCount()}`;\n      });\n      el.appendChild(button);\n    }\n    export default { render };\n    \"\"\"\n    _css=\"\"\"\n    .counter-button { background-color: #ea580c; }\n    .counter-button:hover { background-color: #9a3412; }\n    \"\"\"\n    count = traitlets.Int(0).tag(sync=True)\n\nimport solara\n@solara.component\ndef Page():\n    with solara.Column(style={\"padding\":\"30px\"}):\n        solara.Markdown(\"#Anywidget+Solara\")\n        CounterWidget.element()\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\nFrom Solara documentation, we know that in Solara, we should not create widgets, but elements instead. We can create elements by using the .element(...) method (as we did above). This method takes the same arguments as the widget constructor, but returns an element instead of a widget. The element can be used in the same way as a widget, but it is not a widget. It is a special object that can be used in Solara.\nTo make it more interesting, let‚Äôs modify our CounterWidget by changing the _css and adding some confetti from the canvas-confetti javascript package.\n\nimport anywidget\nimport traitlets\n\nclass CounterWidget(anywidget.AnyWidget):\n    _esm = \"\"\"\n    import confetti from \"https://esm.sh/canvas-confetti@1.6.0\"\n    function render({ model, el }) {\n      let getCount = () =&gt; model.get(\"count\");\n      let button = document.createElement(\"button\");\n      button.classList.add(\"counter-button\");\n      button.innerHTML = `count is ${getCount()}`;\n      button.addEventListener(\"click\", () =&gt; {\n        model.set(\"count\", getCount() + 1);\n        model.save_changes();\n      });\n      model.on(\"change:count\", () =&gt; {\n        button.innerHTML = `count is ${getCount()}`;\n        confetti({ angle: getCount() });\n      });\n      el.appendChild(button);\n    }\n    export default { render };\n    \"\"\"\n    _css=\"\"\"\n    .counter-button { background:blue; padding:10px 50px;}\n    .counter-button:hover { background-color:green; }\n    \"\"\"\n    count = traitlets.Int(0).tag(sync=True)\n\nimport solara\n@solara.component\ndef Page():\n    with solara.Column(style={\"padding\":\"30px\"}):\n        solara.Markdown(\"#Anywidget+Solara\")\n        CounterWidget.element()\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\nIf you want to access the value of the CounterWidget counter, we can do it through a reactive variable counter (thanks to Jonathan and Maarten for this suggestion):\n\nimport anywidget\nimport traitlets\n\nclass CounterWidget(anywidget.AnyWidget):\n    _esm = \"\"\"\n    import confetti from \"https://esm.sh/canvas-confetti@1.6.0\"\n    function render({ model, el }) {\n      let getCount = () =&gt; model.get(\"count\");\n      let button = document.createElement(\"button\");\n      button.classList.add(\"counter-button\");\n      button.innerHTML = `count is ${getCount()}`;\n      button.addEventListener(\"click\", () =&gt; {\n        model.set(\"count\", getCount() + 1);\n        model.save_changes();\n      });\n      model.on(\"change:count\", () =&gt; {\n        button.innerHTML = `count is ${getCount()}`;\n        confetti({ angle: getCount() });\n      });\n      el.appendChild(button);\n    }\n    export default { render };\n    \"\"\"\n    _css=\"\"\"\n    .counter-button { background:blue; padding:10px 50px;}\n    .counter-button:hover { background-color:green; }\n    \"\"\"\n    count = traitlets.Int(0).tag(sync=True)\n\nimport solara\ncounter = solara.reactive(0)\n@solara.component\ndef Page():\n    with solara.Column(style={\"padding\":\"30px\"}):\n        solara.Markdown(\"#Anywidget+Solara\")\n        CounterWidget.element(count=counter.value, on_count=counter.set)\n        solara.Markdown(f\"## Counter value is {counter.value}\")\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\nWe can add a slider from Jupyter Widgets and link it to the reactive variable counter.\n\nimport anywidget\nimport traitlets\n\nclass CounterWidget(anywidget.AnyWidget):\n    _esm = \"\"\"\n    import confetti from \"https://esm.sh/canvas-confetti@1.6.0\"\n    function render({ model, el }) {\n      let getCount = () =&gt; model.get(\"count\");\n      let button = document.createElement(\"button\");\n      button.classList.add(\"counter-button\");\n      button.innerHTML = `count is ${getCount()}`;\n      button.addEventListener(\"click\", () =&gt; {\n        model.set(\"count\", getCount() + 1);\n        model.save_changes();\n      });\n      model.on(\"change:count\", () =&gt; {\n        button.innerHTML = `count is ${getCount()}`;\n        confetti({ angle: getCount() });\n      });\n      el.appendChild(button);\n    }\n    export default { render };\n    \"\"\"\n    _css=\"\"\"\n    .counter-button { background:blue; padding:10px 50px;}\n    .counter-button:hover { background-color:green; }\n    \"\"\"\n    count = traitlets.Int(0).tag(sync=True)\n\nimport solara\nimport ipywidgets as widgets\ncounter = solara.reactive(0)\n@solara.component\ndef Page():\n    with solara.Column(style={\"padding\":\"30px\"}):\n        solara.Markdown(\"#Anywidget+Solara\")\n        CounterWidget.element(count=counter.value, on_count=counter.set)\n        widgets.IntSlider.element(min=-180, max=180, value=counter.value, on_value=counter.set)\n        solara.Markdown(f\"## Counter value is {counter.value}\")\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\nAnd that‚Äôs all. You can generate some other confetti animations from here. Create your own widgets with AnyWidget or use widgets from Jupyter Widgets or Solara itself and add them to your Solara app."
  },
  {
    "objectID": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html",
    "href": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html",
    "title": "Understanding Chat Templates",
    "section": "",
    "text": "If you have worked with LLMs, you might have worked with lists of messages. Indeed, when you send a request to an LLM, we can call it a user messsage and the response can be called an assistant message. A conversation would consist of a list of a user message followed by an assistant message followed by a user message followed by an assistant message, etc. However, an LLM takes one text and outputs another text, so you might be wondering what‚Äôs the input text that‚Äôs being passed to the LLM. So what‚Äôs going on?\nA user message will be something like this:\n\nuser_message = [{\"role\": \"user\", \"content\": \"Hi!\"}]\n\nYou can pass the user message to the LLM to obtain an assistant message (it will take a few minutes the first time since it will download a small LLM):\n\n\n\n        \n        \n\n\nThe assistant message will be something like this:\n\nassistant_message = [{\"role\": \"user\", \"content\": \"Hello! How can I assist you today?\"}]\n\nTo have a conversation, you can pass it a list of messages:\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hi!\"},\n    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n    {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n]\n\nHow do we convert a list of messages to an input text?\nThe first time I encountered this was with Thomas Capelle from W&B. Our conversation went something like this:\n\nThomas: An LLM has no idea about user or assistant messages, it is just an autocompletion program.\nAlonso: So what‚Äôs going on with the list of messages I‚Äôm sending it? Are they just concatenated or what?\nThomas: No, no, no, it‚Äôs more complex than that, specially when you work with tools. You should check the model‚Äôs chat template.\nAlonso: How do I do that?\nThomas: It‚Äôs stored in the tokenizer, let me show you."
  },
  {
    "objectID": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#introduction",
    "href": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#introduction",
    "title": "Understanding Chat Templates",
    "section": "",
    "text": "If you have worked with LLMs, you might have worked with lists of messages. Indeed, when you send a request to an LLM, we can call it a user messsage and the response can be called an assistant message. A conversation would consist of a list of a user message followed by an assistant message followed by a user message followed by an assistant message, etc. However, an LLM takes one text and outputs another text, so you might be wondering what‚Äôs the input text that‚Äôs being passed to the LLM. So what‚Äôs going on?\nA user message will be something like this:\n\nuser_message = [{\"role\": \"user\", \"content\": \"Hi!\"}]\n\nYou can pass the user message to the LLM to obtain an assistant message (it will take a few minutes the first time since it will download a small LLM):\n\n\n\n        \n        \n\n\nThe assistant message will be something like this:\n\nassistant_message = [{\"role\": \"user\", \"content\": \"Hello! How can I assist you today?\"}]\n\nTo have a conversation, you can pass it a list of messages:\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hi!\"},\n    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n    {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n]\n\nHow do we convert a list of messages to an input text?\nThe first time I encountered this was with Thomas Capelle from W&B. Our conversation went something like this:\n\nThomas: An LLM has no idea about user or assistant messages, it is just an autocompletion program.\nAlonso: So what‚Äôs going on with the list of messages I‚Äôm sending it? Are they just concatenated or what?\nThomas: No, no, no, it‚Äôs more complex than that, specially when you work with tools. You should check the model‚Äôs chat template.\nAlonso: How do I do that?\nThomas: It‚Äôs stored in the tokenizer, let me show you."
  },
  {
    "objectID": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#the-chat-template",
    "href": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#the-chat-template",
    "title": "Understanding Chat Templates",
    "section": "The Chat Template",
    "text": "The Chat Template\nThe chat template takes as input a list of messages (and tools but we will talk about it later) and convert them into a single string. Let‚Äôs see what it does with an example.\n\n\n\n        \n        \n\n\nYou should get the following:\n&lt;|im_start|&gt;system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nHi!&lt;|im_end|&gt;\nNotice that when you don‚Äôt provide a system message, the model Qwen/Qwen2.5-0.5B-Instruct adds a system message (‚ÄúYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.‚Äù). Perhaps you want to change that to a different system message:\n\n\n\n        \n        \n\n\nWe can also see how does the chat template convert a conversation like this one:\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hi!\"},\n    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n    {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n]\n\n\n\n\n        \n        \n\n\nEach model has its own chat template. Let‚Äôs take a look at Mistral-7B-v0.3 chat template:\n\n\n\n        \n        \n\n\nYou should get:\n&lt;s&gt;[INST] Hi! [/INST]Hello! How can I assist you today?&lt;/s&gt;[INST] What's the capital of France? [/INST]\nSince after sending a user message, you expect an assistant message, you can help the model by basically saying ‚ÄúNow, it‚Äôs your turn!‚Äù. This is so useful that it has been incorporated into the chat template itself.\nThe instruction is:\ntokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n\n\n        \n        \n\n\nYou should get:\n&lt;|im_start|&gt;system\nYou are a helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nHi!&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nWhat‚Äôs interesting to me is that when you suggest to force a tool call or to force a tool, they look at you thinking you‚Äôre crazy even though it‚Äôs exactly the same thing (and it‚Äôs probably what OpenAI already does with the tool_choice=required and tool_choice: {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}."
  },
  {
    "objectID": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#tool-calls",
    "href": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#tool-calls",
    "title": "Understanding Chat Templates",
    "section": "Tool calls",
    "text": "Tool calls\nThe chat template also handles tool calls. That means that we can provide a list of tools (let‚Äôs do one as an example):\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"Python_REPL\",\n            \"description\": \"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n            \"parameters\": {\n                \"properties\": {\n                    \"python_code\": {\n                        \"description\": \"Valid python command.\",\n                        \"type\": \"string\",\n                    }\n                },\n                \"required\": [\"python_code\"],\n                \"type\": \"object\",\n            },\n        },\n    }\n]\n\nThe instruction is:\ntokenizer.apply_chat_template(messages, tokenize=False, tools=tools, add_generation_prompt=True)\n\n\n\n        \n        \n\n\nYou should get the following:\n&lt;|im_start|&gt;system\nYou are a helpful assistant.\n\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags:\n&lt;tools&gt;\n{\"type\": \"function\", \"function\": {\"name\": \"Python_REPL\", \"description\": \"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\", \"parameters\": {\"properties\": {\"python_code\": {\"description\": \"Valid python command.\", \"type\": \"string\"}}, \"required\": [\"python_code\"], \"type\": \"object\"}}}\n&lt;/tools&gt;\n\nFor each function call, return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags:\n&lt;tool_call&gt;\n{\"name\": &lt;function-name&gt;, \"arguments\": &lt;args-json-object&gt;}\n&lt;/tool_call&gt;&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nWhat's 2 to the power of 5?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nQuite complex string indeed.\nIf you look at some other model NousResearch/Hermes-3-Llama-3.1-8B, you see the following:\n\n\n\n        \n        \n\n\nYou should get the following:\n&lt;|begin_of_text|&gt;&lt;|im_start|&gt;system\nYou are a function calling AI model. You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: &lt;tools&gt; {\"type\": \"function\", \"function\": {\"name\": \"Python_REPL\", \"description\": \"Python_REPL(python_code: str) - A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\n\n    Args:\n        python_code(str): Valid python command.\", \"parameters\": {\"properties\": {\"python_code\": {\"description\": \"Valid python command.\", \"type\": \"string\"}}, \"required\": [\"python_code\"], \"type\": \"object\"}} &lt;/tools&gt;Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}, \"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}}, \"required\": [\"name\", \"arguments\"], \"title\": \"FunctionCall\", \"type\": \"object\"}}\nFor each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n&lt;tool_call&gt;\n{\"name\": &lt;function-name&gt;, \"arguments\": &lt;args-dict&gt;}\n&lt;/tool_call&gt;&lt;|im_end|&gt;\n&lt;|im_start|&gt;system\nYou are a helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nWhat's 2 to the power of 5?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nI don‚Äôt like this chat template. It appears that our messages have two different and consecutive system prompts. I prefer much more the previous chat template of the model ‚ÄúQwen/Qwen2.5-0.5B-Instruct‚Äù."
  },
  {
    "objectID": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#thinking-mode",
    "href": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#thinking-mode",
    "title": "Understanding Chat Templates",
    "section": "Thinking mode",
    "text": "Thinking mode\nA recent addition is the enable_thinking in some new reasoning models where the model will ‚Äúthink‚Äù between the XML tags &lt;think&gt;...&lt;/think&gt;. For example in Qwen/Qwen3-4B the model has the possibility to reason (which is the default), but you can turn this option off if you want to. The instruction is:\ntokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False)\n\n\n\n        \n        \n\n\nYou should get:\n&lt;|im_start|&gt;system\nYou are a helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nHow many r's in strawberry?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;think&gt;\n\n&lt;/think&gt;\nI saw some tweets (with multiple retweets) claiming they found a ‚Äúhack‚Äù to make the model not think and it was appending &lt;think&gt;\\n\\n&lt;/think&gt; while this is exactly what the chat template does!!!"
  },
  {
    "objectID": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#how-does-the-chat-template-handles-this",
    "href": "blog/posts/2025-06-20-Understanding-Chat-Templates/UnderstandingChatTemplates.html#how-does-the-chat-template-handles-this",
    "title": "Understanding Chat Templates",
    "section": "How does the chat template handles this?",
    "text": "How does the chat template handles this?\nThe chat template has been programmed in Jinja which is usually used in web development. You can see the chat template with the following command:\n\n\n\n        \n        \n\n\nYou should see the following:\n{%- if tools %}\n    {{- '&lt;|im_start|&gt;system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags:\\n&lt;tools&gt;\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n&lt;/tools&gt;\\n\\nFor each function call, return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags:\\n&lt;tool_call&gt;\\n{\\\"name\\\": &lt;function-name&gt;, \\\"arguments\\\": &lt;args-json-object&gt;}\\n&lt;/tool_call&gt;&lt;|im_end|&gt;\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '&lt;|im_start|&gt;system\\n' + messages[0].content + '&lt;|im_end|&gt;\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith('&lt;tool_response&gt;') and message.content.endswith('&lt;/tool_response&gt;')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '&lt;|im_start|&gt;' + message.role + '\\n' + content + '&lt;|im_end|&gt;' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '&lt;/think&gt;' in content %}\n                {%- set reasoning_content = content.split('&lt;/think&gt;')[0].rstrip('\\n').split('&lt;think&gt;')[-1].lstrip('\\n') %}\n                {%- set content = content.split('&lt;/think&gt;')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 &gt; ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '&lt;|im_start|&gt;' + message.role + '\\n&lt;think&gt;\\n' + reasoning_content.strip('\\n') + '\\n&lt;/think&gt;\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '&lt;|im_start|&gt;' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '&lt;|im_start|&gt;' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '&lt;tool_call&gt;\\n{\"name\": \"' }}\n                {{- tool_call.name }}\n                {{- '\", \"arguments\": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n&lt;/tool_call&gt;' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '&lt;|im_end|&gt;\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '&lt;|im_start|&gt;user' }}\n        {%- endif %}\n        {{- '\\n&lt;tool_response&gt;\\n' }}\n        {{- content }}\n        {{- '\\n&lt;/tool_response&gt;' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '&lt;|im_end|&gt;\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '&lt;|im_start|&gt;assistant\\n' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- '&lt;think&gt;\\n\\n&lt;/think&gt;\\n\\n' }}\n    {%- endif %}\n{%- endif %}\nAfter spending some time understanding what‚Äôs going on, you can create your own if you want to change its behavior."
  },
  {
    "objectID": "blog/posts/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara.html",
    "href": "blog/posts/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara.html",
    "title": "Build a basic LLM chat app with Solara",
    "section": "",
    "text": "In this post, we will build a basic LLM chat app with Solara. Large Language Models (LLMs) have become increasingly popular and Solara provides several components to work with them. Let‚Äôs dive in.\nFirst things first, let‚Äôs install Solara.\n$ pip install solara\nNow, let‚Äôs start by creating an app.py that sends a simple message with the content ‚ÄúHello!‚Äù as a user. To do that we use the ChatBox and ChatMessage components.\n\nimport solara\n@solara.component\ndef Page():\n    with solara.lab.ChatBox():\n        with solara.lab.ChatMessage(user=True, name=\"User\"):\n            solara.Markdown(\"Hello!\")\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\nYou can modify the user name and/or the message as you please.\n\nimport solara\n@solara.component\ndef Page():\n    with solara.lab.ChatBox():\n        with solara.lab.ChatMessage(user=True, name=\"Morpheus\"):\n            solara.Markdown(\"Wake up, Neo...\")\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\nYou can also send a message as an assistant.\n\nimport solara\n@solara.component\ndef Page():\n    with solara.lab.ChatBox():\n        with solara.lab.ChatMessage(user=False, name=\"Assistant\",):\n            solara.Markdown(\"Hello! How can I assist you today?\")\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\nTo have a conversation, we create a reactive variable messages where we will store the messages. To do that we create a list of dictionaries where we will save the roles (for example, user and assistant) and the messages contents.\n\nimport solara\nfrom typing import List\nfrom typing_extensions import TypedDict\n\nclass MessageDict(TypedDict):\n    role: str\n    content: str\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n\nWe can generate a conversation by adding messages to the reactive variable messages that we previously created and displaying each message one by one.\n\n@solara.component\ndef Page():\n    messages.value = [\n        {\"role\": \"user\", \"content\": \"Hello!\"}, \n        {\"role\": \"assistant\",  \"content\": \"Hello! How can I assist you today?\"},\n    ]\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"Assistant\"\n            ):\n                solara.Markdown(item[\"content\"])\nPage()\n\n\n\n\n\n\n\n        \n        \n\n\nLet‚Äôs now add the possibility to receive messages from the user by adding the ChatInput component and a send function that adds the message to the conversation.\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"Assistant\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\nTry it out by sending a message.\n\n\n\n        \n        \n\n\n\n\nUp to now we are only displaying the message the user sent. Let‚Äôs first simulate a conversation by replying exactly the same message we receive from the user. To do that we need to add a response function and a result function that will reply the last message (which will be the one sent by the user) and it will be activated once every time the counter user_message_count changes.\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": message}]\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"EchoBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\nThe complete code can be found below.\n\n\nShow the code\nimport solara\nfrom typing import List\nfrom typing_extensions import TypedDict\n\nclass MessageDict(TypedDict):\n    role: str\n    content: str\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": message}]\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"EchoBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\n\nUp to now, our EchoBot application looks like this. Try it out!\n\n\n\n        \n        \n\n\n\n\n\nLet‚Äôs now build a Bot that will stream a response message. Let‚Äôs first emulate a streamed response with a function that we call response_generator.\n\n# Streamed response emulator\nimport time\nimport random\ndef response_generator():\n    response = random.choice(\n        [\n            \"Hello! How can I assist you today?\",\n            \"Hello! If you have any questions or need help with something, feel free to ask.\",\n        ]\n    )\n    for word in response.split():\n        yield word + \" \"\n        time.sleep(0.05)\n\nLet‚Äôs see that it‚Äôs working as expected.\n\nfor chunk in response_generator():\n    print(chunk)\n\nHello! \nHow \ncan \nI \nassist \nyou \ntoday? \n\n\nIt works. Notice that for the moment the response_generator function will give one of the two possible responses at random without considering the user message.\nLet‚Äôs now create a function that will be adding the chunks successively to the message.\n\ndef add_chunk_to_ai_message(chunk: str):\n    messages.value = [\n        *messages.value[:-1],\n        {\n            \"role\": \"assistant\",\n            \"content\": messages.value[-1][\"content\"] + chunk,\n        },\n    ]\n\nWe need to modify the EchoBot code to include this functionality as follows.\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator():\n            add_chunk_to_ai_message(chunk)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"StreamBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\nThe complete code can be found below.\n\n\nShow the code\nimport solara\nimport time\nimport random\nfrom typing import List\nfrom typing_extensions import TypedDict\n\nclass MessageDict(TypedDict):\n    role: str\n    content: str\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n\n# Streamed response emulator\ndef response_generator():\n    response = random.choice(\n        [\n            \"Hello! How can I assist you today?\",\n            \"Hello! If you have any questions or need help with something, feel free to ask.\",\n        ]\n    )\n    for word in response.split():\n        yield word + \" \"\n        time.sleep(0.05)\n\ndef add_chunk_to_ai_message(chunk: str):\n    messages.value = [\n        *messages.value[:-1],\n        {\n            \"role\": \"assistant\",\n            \"content\": messages.value[-1][\"content\"] + chunk,\n        },\n    ]\n    \nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator():\n            add_chunk_to_ai_message(chunk)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"StreamBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\n\nOur StreamBot application looks like this. Try it out!\n\n\n\n        \n        \n\n\n\n\n\nThe StreamBot application don‚Äôt take into account the user message. To reply something coherent, let‚Äôs use one of OpenAI models (in this example, gpt-3.5-turbo).\nFirst, obtain an OPENAI_API_KEY=sk-... and replace it below.\n\nimport os\nimport openai\nfrom openai import OpenAI\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\nclient = OpenAI()\n\nNow we can define a new response_generator function that will use OpenAI to give a coherent answer.\n\ndef response_generator(message):\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n           {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n           {\"role\": \"user\", \"content\": message}\n        ],\n        stream=True\n    )\n\nLet‚Äôs see that it works (as you can see in the code, we need to add some cleaning to the chunks and verify they are not None).\n\nfor chunk in response_generator(\"Hello!\"):\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content)\n\n\nHello\n!\n How\n can\n I\n assist\n you\n today\n?\n\n\nWe need to modify the StreamBot code as follows.\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator(message):\n            if chunk.choices[0].delta.content is not None:\n                add_chunk_to_ai_message(chunk.choices[0].delta.content)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"ChatGPT\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\nThe complete code can be found below.\n\n\nShow the code\nimport solara\nfrom typing import List\nfrom typing_extensions import TypedDict\nimport os\nimport openai\nfrom openai import OpenAI\n\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\nclient = OpenAI()\n\ndef response_generator(message):\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n           {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n           {\"role\": \"user\", \"content\": message}\n        ],\n        stream=True\n    )\n\nclass MessageDict(TypedDict):\n    role: str\n    content: str\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator(message):\n            if chunk.choices[0].delta.content is not None:\n                add_chunk_to_ai_message(chunk.choices[0].delta.content)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"ChatGPT\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()"
  },
  {
    "objectID": "blog/posts/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara.html#echobot",
    "href": "blog/posts/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara.html#echobot",
    "title": "Build a basic LLM chat app with Solara",
    "section": "",
    "text": "Up to now we are only displaying the message the user sent. Let‚Äôs first simulate a conversation by replying exactly the same message we receive from the user. To do that we need to add a response function and a result function that will reply the last message (which will be the one sent by the user) and it will be activated once every time the counter user_message_count changes.\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": message}]\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"EchoBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\nThe complete code can be found below.\n\n\nShow the code\nimport solara\nfrom typing import List\nfrom typing_extensions import TypedDict\n\nclass MessageDict(TypedDict):\n    role: str\n    content: str\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": message}]\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"EchoBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\n\nUp to now, our EchoBot application looks like this. Try it out!"
  },
  {
    "objectID": "blog/posts/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara.html#streambot",
    "href": "blog/posts/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara.html#streambot",
    "title": "Build a basic LLM chat app with Solara",
    "section": "",
    "text": "Let‚Äôs now build a Bot that will stream a response message. Let‚Äôs first emulate a streamed response with a function that we call response_generator.\n\n# Streamed response emulator\nimport time\nimport random\ndef response_generator():\n    response = random.choice(\n        [\n            \"Hello! How can I assist you today?\",\n            \"Hello! If you have any questions or need help with something, feel free to ask.\",\n        ]\n    )\n    for word in response.split():\n        yield word + \" \"\n        time.sleep(0.05)\n\nLet‚Äôs see that it‚Äôs working as expected.\n\nfor chunk in response_generator():\n    print(chunk)\n\nHello! \nHow \ncan \nI \nassist \nyou \ntoday? \n\n\nIt works. Notice that for the moment the response_generator function will give one of the two possible responses at random without considering the user message.\nLet‚Äôs now create a function that will be adding the chunks successively to the message.\n\ndef add_chunk_to_ai_message(chunk: str):\n    messages.value = [\n        *messages.value[:-1],\n        {\n            \"role\": \"assistant\",\n            \"content\": messages.value[-1][\"content\"] + chunk,\n        },\n    ]\n\nWe need to modify the EchoBot code to include this functionality as follows.\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator():\n            add_chunk_to_ai_message(chunk)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"StreamBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\nThe complete code can be found below.\n\n\nShow the code\nimport solara\nimport time\nimport random\nfrom typing import List\nfrom typing_extensions import TypedDict\n\nclass MessageDict(TypedDict):\n    role: str\n    content: str\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n\n# Streamed response emulator\ndef response_generator():\n    response = random.choice(\n        [\n            \"Hello! How can I assist you today?\",\n            \"Hello! If you have any questions or need help with something, feel free to ask.\",\n        ]\n    )\n    for word in response.split():\n        yield word + \" \"\n        time.sleep(0.05)\n\ndef add_chunk_to_ai_message(chunk: str):\n    messages.value = [\n        *messages.value[:-1],\n        {\n            \"role\": \"assistant\",\n            \"content\": messages.value[-1][\"content\"] + chunk,\n        },\n    ]\n    \nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator():\n            add_chunk_to_ai_message(chunk)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"StreamBot\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\n\nOur StreamBot application looks like this. Try it out!"
  },
  {
    "objectID": "blog/posts/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara.html#chatgpt-bot",
    "href": "blog/posts/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara/2024-04-19-Build_a_basic_LLM_chat_app_with_Solara.html#chatgpt-bot",
    "title": "Build a basic LLM chat app with Solara",
    "section": "",
    "text": "The StreamBot application don‚Äôt take into account the user message. To reply something coherent, let‚Äôs use one of OpenAI models (in this example, gpt-3.5-turbo).\nFirst, obtain an OPENAI_API_KEY=sk-... and replace it below.\n\nimport os\nimport openai\nfrom openai import OpenAI\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\nclient = OpenAI()\n\nNow we can define a new response_generator function that will use OpenAI to give a coherent answer.\n\ndef response_generator(message):\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n           {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n           {\"role\": \"user\", \"content\": message}\n        ],\n        stream=True\n    )\n\nLet‚Äôs see that it works (as you can see in the code, we need to add some cleaning to the chunks and verify they are not None).\n\nfor chunk in response_generator(\"Hello!\"):\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content)\n\n\nHello\n!\n How\n can\n I\n assist\n you\n today\n?\n\n\nWe need to modify the StreamBot code as follows.\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator(message):\n            if chunk.choices[0].delta.content is not None:\n                add_chunk_to_ai_message(chunk.choices[0].delta.content)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"ChatGPT\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()\n\n\n\n\nThe complete code can be found below.\n\n\nShow the code\nimport solara\nfrom typing import List\nfrom typing_extensions import TypedDict\nimport os\nimport openai\nfrom openai import OpenAI\n\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\nclient = OpenAI()\n\ndef response_generator(message):\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n           {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n           {\"role\": \"user\", \"content\": message}\n        ],\n        stream=True\n    )\n\nclass MessageDict(TypedDict):\n    role: str\n    content: str\n\nmessages: solara.Reactive[List[MessageDict]] = solara.reactive([])\n@solara.component\ndef Page():\n    user_message_count = len([m for m in messages.value if m[\"role\"] == \"user\"])\n    def send(message):\n        messages.value = [*messages.value, {\"role\": \"user\", \"content\": message}]\n    def response(message):\n        messages.value = [*messages.value, {\"role\": \"assistant\", \"content\": \"\"}]\n        for chunk in response_generator(message):\n            if chunk.choices[0].delta.content is not None:\n                add_chunk_to_ai_message(chunk.choices[0].delta.content)\n    def result():\n        if messages.value != []:\n            response(messages.value[-1][\"content\"])\n    result = solara.lab.use_task(result, dependencies=[user_message_count])\n    with solara.lab.ChatBox():\n        for item in messages.value:\n            with solara.lab.ChatMessage(\n                user=item[\"role\"] == \"user\",\n                name=\"User\" if item[\"role\"] == \"user\" else \"ChatGPT\"\n            ):\n                solara.Markdown(item[\"content\"])\n    solara.lab.ChatInput(send_callback=send)\nPage()"
  },
  {
    "objectID": "blog/posts/2025-07-05-Understanding-Function-Calling/Understanding_Function_Calling.html",
    "href": "blog/posts/2025-07-05-Understanding-Function-Calling/Understanding_Function_Calling.html",
    "title": "Understanding Function Calling",
    "section": "",
    "text": "Language models take text as input and then predict which text should come next. Given that information, what does function calling even mean?"
  },
  {
    "objectID": "blog/posts/2025-07-05-Understanding-Function-Calling/Understanding_Function_Calling.html#basic-example",
    "href": "blog/posts/2025-07-05-Understanding-Function-Calling/Understanding_Function_Calling.html#basic-example",
    "title": "Understanding Function Calling",
    "section": "Basic example",
    "text": "Basic example\nLet‚Äôs start with a basic example. Imagine that we ask a language model to perform the multiplication of 1234567 times 8765432 whose result is:\n\nprint(f\"{1234567*8765432:,}\")\n\n10,821,513,087,944\n\n\nThis is the answer of the language model:\n\n\nShow the code\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"Qwen/Qwen3-0.6B\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, cache_dir=\"/big_storage/llms/hf_models/\"\n).to(\"cuda\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What's 1234567 times 8765432?\"},\n]\n\nprompt = tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True, enable_thinking=False\n)\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs[\"input_ids\"].shape[1]\noutputs = model.generate(\n    **model_inputs,\n    max_new_tokens=1024,\n    do_sample=False,\n    temperature=1.0,\n    top_p=1.0,\n    top_k=50,\n)\nassistant_response = tokenizer.decode(outputs[0][prompt_length:])\nprint(assistant_response)\n\n\nTo find the product of **1234567 √ó 8765432**, we can use a calculator or a multiplication table. However, since this is a large number, it's best to use a calculator or a computational tool.\n\n### Final Answer:\n$$\n1234567 \\times 8765432 = \\boxed{1099999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999\n\n\nThis model is not particularly good but even GPT-4o-mini fails without using function calling:\n\nHere is the link to that conversation: https://chatgpt.com/s/t_686901e99a648191815170ed3c83083b\nAn analogy of function calling is that to ask a language model directly to perform a computation is similar to ask someone to make that computation in his head. It‚Äôs hard! However, if we gave him a calculator, he could solve it quite easily. He needs to know only two things:\n\nwhich operation (or function) to use\nwhich numbers (or arguments) to use\n\nThe same is true for doing function calling with a language model. It needs to know which function and which arguments to use.\nLet‚Äôs provide a description of a function to the language model so it knows what‚Äôs the function name and which are the arguments the function expects.\nYou can provide the description manually (after all it‚Äôs just text) but I will use Pydantic to do that. Here is the description of the multiply function\n\n\nShow the code\nfrom pydantic import BaseModel, Field\nimport json\nfrom openai import pydantic_function_tool\n\nclass multiply(BaseModel):\n    \"\"\"Multiply two integers together.\"\"\"\n\n    a: int = Field(..., description=\"First integer\")\n    b: int = Field(..., description=\"Second integer\")\n\ntool = pydantic_function_tool(multiply)\nprint(json.dumps(tool, indent=4))\n\n\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"multiply\",\n        \"strict\": true,\n        \"parameters\": {\n            \"description\": \"Multiply two integers together.\",\n            \"properties\": {\n                \"a\": {\n                    \"description\": \"First integer\",\n                    \"title\": \"A\",\n                    \"type\": \"integer\"\n                },\n                \"b\": {\n                    \"description\": \"Second integer\",\n                    \"title\": \"B\",\n                    \"type\": \"integer\"\n                }\n            },\n            \"required\": [\n                \"a\",\n                \"b\"\n            ],\n            \"title\": \"multiply\",\n            \"type\": \"object\",\n            \"additionalProperties\": false\n        },\n        \"description\": \"Multiply two integers together.\"\n    }\n}\n\n\nAfter we have provided a function/tool description, we can see if the model knows what to do with it:\n\n\nShow the code\nprompt = tokenizer.apply_chat_template(\n    messages, tools=[tool], tokenize=False, add_generation_prompt=True, enable_thinking=False\n)\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs[\"input_ids\"].shape[1]\noutputs = model.generate(\n    **model_inputs,\n    max_new_tokens=1024,\n    do_sample=False,\n    temperature=1.0,\n    top_p=1.0,\n    top_k=50,\n)\nassistant_response = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\nprint(assistant_response)\n\n\n&lt;tool_call&gt;\n{\"name\": \"multiply\", \"arguments\": {\"a\": 1234567, \"b\": 8765432}}\n&lt;/tool_call&gt;\n\n\nEven this small model is able to get correctly:\n\nthe function name (multiply)\nthe arguments to provide to the function (1234567 and 8765432)\n\nNotice that we have not yet implemented the function itself (!), which we might or might not do (as we will see in other examples next).\nWe can get the assistant response and simply do the multiplication ourselves. This is important, what we call function calling in reality is the model telling us which function to call and which arguments to provide to that function. It is up to us to run or not that function. In this case, we will run it:\n\n\nShow the code\nassistant_response_clean = assistant_response.split(\"&lt;tool_call&gt;\")[-1].split(\"&lt;/tool_call&gt;\")[0]\nassistant_response_json = json.loads(assistant_response_clean)\n\ndef execute_function_call(assistant_response_json):\n    if assistant_response_json['name'] == 'multiply':\n        tool_response = assistant_response_json['arguments']['a']*assistant_response_json['arguments']['b']\n    else:\n        tool_response = assistant_response_clean\n    return tool_response\n\ntool_response = execute_function_call(assistant_response_json)\nprint(f\"{tool_response:,}\")\n\n\n10,821,513,087,944\n\n\nThis is great. We already got the correct answer! However, we usually want to have a more ‚Äúhuman‚Äù response. To do that we can append two messages (a message for the function call and another for the tool response:\n\n\nShow the code\nmessages.append({\n    \"role\": \"assistant\",\n    \"content\": \"\",\n    \"function_call\": None,\n    \"tool_calls\": [{\n        \"name\": assistant_response_json[\"name\"],\n        \"arguments\": assistant_response_json[\"arguments\"],\n    }],\n})\n\nmessages.append(\n    {\n        \"role\": \"tool\",\n        \"content\": f\"{tool_response}\",\n    }\n)\n\n\nSince we have full control in this setting, we can actually see what‚Äôs the text the language model is receiving as input:\n\n\nShow the code\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tools=[tool],\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False,\n)\nprint(prompt)\n\n\n&lt;|im_start|&gt;system\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags:\n&lt;tools&gt;\n{\"type\": \"function\", \"function\": {\"name\": \"multiply\", \"strict\": true, \"parameters\": {\"description\": \"Multiply two integers together.\", \"properties\": {\"a\": {\"description\": \"First integer\", \"title\": \"A\", \"type\": \"integer\"}, \"b\": {\"description\": \"Second integer\", \"title\": \"B\", \"type\": \"integer\"}}, \"required\": [\"a\", \"b\"], \"title\": \"multiply\", \"type\": \"object\", \"additionalProperties\": false}, \"description\": \"Multiply two integers together.\"}}\n&lt;/tools&gt;\n\nFor each function call, return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags:\n&lt;tool_call&gt;\n{\"name\": &lt;function-name&gt;, \"arguments\": &lt;args-json-object&gt;}\n&lt;/tool_call&gt;&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nWhat's 1234567 times 8765432?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;tool_call&gt;\n{\"name\": \"multiply\", \"arguments\": {\"a\": 1234567, \"b\": 8765432}}\n&lt;/tool_call&gt;&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n&lt;tool_response&gt;\n10821513087944\n&lt;/tool_response&gt;&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;think&gt;\n\n&lt;/think&gt;\n\n\n\n\nWe can then make a call to the model to provide a ‚Äúhuman‚Äù response:\n\n\nShow the code\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs[\"input_ids\"].shape[1]\noutputs = model.generate(\n    **model_inputs,\n    max_new_tokens=1024,\n    do_sample=False,\n    temperature=1.0,\n    top_p=1.0,\n    top_k=50,\n)\nassistant_response = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\nprint(assistant_response)\n\n\n1234567 √ó 8765432 = 10821513087944\n\n\nOK, this response is correct and more ‚Äúhuman‚Äù. It could be improved but that‚Äôs because it‚Äôs a very small model.\nI hope you realize that even a small language model (in this example, with 0.6B parameters) provided with a function/tool can answer better that question than a powerful model such as GPT-4o-mini.\nThat‚Äôs powerful!"
  },
  {
    "objectID": "blog/posts/2025-07-05-Understanding-Function-Calling/Understanding_Function_Calling.html#language-model-using-python",
    "href": "blog/posts/2025-07-05-Understanding-Function-Calling/Understanding_Function_Calling.html#language-model-using-python",
    "title": "Understanding Function Calling",
    "section": "Language Model Using Python",
    "text": "Language Model Using Python\nIn our basic example, we used a very tailored function because I wanted to show that function calling can use several arguments. We can however provided with a Python REPL which will allow us to use a pletora of tools in Python. Here is a description of a Python REPL:\n\n\nShow the code\nclass Python_REPL(BaseModel):\n    \"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\"\n\n    python_code: str = Field(..., description=\"Valid python command.\")\n\ntool = pydantic_function_tool(Python_REPL)\nprint(json.dumps(tool, indent=4))\n\n\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"Python_REPL\",\n        \"strict\": true,\n        \"parameters\": {\n            \"description\": \"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n            \"properties\": {\n                \"python_code\": {\n                    \"description\": \"Valid python command.\",\n                    \"title\": \"Python Code\",\n                    \"type\": \"string\"\n                }\n            },\n            \"required\": [\n                \"python_code\"\n            ],\n            \"title\": \"Python_REPL\",\n            \"type\": \"object\",\n            \"additionalProperties\": false\n        },\n        \"description\": \"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\"\n    }\n}\n\n\nWe can ask our model to, for example, make a bar plot. In this case, the language model couldn‚Äôt figure out what to do:\n\n\nShow the code\nmessages = [\n    {\"role\": \"user\", \"content\": \"Make a bar plot\"},\n]\n\nprompt = tokenizer.apply_chat_template(\n    messages, tools=[tool], tokenize=False, add_generation_prompt=True, enable_thinking=False\n)\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs[\"input_ids\"].shape[1]\noutputs = model.generate(\n    **model_inputs,\n    max_new_tokens=1024,\n    do_sample=False,\n    temperature=1.0,\n    top_p=1.0,\n    top_k=50,\n)\nassistant_response = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\nprint(assistant_response)\n\n\nI cannot make a bar plot directly, but I can help you create one using Python. Could you please provide the data you want to plot?\n\n\nThe model was unable to call the tool but we can help it by adding it directly to the prompt (notice that this is equivalent to add_generation_prompt=True when we apply the chat template). This is probably what OpenAI does with the option tool_choice=required and we could even impose which tool to call which is probably equivalent to the forced function option that OpenAI provides). Let‚Äôs add the tool call directly to the prompt:\n\n\nShow the code\nprompt += \"&lt;tool_call&gt;\" # added directly to the prompt\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs[\"input_ids\"].shape[1]\noutputs = model.generate(\n    **model_inputs,\n    max_new_tokens=1024,\n    do_sample=False,\n    temperature=1.0,\n    top_p=1.0,\n    top_k=50,\n)\nassistant_response = \"&lt;tool_call&gt;\" + tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True) # it was added to the prompt\nprint(assistant_response)\n\n\n&lt;tool_call&gt;\n{\"name\": \"Python_REPL\", \"arguments\": {\"python_code\": \"import matplotlib.pyplot as plt\\nplt.bar([1, 2, 3], [10, 20, 30])\\nplt.show()\"}}\n&lt;/tool_call&gt;\n\n\nThis small model was able to provide the code to do a bar plot. It is up to us to run or not that code. In this case, we will run it:\n\n\nShow the code\nassistant_response_clean = assistant_response.split(\"&lt;tool_call&gt;\")[-1].split(\"&lt;/tool_call&gt;\")[0]\nassistant_response_json = json.loads(assistant_response_clean)\n\ndef execute_function_call(assistant_response_json):\n    if assistant_response_json['name'] == 'Python_REPL':\n        tool_response = exec(assistant_response_json['arguments']['python_code'])\n    else:\n        tool_response = assistant_response_json\n    return tool_response\n\nexecute_function_call(assistant_response_json)\n\n\n\n\n\n\n\n\n\nBy imposing that the language model needs to call a tool, we cannot let the model to respond to a conversational question. For example to tell us a joke:\n\n\nShow the code\nmessages = [\n    {\"role\": \"user\", \"content\": \"Tell me a joke\"},\n]\n\nprompt = tokenizer.apply_chat_template(\n    messages, tools=[tool], tokenize=False, add_generation_prompt=True, enable_thinking=False\n)\nprompt += \"&lt;tool_call&gt;\" # imposed tool call\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs[\"input_ids\"].shape[1]\noutputs = model.generate(\n    **model_inputs,\n    max_new_tokens=1024,\n    do_sample=False,\n    temperature=1.0,\n    top_p=1.0,\n    top_k=50,\n)\nassistant_response = \"&lt;tool_call&gt;\" + tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\nprint(assistant_response)\n\n\n&lt;tool_call&gt;\n{\"name\": \"Python_REPL\", \"arguments\": {\"python_code\": \"print('Hello, world!')\"}}\n&lt;/tool_call&gt;\n\n\nWe could use a trick. We could provide a ‚Äútool‚Äù to get back that behavior. Let‚Äôs give a description of that ‚Äútool‚Äù:\n\n\nShow the code\nclass ConversationalResponse(BaseModel):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: str = Field(description=\"A conversational response to the user's query\")\ntool_conversational_response = pydantic_function_tool(ConversationalResponse)\nprint(json.dumps(tool_conversational_response, indent=4))\n\n\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"ConversationalResponse\",\n        \"strict\": true,\n        \"parameters\": {\n            \"description\": \"Respond in a conversational manner. Be kind and helpful.\",\n            \"properties\": {\n                \"response\": {\n                    \"description\": \"A conversational response to the user's query\",\n                    \"title\": \"Response\",\n                    \"type\": \"string\"\n                }\n            },\n            \"required\": [\n                \"response\"\n            ],\n            \"title\": \"ConversationalResponse\",\n            \"type\": \"object\",\n            \"additionalProperties\": false\n        },\n        \"description\": \"Respond in a conversational manner. Be kind and helpful.\"\n    }\n}\n\n\n\n\nShow the code\nmessages = [\n    {\"role\": \"user\", \"content\": \"Tell me a joke\"},\n]\n\nprompt = tokenizer.apply_chat_template(\n    messages, tools=[tool, tool_conversational_response], tokenize=False, add_generation_prompt=True, enable_thinking=False\n)\nprompt += \"&lt;tool_call&gt;\" # imposed tool call\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs[\"input_ids\"].shape[1]\noutputs = model.generate(\n    **model_inputs,\n    max_new_tokens=1024,\n    do_sample=False,\n    temperature=1.0,\n    top_p=1.0,\n    top_k=50,\n)\nassistant_response = \"&lt;tool_call&gt;\" + tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\nprint(assistant_response)\n\n\n&lt;tool_call&gt;\n{\"name\": \"ConversationalResponse\", \"arguments\": {\"response\": \"Here's a joke for you: A man in a hat with a hat on it, a hat with a hat on it, and a hat with a hat on it... It's a hat with a hat on it!\"}}\n&lt;/tool_call&gt;\n\n\n\nassistant_response_clean = assistant_response.split(\"&lt;tool_call&gt;\")[-1].split(\"&lt;/tool_call&gt;\")[0]\nassistant_response_json = json.loads(assistant_response_clean)\n\ndef execute_function_call(assistant_response_json):\n    if assistant_response_json['name'] == 'Python_REPL':\n        tool_response = exec(assistant_response_json['arguments']['python_code'])\n    elif assistant_response_json['name'] == 'ConversationalResponse':\n        tool_response = assistant_response_json['arguments']['response']\n    else:\n        tool_response = assistant_response_json\n    return tool_response\n\nexecute_function_call(assistant_response_json)\n\n\"Here's a joke for you: A man in a hat with a hat on it, a hat with a hat on it, and a hat with a hat on it... It's a hat with a hat on it!\"\n\n\nNot clear that it‚Äôs a good joke but we got back the behavior we were expecting."
  },
  {
    "objectID": "blog/posts/2025-07-05-Understanding-Function-Calling/Understanding_Function_Calling.html#webassembly",
    "href": "blog/posts/2025-07-05-Understanding-Function-Calling/Understanding_Function_Calling.html#webassembly",
    "title": "Understanding Function Calling",
    "section": "WebAssembly",
    "text": "WebAssembly\nWe can run the previous code in the browser thanks to WebAssembly!\nWe first need to install some packages:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can download a quantized version of the model we used (not everything will work but most of it will). This step should take a few minutes the first time (later it should be stored in the cache):\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can generate a prompt:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can define the tools and tokenize the prompt:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis is the generation part and it can take one or two minutes:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis is the generated code:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can execute the code:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  }
]