{
 "cells": [
  {
   "cell_type": "raw",
   "id": "56e1eb3a-e203-412d-bc5a-c9dca74cba5a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Force a language model not to use the letter 'e'\"\n",
    "author: \"Alonso Silva\"\n",
    "date: \"2025-07-23\"\n",
    "categories: [code, analysis]\n",
    "image: ./assets/Plaque_perec.jpg\n",
    "filters:\n",
    "  - line-highlight\n",
    "format:\n",
    "    html:\n",
    "        code-tools: true\n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d1288c-b485-4c9b-ab97-eaeebf376805",
   "metadata": {},
   "source": [
    "## Force a language model not to use the letter 'e'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f740bb8-94df-4987-af9a-e608c5ee89c7",
   "metadata": {},
   "source": [
    "![](./assets/Plaque_perec.jpg){width=60% fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e6fc3-496d-4760-8d16-286428e10334",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14fec8a-a325-4545-b092-1db8f095864d",
   "metadata": {},
   "source": [
    "Georges Perec (1936-1982) was a French novelist, famous among other things for writing a $300$-page novel called « La disparition  », without using during the whole novel the letter 'e' (the most used vowel in French). The English translation \"A Void\" also kept that constraint, while the Spanish translation \"El Secuestro\" is written without using the letter 'a' (the most used vowel in Spanish). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0de54c-1cbe-4100-934d-d1da1fb65e27",
   "metadata": {},
   "source": [
    "This type of writing is called a *lipogram* and it is a type of constrained writing. Writing a lipogram is a trivial task when avoiding uncommon letters (for example, z, j, q, or x), but it is much more challenging to avoid common letters (for example, e, t, or a) since we must omit many ordinary words ^[https://en.wikipedia.org/wiki/Lipogram]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da572cce-7422-4540-b4aa-3540f920563d",
   "metadata": {},
   "source": [
    "In this post, we want to force a language model not to use the letter 'e' in its responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0936c-18cc-43bc-8788-6e4d014169cd",
   "metadata": {},
   "source": [
    "### The Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26fed25-576b-4871-a0aa-f030b0a9929a",
   "metadata": {},
   "source": [
    "Let's first see that this is a difficult task for a language model.\n",
    "\n",
    "First, let's download a small language model and its tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b46ac0-6997-4c41-b3ac-37e4875c919e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T10:18:48.337268Z",
     "iopub.status.busy": "2025-07-18T10:18:48.336732Z",
     "iopub.status.idle": "2025-07-18T10:18:48.342913Z",
     "shell.execute_reply": "2025-07-18T10:18:48.341799Z",
     "shell.execute_reply.started": "2025-07-18T10:18:48.337218Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f6c9033-2ee1-4f46-97e7-b8c2e68c734a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T10:18:48.344762Z",
     "iopub.status.busy": "2025-07-18T10:18:48.344208Z",
     "iopub.status.idle": "2025-07-18T10:18:55.221251Z",
     "shell.execute_reply": "2025-07-18T10:18:55.220476Z",
     "shell.execute_reply.started": "2025-07-18T10:18:48.344714Z"
    }
   },
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "import torch\n",
    "from typing import List\n",
    "from threading import Thread\n",
    "from unicodedata import normalize\n",
    "from transformers.generation import LogitsProcessor\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "from transformers.generation import LogitsProcessor\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# Auto select device (CUDA > MPS > CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "model_id = \"Qwen/Qwen3-0.6B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, cache_dir=\"/big_storage/llms/hf_models/\"\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e97ad-bad5-40a4-b13d-cf65c502303f",
   "metadata": {},
   "source": [
    "Let's ask the following:\n",
    "\n",
    "> Tell me a short story without using the letter 'e'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca0d81-b923-4a2e-81d9-af654ba85336",
   "metadata": {},
   "source": [
    "Here is the language model's response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff4cfc0-ec97-4d2d-88a4-499ce506b510",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T10:18:55.222200Z",
     "iopub.status.busy": "2025-07-18T10:18:55.221890Z",
     "iopub.status.idle": "2025-07-18T10:18:57.847452Z",
     "shell.execute_reply": "2025-07-18T10:18:57.846407Z",
     "shell.execute_reply.started": "2025-07-18T10:18:55.222185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "<div style=\"\n",
       "    background-color: #D1ECF1;\n",
       "    color: #0C5460;\n",
       "    border-left: 4px solid #0C5460;\n",
       "    padding: 8px 12px;\n",
       "    border-radius: 4px;\">\n",
       "In a quiet village, a young girl named Lily found a hidden book tucked under a tree. She loved reading and spent her days there, surrounded by stories that brought her joy. One day, she discovered a secret message hidden in the book: *“The world is full of wonders, and I am here to share them.”* With a smile, she returned to her village, carrying the message with her.\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "\n",
    "user_input = \"Tell me a short story without using the letter 'e'\"\n",
    "\n",
    "def generate_response(user_input, logits_processor=[], enable_thinking=False):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking,\n",
    "    )\n",
    "\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    prompt_length = model_inputs['input_ids'].shape[-1]\n",
    "\n",
    "    generation_kwargs = dict(\n",
    "        model_inputs,\n",
    "        streamer=streamer,\n",
    "        logits_processor=logits_processor,\n",
    "        max_new_tokens=4 * 1024,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        top_p=1.0,\n",
    "        top_k=50,\n",
    "    )\n",
    "\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    assistant_response = \"\"\n",
    "    for chunk in streamer:\n",
    "        assistant_response += chunk\n",
    "        # print(chunk, end=\"\")\n",
    "\n",
    "    clean_assistant_response = assistant_response.split(\"<|im_end|>\")[0]\n",
    "\n",
    "    thread.join()\n",
    "    return clean_assistant_response\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def callout(text, kind=\"info\"):\n",
    "    colors = {\n",
    "        \"info\": (\"#D1ECF1\", \"#0C5460\"),\n",
    "        \"warning\": (\"#FFF3CD\", \"#856404\"),\n",
    "        \"success\": (\"#D4EDDA\", \"#155724\"),\n",
    "        \"danger\": (\"#F8D7DA\", \"#721C24\"),\n",
    "    }\n",
    "    bg, fg = colors.get(kind, colors[\"info\"])\n",
    "    md = f\"\"\"\n",
    "<div style=\"\n",
    "    background-color: {bg};\n",
    "    color: {fg};\n",
    "    border-left: 4px solid {fg};\n",
    "    padding: 8px 12px;\n",
    "    border-radius: 4px;\">\n",
    "{text}\n",
    "</div>\n",
    "\"\"\"\n",
    "    display(Markdown(md))\n",
    "\n",
    "assistant_response = generate_response(user_input)\n",
    "callout(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a50159-28ea-4f47-aaee-01e2b8a46896",
   "metadata": {},
   "source": [
    "\n",
    "A complete failure!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7bf4d9-87b8-4613-aeaa-1b4d6faf3715",
   "metadata": {},
   "source": [
    "Now, this is a small language model, so let's see GPT-4o response: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b275fa77-4da3-42f9-99cd-427398248ffa",
   "metadata": {},
   "source": [
    "![](./assets/lipogram_chatgpt.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e4dabf-8116-40b8-9679-418364d500ff",
   "metadata": {},
   "source": [
    "Another failure!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eb7f02-fe89-40bf-87ba-1776e08a694a",
   "metadata": {},
   "source": [
    "Here is the [link to that conversation](https://chatgpt.com/share/68790887-6d9c-800a-a928-ff882e6cb198)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c669cc8a-ef60-4ab0-8c33-0e1ab14d826b",
   "metadata": {},
   "source": [
    "### The Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238a8d11-3885-42bf-a76a-79c8ec0bbb7f",
   "metadata": {},
   "source": [
    "In my [previous post](https://alonsosilvaallende.github.io/blog/posts/2025-07-16-Understanding-Logits-Processors/Understanding_Logits_Processors.html), I discussed about logits processors and how they can be used to force language models to do all sorts of things. Well, we are going to use a logits processor to accomplish this task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3f5c503-2fa2-4eff-abb7-0710ce4fd7fd",
   "metadata": {},
   "source": [
    "Let's see that 'e' is indeed one of the most used vowels. First, for each vowel we find all the tokens that contain that vowel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df49ecc4-2a79-4473-b3d8-5c6d3836f734",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T10:20:31.853846Z",
     "iopub.status.busy": "2025-07-18T10:20:31.853399Z",
     "iopub.status.idle": "2025-07-18T10:20:44.471542Z",
     "shell.execute_reply": "2025-07-18T10:20:44.470824Z",
     "shell.execute_reply.started": "2025-07-18T10:20:31.853821Z"
    }
   },
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "list_of_vowels = [\"a\", \"e\", \"i\", \"o\", \"u\"]\n",
    "tokens_per_vowel = dict()\n",
    "for vowel in list_of_vowels:\n",
    "    tokens_containing_a_given_vowel = []\n",
    "    for token_id in range(tokenizer.vocab_size):\n",
    "        # check uppercase and accents\n",
    "        if (\n",
    "            vowel in tokenizer.decode(token_id)\n",
    "            or vowel.upper() in tokenizer.decode(token_id)\n",
    "            or normalize('NFC', f\"{vowel}\\u0300\") in tokenizer.decode(token_id)\n",
    "            or normalize('NFC', f\"{vowel}\\u0301\") in tokenizer.decode(token_id)\n",
    "            or normalize('NFC', f\"{vowel}\\u0302\") in tokenizer.decode(token_id)\n",
    "            or normalize('NFC', f\"{vowel}\\u0303\") in tokenizer.decode(token_id)\n",
    "            or normalize('NFC', f\"{vowel}\\u0308\") in tokenizer.decode(token_id)\n",
    "        ):\n",
    "            tokens_containing_a_given_vowel.append(token_id)\n",
    "    tokens_per_vowel[vowel] = tokens_containing_a_given_vowel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1d28d-c5c3-4f9b-9a5d-524831dd70e1",
   "metadata": {},
   "source": [
    "Let's verify the code is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e25954a-72e3-4327-a48b-1045c6a9da18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T10:21:38.537758Z",
     "iopub.status.busy": "2025-07-18T10:21:38.537147Z",
     "iopub.status.idle": "2025-07-18T10:21:38.544575Z",
     "shell.execute_reply": "2025-07-18T10:21:38.543552Z",
     "shell.execute_reply.started": "2025-07-18T10:21:38.537711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens that contain letter a: A a Ġa at an ar al as am Ġand ad ate ag ay ĠA...\n",
      "Tokens that contain letter e: E e er re en le Ġthe es ed et el ent Ġre se ex...\n",
      "Tokens that contain letter i: I i in it is ing ion ic Ġin id im il if ig iv...\n",
      "Tokens that contain letter o: O o on or ou ion Ġo ro Ġto Ġof om ol od ot ow...\n",
      "Tokens that contain letter u: U u ou ur ut us un ul ue um ub urn out turn our...\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "for vowel in list_of_vowels:\n",
    "    print(f\"Tokens that contain letter {vowel}: {\" \".join(tokenizer.convert_ids_to_tokens(tokens_per_vowel[vowel][:15]))}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13fd3e6-c0be-4d75-bc3b-946dacb6eb8e",
   "metadata": {},
   "source": [
    "It is working. Let's see how many tokens per vowel are there in this tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f33dc0fe-5d04-4123-8591-88fe76582410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T10:22:01.432898Z",
     "iopub.status.busy": "2025-07-18T10:22:01.431872Z",
     "iopub.status.idle": "2025-07-18T10:22:01.441067Z",
     "shell.execute_reply": "2025-07-18T10:22:01.439803Z",
     "shell.execute_reply.started": "2025-07-18T10:22:01.432845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of vocab tokens: 151,643\n",
      "There are 36,963 tokens that contain vowel 'a' or 24.38% of tokens\n",
      "There are 47,861 tokens that contain vowel 'e' or 31.56% of tokens\n",
      "There are 34,809 tokens that contain vowel 'i' or 22.95% of tokens\n",
      "There are 30,287 tokens that contain vowel 'o' or 19.97% of tokens\n",
      "There are 16,789 tokens that contain vowel 'u' or 11.07% of tokens\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "print(f\"Total number of vocab tokens: {tokenizer.vocab_size:,}\")\n",
    "for vowel in list_of_vowels:\n",
    "    print(f\"There are {len(tokens_per_vowel[vowel]):,} tokens that contain vowel '{vowel}' or {len(tokens_per_vowel[vowel])/tokenizer.vocab_size:.2%} of tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb88326c-c5c4-471a-9c85-150ccb6b3c9f",
   "metadata": {},
   "source": [
    "OK, so the letter 'e' is indeed a quite used vowel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846b5fc8-254c-4054-9e59-f57de57182f1",
   "metadata": {},
   "source": [
    "We want to create a logits processor that will forbid all the tokens that contain the letter 'e'. \n",
    "\n",
    "We first create a logits processor that will receive a list of forbidden tokens and that it will lower the raw scores of the forbiden tokens to minus infinite as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab9cc740-5212-4a37-a668-3275d5d4077a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T10:22:28.707331Z",
     "iopub.status.busy": "2025-07-18T10:22:28.706390Z",
     "iopub.status.idle": "2025-07-18T10:22:28.715093Z",
     "shell.execute_reply": "2025-07-18T10:22:28.714083Z",
     "shell.execute_reply.started": "2025-07-18T10:22:28.707286Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "class GeorgePerecLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, forbidden_tokens: List[int]):\n",
    "        self.forbidden_tokens = forbidden_tokens\n",
    "\n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        scores_processed = scores.clone()\n",
    "        vocab_tensor = torch.arange(scores.shape[-1], device=scores.device)\n",
    "        forbidden_tokens = torch.tensor(self.forbidden_tokens, device=scores.device)\n",
    "        forbidden_tokens_mask = torch.isin(vocab_tensor, forbidden_tokens)\n",
    "        scores_processed = torch.where(forbidden_tokens_mask, -torch.inf, scores)\n",
    "\n",
    "        return scores_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c5313-d8dd-4a5b-909e-87da52de501c",
   "metadata": {},
   "source": [
    "We can instantiate the logits processor by putting all the tokens that contain the letter 'e' as forbidden tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e6c25ea-b971-43b2-ab3f-96af9d2be45a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T10:22:36.858608Z",
     "iopub.status.busy": "2025-07-18T10:22:36.858000Z",
     "iopub.status.idle": "2025-07-18T10:22:36.863641Z",
     "shell.execute_reply": "2025-07-18T10:22:36.862677Z",
     "shell.execute_reply.started": "2025-07-18T10:22:36.858566Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "logits_processors = [\n",
    "    GeorgePerecLogitsProcessor(\n",
    "        forbidden_tokens=tokens_per_vowel[\"e\"],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68962919-77ad-4c78-ba77-f8b5cd29dbb0",
   "metadata": {},
   "source": [
    "We can ask the language model the following:\n",
    "\n",
    "> Tell me a story about pirates and adventures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d616fe7-1bae-4581-9f56-10ac9ab513f5",
   "metadata": {},
   "source": [
    "Here is the language model's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cd01d02-4eee-410d-b08c-8b65d2001b10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T10:22:40.814700Z",
     "iopub.status.busy": "2025-07-18T10:22:40.813767Z",
     "iopub.status.idle": "2025-07-18T10:22:54.204691Z",
     "shell.execute_reply": "2025-07-18T10:22:54.203578Z",
     "shell.execute_reply.started": "2025-07-18T10:22:40.814657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "<div style=\"\n",
       "    background-color: #D1ECF1;\n",
       "    color: #0C5460;\n",
       "    border-left: 4px solid #0C5460;\n",
       "    padding: 8px 12px;\n",
       "    border-radius: 4px;\">\n",
       "**A Story of Shadows and Stars**\n",
       "\n",
       "In a distant land far from civilization, known as *Varkhara*, a small island shrouding in mist and fog, lay a kingdom of sailors and warriors. It was said that this land was born from a storm, and that its only way to find its way was to sail through its own storms.\n",
       "\n",
       "Long ago, a young man, **Karan**, was born to a family of fish and sailors. His family had always known that to find a path to glory, a man must sail through storms. So, at a young and curious mind, Karan took his first sail. With a small boat and a compass, and a spirit of daring, Karan rowdily rowing his way through a storm.\n",
       "\n",
       "But as Karan rowdily rowing his way through a storm, a shadow looms on his path. A dark, old man, **Malik**, a long-forging captain, had always known that to sail through storms, a man must not only row but also fight. Malik had always taught Karan that storms could not only bring rain but also bring warriors.\n",
       "\n",
       "With Malik’s wisdom and Karan’s daring spirit, Karan and Malik soon found a way to sail through storms. With a small boat and a compass, and a spirit of daring, Karan rowdily rowing his way through a storm.\n",
       "\n",
       "But as Karan rowdily rowing his way through a storm, a shadow looms on his path. A dark, old man, **Malik**, a long-forging captain, had always known that to sail through storms, a man must not only row but also fight. Malik had always taught Karan that storms could not only bring rain but also bring warriors.\n",
       "\n",
       "With Malik’s wisdom and Karan’s daring spirit, Karan and Malik soon found a way to sail through storms. With a small boat and a compass, and a spirit of daring, Karan rowdily rowing his way through a storm.\n",
       "\n",
       "And so, Karan and Malik, two sailors from Varkhara, found a way to sail through storms, not just through storms, but through *war*.\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "user_input = \"Tell me a story about pirates and adventures\"\n",
    "assistant_response = generate_response(user_input, logits_processor=logits_processors)\n",
    "callout(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506028f-e4b6-43a6-b9c4-fd74dd19eed8",
   "metadata": {},
   "source": [
    "Perfect! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda353d7-f1e4-4f47-90d2-cd8bd928d805",
   "metadata": {},
   "source": [
    "Here is an app to talk to a language model like this one yourself (it is running on CPU, so it might be slow):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4843f695-7b07-4ed6-b4ec-dc4ff7430a5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T10:50:49.501880Z",
     "iopub.status.busy": "2025-07-18T10:50:49.501257Z",
     "iopub.status.idle": "2025-07-18T10:50:49.509858Z",
     "shell.execute_reply": "2025-07-18T10:50:49.508693Z",
     "shell.execute_reply.started": "2025-07-18T10:50:49.501834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"850\"\n",
       "            height=\"600\"\n",
       "            src=\"https://alonsosilva-georgesperecassistant.hf.space\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x72640822a330>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://alonsosilva-georgesperecassistant.hf.space\", width=850, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5c36c6-eefa-4188-8479-f3cfa9405ede",
   "metadata": {},
   "source": [
    "Here is the [link to the app](https://alonsosilva-georgesperecassistant.hf.space)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed655769-6aef-4343-b308-7c95489862ad",
   "metadata": {},
   "source": [
    "For faster inference, you can also clone [this repo](https://github.com/alonsosilvaallende/Georges-Perec-Assistant) and run it on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48f8186-ef09-42e8-adc2-75776d1f80c6",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f0247-1267-43d1-90a6-7dda3a71c5dd",
   "metadata": {},
   "source": [
    "We successfully forced a small language model (0.6B parameters) not to use the letter 'e'. I think it's super interesting to see applications in which a basic logits processor can reliably make a small language model fulfill a task where much larger models fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86fe2b-023a-4b20-864c-cc1730337c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
