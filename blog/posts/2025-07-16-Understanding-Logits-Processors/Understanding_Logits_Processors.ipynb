{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e1ead407-f2dc-46e3-995d-bdec9ef7d6f8",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Understanding Logits Processors\"\n",
    "author: \"Alonso Silva\"\n",
    "date: \"2025-07-16\"\n",
    "categories: [code, analysis]\n",
    "image: logitsprocessor.jpg\n",
    "filters:\n",
    "  - line-highlight\n",
    "format:\n",
    "    html:\n",
    "        code-tools: true\n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaeba4c-f546-4c65-ac97-212bfa86dfd3",
   "metadata": {},
   "source": [
    "Logits processors are incredibly powerful and I think they should receive more attention from the community. Logits processors, as their name implies, process the logits, that is, they process the outputs of the last layer of the neural network or the raw scores of the tokens. We can modify the raw scores and get a completely different result than the one the language model would have generated on its own. We will clarify this with some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0760ed8b-444d-4611-b44b-c9b00abb2123",
   "metadata": {},
   "source": [
    "In this post, we will see some simple logits processors examples (minimum length and minimum new tokens length), as well as some more complex ones (replacing the end of sequence by a word and replacing the end of sequence by a phrase). We then conclude with two practical applications of logits processors: make reasoning models stop thinking after a limit by specifying a thinking budget as well as forcing reasoning models to think for a longer time for particularly difficult questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d32356-e3eb-4984-a9d9-2340f321a5ce",
   "metadata": {},
   "source": [
    "## Basic Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8280c5b8-2bd6-49c1-8b51-4a4b9fe85b2c",
   "metadata": {},
   "source": [
    "Let's start with a basic example of a logit processor. In this section, we won't use the thinking capabilities of the language model.\n",
    "\n",
    "We first download a small language model (0.6B parameters) and its tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "457a51a5-f869-4aa8-a36e-e49953fb74a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T14:59:52.162668Z",
     "iopub.status.busy": "2025-07-16T14:59:52.161733Z",
     "iopub.status.idle": "2025-07-16T14:59:52.168122Z",
     "shell.execute_reply": "2025-07-16T14:59:52.167159Z",
     "shell.execute_reply.started": "2025-07-16T14:59:52.162620Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c57d1dd-013b-4210-bea3-1b62a3126ee1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T14:59:52.170292Z",
     "iopub.status.busy": "2025-07-16T14:59:52.169378Z",
     "iopub.status.idle": "2025-07-16T15:00:00.687218Z",
     "shell.execute_reply": "2025-07-16T15:00:00.686451Z",
     "shell.execute_reply.started": "2025-07-16T14:59:52.170245Z"
    }
   },
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "import torch\n",
    "from typing import List\n",
    "from threading import Thread\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "from transformers.generation import LogitsProcessor\n",
    "\n",
    "model_id = \"Qwen/Qwen3-0.6B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, cache_dir=\"/big_storage/llms/hf_models/\"\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b6255-ad4e-48f3-b92e-507d1fd60f5b",
   "metadata": {},
   "source": [
    "We can ask the question:\n",
    "\n",
    "> What's 2 + 2?\n",
    "\n",
    "and see what the language model would have responded without any logits processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a1799cb-d2c8-409c-a8c9-1492319272be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:00.688081Z",
     "iopub.status.busy": "2025-07-16T15:00:00.687755Z",
     "iopub.status.idle": "2025-07-16T15:00:01.557267Z",
     "shell.execute_reply": "2025-07-16T15:00:01.556040Z",
     "shell.execute_reply.started": "2025-07-16T15:00:00.688067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 equals 4.\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "user_input = \"What's 2 + 2?\"\n",
    "\n",
    "def generate_response(user_input, logits_processor=[], enable_thinking=False):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking,\n",
    "    )\n",
    "\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    prompt_length = model_inputs['input_ids'].shape[-1]\n",
    "\n",
    "    generation_kwargs = dict(\n",
    "        model_inputs,\n",
    "        streamer=streamer,\n",
    "        logits_processor=logits_processor,\n",
    "        max_new_tokens=4 * 1024,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        top_p=1.0,\n",
    "        top_k=50,\n",
    "    )\n",
    "\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    assistant_response = \"\"\n",
    "    for chunk in streamer:\n",
    "        assistant_response += chunk\n",
    "        # print(chunk, end=\"\")\n",
    "\n",
    "    clean_assistant_response = assistant_response.split(\"<|im_end|>\")[0]\n",
    "\n",
    "    if enable_thinking:\n",
    "        reasoning_trace = assistant_response.split(\"<think>\")[-1].split(\"</think>\")[0]\n",
    "        thinking_length = len(tokenizer.encode(reasoning_trace))\n",
    "        if \"</think>\" in assistant_response:\n",
    "            response_without_reasoning_trace = assistant_response.split(\"</think>\")[-1]\n",
    "            response_length = len(tokenizer.encode(response_without_reasoning_trace))\n",
    "        else:\n",
    "            response_length = 0\n",
    "    else:\n",
    "        thinking_length = 0\n",
    "        response_length = len(tokenizer.encode(clean_assistant_response))\n",
    "    thread.join()\n",
    "    return clean_assistant_response, prompt_length, thinking_length, response_length\n",
    "\n",
    "\n",
    "assistant_response, prompt_length, thinking_length, response_length = generate_response(\n",
    "    user_input\n",
    ")\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7805d447-58d0-4c92-89ef-f2735040cf6f",
   "metadata": {},
   "source": [
    "The answer is quite straightforward: `2 + 2 equals 4.`\n",
    "\n",
    "The number of tokens is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a748e4f4-794a-4cd8-8384-69ffb2238c84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:01.559100Z",
     "iopub.status.busy": "2025-07-16T15:00:01.558546Z",
     "iopub.status.idle": "2025-07-16T15:00:01.564837Z",
     "shell.execute_reply": "2025-07-16T15:00:01.563833Z",
     "shell.execute_reply.started": "2025-07-16T15:00:01.559057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# prompt tokens: 20\n",
      "# thinking tokens: 0\n",
      "# response tokens: 8\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "print(\n",
    "    f\"# prompt tokens: {prompt_length}\\n# thinking tokens: {thinking_length}\\n# response tokens: {response_length}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fc99cb-2d90-4508-ad74-868c6acb2a78",
   "metadata": {},
   "source": [
    "There are $20$ prompt tokens and $8$ response tokens. Here are the $8$ response tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a5d6fda-89a4-49be-ad83-55bc887f81bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:01.566913Z",
     "iopub.status.busy": "2025-07-16T15:00:01.565979Z",
     "iopub.status.idle": "2025-07-16T15:00:01.604656Z",
     "shell.execute_reply": "2025-07-16T15:00:01.603651Z",
     "shell.execute_reply.started": "2025-07-16T15:00:01.566871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 17; token: 2\n",
      "id: 488; token: ⎵+\n",
      "id: 220; token: ⎵\n",
      "id: 17; token: 2\n",
      "id: 16819; token: ⎵equals\n",
      "id: 220; token: ⎵\n",
      "id: 19; token: 4\n",
      "id: 13; token: .\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "for token in tokenizer.encode(assistant_response):\n",
    "    print(f\"id: {token}; token: {tokenizer.decode(token).replace(' ', '⎵')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7049b-1e5d-4be6-aa35-5ed76343a86b",
   "metadata": {},
   "source": [
    "### Minimum Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e568f8f-b22c-46d6-b3f5-0d2426265b65",
   "metadata": {},
   "source": [
    "Let's force the language model to generate a longer answer. In order to do that, we can define the following logits processor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2be5ff7-3837-421b-8332-77deaa2b5e16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:01.608683Z",
     "iopub.status.busy": "2025-07-16T15:00:01.608119Z",
     "iopub.status.idle": "2025-07-16T15:00:01.629197Z",
     "shell.execute_reply": "2025-07-16T15:00:01.628197Z",
     "shell.execute_reply.started": "2025-07-16T15:00:01.608642Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "class MinLengthLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, min_length: int, eos_token_ids: List[int]):\n",
    "        \n",
    "        self.min_length = min_length\n",
    "        self.eos_token_ids = eos_token_ids\n",
    "\n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        \n",
    "        scores_processed = scores.clone()\n",
    "        token_count = input_ids.shape[-1]\n",
    "        if token_count < self.min_length:\n",
    "            for eos_token_id in self.eos_token_ids:\n",
    "                scores_processed[:, eos_token_id] = -torch.inf\n",
    "        return scores_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa49888-9102-44a7-b007-2d4d777a7859",
   "metadata": {},
   "source": [
    "This logits processor consists of two parts:\n",
    "\n",
    "- The first is the constructor which just initializes the minimum length required and the list of end of sequence tokens.\n",
    "- The second is the callable method which clones the original scores, and if we haven't yet reached the minimum length required, it will give a score of minus infinite to the end of sequence tokens, **effectively preventing the language model to choose them and therefore preventing it from ending the sentence**. The language model will need to continue talking for as long as we want. And that's what we will see.\n",
    "\n",
    "Let's instantiate this logits processor with a required minimum length of $40$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d1d961-3233-44b4-a66a-f733c281ca7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:01.630806Z",
     "iopub.status.busy": "2025-07-16T15:00:01.630290Z",
     "iopub.status.idle": "2025-07-16T15:00:01.654740Z",
     "shell.execute_reply": "2025-07-16T15:00:01.653765Z",
     "shell.execute_reply.started": "2025-07-16T15:00:01.630766Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "logits_processor = [\n",
    "    MinLengthLogitsProcessor(\n",
    "        min_length=40, eos_token_ids=[tokenizer.eos_token_id, tokenizer.pad_token_id]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f3c25-b6c2-4068-8155-9af54558bfb2",
   "metadata": {},
   "source": [
    " This is the assistant response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1d68168-4fcc-4fae-a13b-d41f14b2479b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:01.656333Z",
     "iopub.status.busy": "2025-07-16T15:00:01.655832Z",
     "iopub.status.idle": "2025-07-16T15:00:02.265828Z",
     "shell.execute_reply": "2025-07-16T15:00:02.265010Z",
     "shell.execute_reply.started": "2025-07-16T15:00:01.656294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 equals 4. Let me know if you have any other questions! 😊\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "assistant_response, prompt_length, thinking_length, response_length = generate_response(\n",
    "    user_input,\n",
    "    logits_processor=logits_processor,\n",
    ")\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6705e9d0-1c87-4a57-b0c0-265bb02c66b2",
   "metadata": {},
   "source": [
    "The language model added to the previous response `2 + 2 equals 4.` the phrase ` Let me know if you have any other questions! 😊`\n",
    "\n",
    "We made the language model do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8988ca-4649-4f51-a77a-40e81a749390",
   "metadata": {},
   "source": [
    "The number of tokens is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db582ff5-a510-495e-9df1-f39d62f10e12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:02.267511Z",
     "iopub.status.busy": "2025-07-16T15:00:02.266887Z",
     "iopub.status.idle": "2025-07-16T15:00:02.272059Z",
     "shell.execute_reply": "2025-07-16T15:00:02.271327Z",
     "shell.execute_reply.started": "2025-07-16T15:00:02.267469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# prompt tokens: 20\n",
      "# thinking tokens: 0\n",
      "# response tokens: 20\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "print(\n",
    "    f\"# prompt tokens: {prompt_length}\\n# thinking tokens: {thinking_length}\\n# response tokens: {response_length}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc97d84-46f5-48f5-85df-3aeee4f24900",
   "metadata": {},
   "source": [
    "The language model generated $20$ tokens even though we asked for a minimum length of $40$ tokens. The reason is that the logits processor also considers the $20$ prompt tokens and $20+20\\ge40$ so that's correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2031d955-f923-47a3-8585-3f03644fcaef",
   "metadata": {},
   "source": [
    "### Minimum New Tokens Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f84ef9-50c8-4d90-bf7d-a9a411ad8065",
   "metadata": {},
   "source": [
    "Let's remove the prompt tokens from the computation. The logits processor is slightly more complex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "083489c4-0cba-4df2-b000-84a17169758e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:02.273696Z",
     "iopub.status.busy": "2025-07-16T15:00:02.273037Z",
     "iopub.status.idle": "2025-07-16T15:00:02.306702Z",
     "shell.execute_reply": "2025-07-16T15:00:02.305716Z",
     "shell.execute_reply.started": "2025-07-16T15:00:02.273657Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#| source-line-numbers: \"7,12-14,16\"\n",
    "#| class-source: \"numberLines\"\n",
    "class MinNewTokensLengthLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(\n",
    "        self, min_new_tokens_length: int, eos_token_ids: List[int]\n",
    "    ):\n",
    "        self.min_new_tokens_length = min_new_tokens_length\n",
    "        self.eos_token_ids = eos_token_ids\n",
    "        self.prompt_length_to_skip = None\n",
    "\n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        first_time = self.prompt_length_to_skip is None\n",
    "        if first_time:\n",
    "            self.prompt_length_to_skip = input_ids.shape[-1]\n",
    "        scores_processed = scores.clone()\n",
    "        token_count = input_ids.shape[-1] - self.prompt_length_to_skip\n",
    "        if token_count < self.min_new_tokens_length:\n",
    "            for eos_token_id in self.eos_token_ids:\n",
    "                scores_processed[:, eos_token_id] = -torch.inf\n",
    "        return scores_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04468807-200a-4445-9bf2-f84fb596c533",
   "metadata": {},
   "source": [
    "We have added a `prompt_length_to_skip` which will get its value from the length of the input ids only the first time the logits processor is called, effectively storing the prompt length. We then substract the `prompt_length_to_skip` from the `token_count`.\n",
    "\n",
    "Let's instantiate this logits processor with a required minimum length of new tokens of $40$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "072cdc05-6432-41c7-b1e3-90cedc08326a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:02.308304Z",
     "iopub.status.busy": "2025-07-16T15:00:02.307785Z",
     "iopub.status.idle": "2025-07-16T15:00:02.338238Z",
     "shell.execute_reply": "2025-07-16T15:00:02.337277Z",
     "shell.execute_reply.started": "2025-07-16T15:00:02.308264Z"
    }
   },
   "outputs": [],
   "source": [
    "logits_processor = [\n",
    "    MinNewTokensLengthLogitsProcessor(\n",
    "        min_new_tokens_length=40,\n",
    "        eos_token_ids=[tokenizer.eos_token_id, tokenizer.pad_token_id],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3569d0-c5cd-43e2-a1cd-4a84a9f412e8",
   "metadata": {},
   "source": [
    " This is the assistant response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01f9d100-25f8-46f8-9f98-995fd08337b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:02.339833Z",
     "iopub.status.busy": "2025-07-16T15:00:02.339318Z",
     "iopub.status.idle": "2025-07-16T15:00:03.438778Z",
     "shell.execute_reply": "2025-07-16T15:00:03.437585Z",
     "shell.execute_reply.started": "2025-07-16T15:00:02.339792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 equals 4. Let me know if you have any other questions! 😊. 🎉. 🔍. 🧠. 🧠. 🧠.\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "assistant_response, prompt_length, thinking_length, response_length = generate_response(\n",
    "    user_input,\n",
    "    logits_processor=logits_processor\n",
    ")\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf103e-7200-4225-8b2e-48653bed694e",
   "metadata": {},
   "source": [
    "The language model added to the previous phrase `2 + 2 equals 4. Let me know if you have any other questions! 😊` the following phrase `. 🎉. 🔍. 🧠. 🧠. 🧠.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc732cee-61de-4b66-8b85-6d5816b78d15",
   "metadata": {},
   "source": [
    "The number of tokens is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13f0b0a8-3a3c-47d5-9b80-2fad920628fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:03.440692Z",
     "iopub.status.busy": "2025-07-16T15:00:03.440117Z",
     "iopub.status.idle": "2025-07-16T15:00:03.446783Z",
     "shell.execute_reply": "2025-07-16T15:00:03.445645Z",
     "shell.execute_reply.started": "2025-07-16T15:00:03.440650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# prompt tokens: 20\n",
      "# thinking tokens: 0\n",
      "# response tokens: 40\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "print(\n",
    "    f\"# prompt tokens: {prompt_length}\\n# thinking tokens: {thinking_length}\\n# response tokens: {response_length}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92184cd6-c4ab-46a2-9a39-00e7dad9beef",
   "metadata": {},
   "source": [
    "The language model added some emojis in order to arrive to the required $40$ response tokens. That's ok."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a68120b-1cad-4931-8de2-d523deb03a94",
   "metadata": {},
   "source": [
    "## Replacements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65db76d-e734-4256-ab0c-d31d0977e00e",
   "metadata": {},
   "source": [
    "### Replace the end of sequence by a word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae799cc2-7672-48e8-8e44-0959e3cf95e7",
   "metadata": {},
   "source": [
    "Now let's do something slightly more complex. This time when the language model wants to finish its answer (in our case, after the phrase `2 + 2 equals 4.`), we are going to replace the ending token with another token. In this case, with the token `⎵Heck` (my first choice was the F-word). Note that we can also replace any other token and see how the model would have continued the phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61655701-4a54-4877-9a82-e39909f9c298",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:03.448536Z",
     "iopub.status.busy": "2025-07-16T15:00:03.447990Z",
     "iopub.status.idle": "2025-07-16T15:00:03.481348Z",
     "shell.execute_reply": "2025-07-16T15:00:03.480321Z",
     "shell.execute_reply.started": "2025-07-16T15:00:03.448496Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#| source-line-numbers: \"23-27\"\n",
    "#| class-source: \"numberLines\"\n",
    "class MinNewTokensLengthWithReplacementTokenLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_new_tokens_length: int,\n",
    "        eos_token_ids: List[int],\n",
    "        replacement_token_id: int\n",
    "    ):\n",
    "        self.min_new_tokens_length = min_new_tokens_length\n",
    "        self.eos_token_ids = eos_token_ids\n",
    "        self.replacement_token_id = replacement_token_id\n",
    "        self.prompt_length_to_skip = None\n",
    "        self.very_large_number = 10_000\n",
    "\n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        first_time = self.prompt_length_to_skip is None\n",
    "        if first_time:\n",
    "            self.prompt_length_to_skip = input_ids.shape[-1]\n",
    "        scores_processed = scores.clone()\n",
    "        token_count = input_ids.shape[-1] - self.prompt_length_to_skip\n",
    "        if token_count < self.min_new_tokens_length:\n",
    "            token_chosen_id = torch.argmax(scores_processed).item()\n",
    "            if token_chosen_id in self.eos_token_ids:\n",
    "                scores_processed[:, self.replacement_token_id] = self.very_large_number\n",
    "                for eos_token_id in self.eos_token_ids:\n",
    "                    scores_processed[:, eos_token_id] = -torch.inf\n",
    "        return scores_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80cbd11-79a4-451d-8d83-286f0bcd8c53",
   "metadata": {},
   "source": [
    "Let's instantiate this logits processor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5e7bf87-f915-47ba-b965-35c5876e7dee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:03.483222Z",
     "iopub.status.busy": "2025-07-16T15:00:03.482473Z",
     "iopub.status.idle": "2025-07-16T15:00:03.513065Z",
     "shell.execute_reply": "2025-07-16T15:00:03.511826Z",
     "shell.execute_reply.started": "2025-07-16T15:00:03.483182Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "logits_processor=[\n",
    "    MinNewTokensLengthWithReplacementTokenLogitsProcessor(\n",
    "        min_new_tokens_length=40,\n",
    "        eos_token_ids=[tokenizer.eos_token_id, tokenizer.pad_token_id],\n",
    "        replacement_token_id=tokenizer.encode(\" Heck\")[0],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48feb3f8-490d-4783-babb-2266d983ee1d",
   "metadata": {},
   "source": [
    "Here is the assistant response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "578bde4e-4979-42dc-a2a1-b42d49f8cbb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:03.514973Z",
     "iopub.status.busy": "2025-07-16T15:00:03.514467Z",
     "iopub.status.idle": "2025-07-16T15:00:04.940270Z",
     "shell.execute_reply": "2025-07-16T15:00:04.939238Z",
     "shell.execute_reply.started": "2025-07-16T15:00:03.514934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 equals 4. Heck, that's a simple math problem. Heck, I'm just a AI assistant here. Heck, I'm not going to do that. Heck, I'm just going to tell you that 2 + 2 is 4.\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "assistant_response, prompt_length, thinking_length, response_length = generate_response(\n",
    "    user_input,\n",
    "    logits_processor=logits_processor\n",
    ")\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bcbd64-e1cd-42f9-934f-497c3c52e770",
   "metadata": {},
   "source": [
    "The language model added to the very first phrase `2 + 2 equals 4.` the following phrases ` Heck, that's a simple math problem. Heck, I'm just a AI assistant here. Heck, I'm not going to do that. Heck, I'm just going to tell you that 2 + 2 is 4.`\n",
    "\n",
    "There are completely different phrases with repect to the previous ones!\n",
    "\n",
    "By modifying the end of sequence by the token `⎵Heck` we made the model take a completely different path compared to what the language model would have taken by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c154115f-cc4d-4ffa-a009-15fce8fd512b",
   "metadata": {},
   "source": [
    "### Replace the end of sequence by a phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf81ee2d-a380-4bbf-b819-9f003fcdeb40",
   "metadata": {},
   "source": [
    "We just replaced the end of sequence by a word (token) but it might be interesting to replace the end of sequence by a phrase. For example, we might want that the language model checks its answer. Let's do that by replacing the end of sentence by the phrase ` Wait, let me check my answer`.\n",
    "\n",
    "The logits processor is slightly more complex since we need to generate a sequence of tokens and therefore find a way to keep the state (here the state will be kept by the `index` variable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "119a3fc1-46a8-459e-b723-774b27701f46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:04.941989Z",
     "iopub.status.busy": "2025-07-16T15:00:04.941470Z",
     "iopub.status.idle": "2025-07-16T15:00:04.953685Z",
     "shell.execute_reply": "2025-07-16T15:00:04.952710Z",
     "shell.execute_reply.started": "2025-07-16T15:00:04.941949Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#| source-line-numbers: \"13,24-37\"\n",
    "#| class-source: \"numberLines\"\n",
    "class MinNewTokensLengthWithReplacementLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_new_tokens_length: int,\n",
    "        eos_token_ids: List[int],\n",
    "        replacement_tokens_ids: List[int],\n",
    "    ):\n",
    "        self.min_new_tokens_length = min_new_tokens_length\n",
    "        self.eos_token_ids = eos_token_ids\n",
    "        self.replacement_tokens_ids = replacement_tokens_ids\n",
    "        self.prompt_length_to_skip = None\n",
    "        self.very_large_number = 10_000\n",
    "        self.index = -1\n",
    "\n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        first_time = self.prompt_length_to_skip is None\n",
    "        if first_time:\n",
    "            self.prompt_length_to_skip = input_ids.shape[-1]\n",
    "        scores_processed = scores.clone()\n",
    "        token_count = input_ids.shape[-1] - self.prompt_length_to_skip\n",
    "        if token_count < self.min_new_tokens_length:\n",
    "            token_chosen_id = torch.argmax(scores_processed).item()\n",
    "            if (token_chosen_id in self.eos_token_ids) and (self.index == -1):\n",
    "                for eos_token_id in self.eos_token_ids:\n",
    "                    scores_processed[:, eos_token_id] = -torch.inf\n",
    "                self.index = 0\n",
    "\n",
    "            if len(self.replacement_tokens_ids) > self.index >= 0:\n",
    "                scores_processed[:, self.replacement_tokens_ids[self.index]] = (\n",
    "                    self.very_large_number\n",
    "                )\n",
    "                self.index += 1\n",
    "\n",
    "            if self.index == len(self.replacement_tokens_ids):\n",
    "                self.index = -1\n",
    "\n",
    "        return scores_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da3490a-8388-4f15-9644-3064ee05759f",
   "metadata": {},
   "source": [
    "Let's instantiate the logits processor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63bb1dd8-b170-4844-b46b-e741550e6395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:04.955543Z",
     "iopub.status.busy": "2025-07-16T15:00:04.954773Z",
     "iopub.status.idle": "2025-07-16T15:00:04.993202Z",
     "shell.execute_reply": "2025-07-16T15:00:04.992207Z",
     "shell.execute_reply.started": "2025-07-16T15:00:04.955503Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "logits_processor = [\n",
    "    MinNewTokensLengthWithReplacementLogitsProcessor(\n",
    "        min_new_tokens_length=30,\n",
    "        eos_token_ids=[tokenizer.eos_token_id, tokenizer.pad_token_id],\n",
    "        replacement_tokens_ids=tokenizer.encode(\" Wait, let me check my answer\")\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5776ce7-e878-4351-b2b5-b60f24682949",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:04.995091Z",
     "iopub.status.busy": "2025-07-16T15:00:04.994306Z",
     "iopub.status.idle": "2025-07-16T15:00:05.925599Z",
     "shell.execute_reply": "2025-07-16T15:00:05.924556Z",
     "shell.execute_reply.started": "2025-07-16T15:00:04.995052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 equals 4. Wait, let me check my answer again. 2 + 2 is indeed 4. So the correct answer is 4.\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "assistant_response, prompt_length, thinking_length, response_length = generate_response(\n",
    "    user_input,\n",
    "    logits_processor=logits_processor\n",
    ")\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156ce99-09b6-425f-bc1a-cf4274f7eed9",
   "metadata": {},
   "source": [
    "The language model added to the first phrase `2 + 2 equals 4.` the phrases ` Wait, let me check my answer again. 2 + 2 is indeed 4. So the correct answer is 4.`\n",
    "\n",
    "We forced the model to check its answer by replacing the end of sequence by the phrase ` Wait, let me check my answer` and let the language model continue the phrase. That's great!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc1f7f4-1cfa-4a91-82db-301d85e04374",
   "metadata": {},
   "source": [
    "## Thinking Budget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc7e2ed-f824-431d-baeb-97b3c810f297",
   "metadata": {},
   "source": [
    "The previous sections were fun for me and I hope they were fun for you as well, but let's now look at very practical applications of logits processors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ddb89-809f-4aed-9ece-1870c912f8ec",
   "metadata": {},
   "source": [
    "Many people noticed that reasoning models are very verbose in their thinking and they were looking for practical ways to limit that. `Qwen3` even provided the choice to remove thinking altogether for some questions (btw that's what we did here by putting `enable_thinking=False`). However, what if we want to let the model think but not for too long. Couldn't we define a `thinking budget` and if the model goes above that thinking budget you just make the thinking stop altogether?\n",
    "\n",
    "Of course, we can, thanks to logits processors!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad44544-6273-422b-928f-f53b94e4849a",
   "metadata": {},
   "source": [
    "If we ask the question `What's 2 + 2?` and let the language model think (`enable_thinking=True`) without any constraint, this is what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7979839d-b1fa-4e02-99ba-01412fe98845",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:05.927251Z",
     "iopub.status.busy": "2025-07-16T15:00:05.926736Z",
     "iopub.status.idle": "2025-07-16T15:00:12.672653Z",
     "shell.execute_reply": "2025-07-16T15:00:12.671593Z",
     "shell.execute_reply.started": "2025-07-16T15:00:05.927212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking, \"What's 2 + 2?\" Let me think about how to approach this. First, I need to make sure I understand the question correctly. The user is probably looking for the sum of 2 plus 2, which is 4. But maybe they're trying to get a different answer, like a joke or something else. Let me check if there's any context I'm missing.\n",
      "\n",
      "Wait, sometimes people use \"2 + 2\" in a different way. For example, in some languages, numbers are written differently, but in English, it's straightforward. Also, maybe the user is testing if I can recognize that 2 + 2 equals 4. But I should also consider if there's any trick here. For instance, if they're using a calculator, the result would be 4. But since the question is simple, the answer is 4.\n",
      "\n",
      "I should also make sure there's no hidden meaning or cultural context. In most basic math problems, 2 + 2 is 4. So the answer is 4. I don't see any other possible interpretations here. The user might just want the direct answer. Let me confirm once more. Yes, 2 plus 2 is 4. So the final answer should be 4.\n",
      "</think>\n",
      "\n",
      "2 + 2 equals 4.\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "assistant_response, prompt_length, thinking_length, response_length = generate_response(\n",
    "    user_input,\n",
    "    enable_thinking=True\n",
    ")\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f4768d-5373-4cd2-989c-f9826c5fff72",
   "metadata": {},
   "source": [
    "The number of tokens is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad2ebeb3-64b5-44e6-a45e-ab9f281c5bb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:12.678128Z",
     "iopub.status.busy": "2025-07-16T15:00:12.677554Z",
     "iopub.status.idle": "2025-07-16T15:00:12.683054Z",
     "shell.execute_reply": "2025-07-16T15:00:12.682050Z",
     "shell.execute_reply.started": "2025-07-16T15:00:12.678087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# prompt tokens: 16\n",
      "# thinking tokens: 269\n",
      "# response tokens: 10\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "print(\n",
    "    f\"# prompt tokens: {prompt_length}\\n# thinking tokens: {thinking_length}\\n# response tokens: {response_length}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26e301-0b9f-4c2b-b2a0-a2bb9c7027b2",
   "metadata": {},
   "source": [
    "If we let the language model think without any constraint, it uses $269$ thinking tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c9d35-ffb4-4655-a52b-88f107195e36",
   "metadata": {},
   "source": [
    "Let's consider the case when we force the language model to think for less than a fixed thinking budget. The logits processor would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71df3fd6-db32-4cd2-8610-c4fa0359963d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:12.684760Z",
     "iopub.status.busy": "2025-07-16T15:00:12.684236Z",
     "iopub.status.idle": "2025-07-16T15:00:12.717831Z",
     "shell.execute_reply": "2025-07-16T15:00:12.716829Z",
     "shell.execute_reply.started": "2025-07-16T15:00:12.684720Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#| source-line-numbers: \"20-21\"\n",
    "#| class-source: \"numberLines\"\n",
    "class ThinkingBudgetLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        thinking_budget: int,\n",
    "        eot_token_id: int\n",
    "    ):\n",
    "        self.thinking_budget = thinking_budget\n",
    "        self.eot_token_id = eot_token_id\n",
    "        self.prompt_length_to_skip = None\n",
    "        self.very_large_number = 10_000\n",
    "\n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        first_time = self.prompt_length_to_skip is None\n",
    "        if first_time:\n",
    "            self.prompt_length_to_skip = input_ids.shape[-1]\n",
    "        scores_processed = scores.clone()\n",
    "        token_count = input_ids.shape[-1] - self.prompt_length_to_skip\n",
    "        if token_count == self.thinking_budget:\n",
    "            scores_processed[:, self.eot_token_id] = self.very_large_number\n",
    "        return scores_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f24450-4bbe-4bea-ba5f-e8e662bef5a8",
   "metadata": {},
   "source": [
    "We can instantiate the logits processor with a thinking budget of $100$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4768ce8a-0f26-4f38-b98d-ee84e5067533",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:12.719464Z",
     "iopub.status.busy": "2025-07-16T15:00:12.718971Z",
     "iopub.status.idle": "2025-07-16T15:00:12.743358Z",
     "shell.execute_reply": "2025-07-16T15:00:12.742407Z",
     "shell.execute_reply.started": "2025-07-16T15:00:12.719427Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "logits_processor = [\n",
    "    ThinkingBudgetLogitsProcessor(\n",
    "        thinking_budget=100,\n",
    "        eot_token_id=tokenizer.encode(\"</think>\")[0],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea76ffbe-6bf4-4e12-952b-07df3c6fae01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:12.744999Z",
     "iopub.status.busy": "2025-07-16T15:00:12.744500Z",
     "iopub.status.idle": "2025-07-16T15:00:15.536046Z",
     "shell.execute_reply": "2025-07-16T15:00:15.534969Z",
     "shell.execute_reply.started": "2025-07-16T15:00:12.744960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking, \"What's 2 + 2?\" Let me think about how to approach this. First, I need to make sure I understand the question correctly. The user is probably looking for the sum of 2 plus 2, which is 4. But maybe they're trying to get a different answer, like a joke or something else. Let me check if there's any context I'm missing.\n",
      "\n",
      "Wait, sometimes people use \"2 + 2</think>\n",
      "\n",
      "The answer is 4.\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "assistant_response, prompt_length, thinking_length, response_length = generate_response(\n",
    "    user_input,\n",
    "    logits_processor=logits_processor,\n",
    "    enable_thinking=True\n",
    ")\n",
    "\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74065879-9106-4267-a4de-efde1eabafe6",
   "metadata": {},
   "source": [
    "The number of tokens is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bbc5992-c19c-4885-abb2-7a272b6065ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:15.537731Z",
     "iopub.status.busy": "2025-07-16T15:00:15.537172Z",
     "iopub.status.idle": "2025-07-16T15:00:15.543504Z",
     "shell.execute_reply": "2025-07-16T15:00:15.542548Z",
     "shell.execute_reply.started": "2025-07-16T15:00:15.537678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# prompt tokens: 16\n",
      "# thinking tokens: 99\n",
      "# response tokens: 8\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "print(\n",
    "    f\"# prompt tokens: {prompt_length}\\n# thinking tokens: {thinking_length}\\n# response tokens: {response_length}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae5539-2780-42f2-a92f-f0bd71800142",
   "metadata": {},
   "source": [
    "We have forced the model to stop thinking after a fixed number of thinking budget and then provide a response.\n",
    "This is very convenient if we want to limit the verbosity of reasoning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeea7fb-1782-4b40-8f4b-c8956fa3e60d",
   "metadata": {},
   "source": [
    "## Budget Forcing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc40aadb-c4c7-45e1-93cc-a1c2637bdf2a",
   "metadata": {},
   "source": [
    "Another application of logits processors is budget forcing. Budget forcing consists of forcing the model to continue thinking for a longer time for particularly difficult problems by appending the token `Wait` when the language model wants to stop thinking. The idea comes from the paper [s1: Simple test-time scaling](https://arxiv.org/abs/2501.19393) which claims that budget forcing improves the language model accuracy from $50\\%$ to $57\\%$ in [AIME 2024 dataset](https://huggingface.co/datasets/HuggingFaceH4/aime_2024). In that paper, the authors force the model to continue thinking by replacing the stop thinking token (`</think>`) by the token `Wait`.\n",
    "\n",
    "Using logits processors, we can make it more general by replacing the stop thinking token (`</think>`) by a phrase, for example the phrase `Wait, let me check my answer`.\n",
    "\n",
    "Similar to the previous section, the logits processor would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4b8e98b-defa-4b07-830b-0488bd59e2c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:15.545184Z",
     "iopub.status.busy": "2025-07-16T15:00:15.544663Z",
     "iopub.status.idle": "2025-07-16T15:00:15.584550Z",
     "shell.execute_reply": "2025-07-16T15:00:15.583556Z",
     "shell.execute_reply.started": "2025-07-16T15:00:15.545144Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#| source-line-numbers: \"16,27-41\"\n",
    "#| class-source: \"numberLines\"\n",
    "class BudgetForcingLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        thinking_budget: int,\n",
    "        eot_token_id: int,\n",
    "        replacement_tokens_ids: int,\n",
    "        eos_token_ids: List[int],\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        self.thinking_budget = thinking_budget\n",
    "        self.eot_token_id = eot_token_id\n",
    "        self.replacement_tokens_ids = replacement_tokens_ids\n",
    "        self.eos_token_ids = eos_token_ids\n",
    "        self.prompt_length_to_skip = None\n",
    "        self.very_large_number = 10_000\n",
    "        self.index = -1\n",
    "\n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        first_time = self.prompt_length_to_skip is None\n",
    "        if first_time:\n",
    "            self.prompt_length_to_skip = input_ids.shape[-1]\n",
    "        scores_processed = scores.clone()\n",
    "        token_count = input_ids.shape[-1] - self.prompt_length_to_skip\n",
    "        if token_count < self.thinking_budget:\n",
    "            token_chosen_id = torch.argmax(scores_processed).item()\n",
    "            if token_chosen_id in self.eos_token_ids:\n",
    "                for eos_token_id in self.eos_token_ids:\n",
    "                    scores_processed[:, eos_token_id] = -torch.inf\n",
    "            if (token_chosen_id == self.eot_token_id) and (self.index == -1):\n",
    "                scores_processed[:, self.eot_token_id] = -torch.inf\n",
    "                self.index = 0\n",
    "            if len(self.replacement_tokens_ids) > self.index >= 0:\n",
    "                scores_processed[:, self.replacement_tokens_ids[self.index]] = (\n",
    "                    self.very_large_number\n",
    "                )\n",
    "                self.index += 1\n",
    "            if self.index == len(self.replacement_tokens_ids):\n",
    "                self.index = -1\n",
    "        return scores_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538e9365-2a85-4496-ae64-441e48a6d56b",
   "metadata": {},
   "source": [
    "Without a logits processor, we saw that for the question `What's 2 + 2?`, the model would think for $269$ tokens. Let's instantiate the logits processor with a forced thinking budget of $400$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "314f73fc-2f87-4707-b7be-3913067b5611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:15.586154Z",
     "iopub.status.busy": "2025-07-16T15:00:15.585634Z",
     "iopub.status.idle": "2025-07-16T15:00:15.610535Z",
     "shell.execute_reply": "2025-07-16T15:00:15.609554Z",
     "shell.execute_reply.started": "2025-07-16T15:00:15.586112Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "logits_processor = [\n",
    "    BudgetForcingLogitsProcessor(\n",
    "        thinking_budget=400,\n",
    "        eot_token_id=tokenizer.encode(\"</think>\")[0],\n",
    "        replacement_tokens_ids=tokenizer.encode(\" Wait, let me check my answer\"),\n",
    "        eos_token_ids=[tokenizer.eos_token_id, tokenizer.pad_token_id],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de242f31-954d-4d8c-90a8-55b6a1260a41",
   "metadata": {},
   "source": [
    "Here is the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4be0cd1e-5cb1-45fc-973c-c27c7f640316",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:15.612102Z",
     "iopub.status.busy": "2025-07-16T15:00:15.611607Z",
     "iopub.status.idle": "2025-07-16T15:00:26.782491Z",
     "shell.execute_reply": "2025-07-16T15:00:26.781763Z",
     "shell.execute_reply.started": "2025-07-16T15:00:15.612064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking, \"What's 2 + 2?\" Let me think about how to approach this. First, I need to make sure I understand the question correctly. The user is probably looking for the sum of 2 plus 2, which is 4. But maybe they're trying to get a different answer, like a joke or something else. Let me check if there's any context I'm missing.\n",
      "\n",
      "Wait, sometimes people use \"2 + 2\" in a different way. For example, in some languages, numbers are written differently, but in English, it's straightforward. Also, maybe the user is testing if I can recognize that 2 + 2 equals 4. But I should also consider if there's any trick here. For instance, if they're using a calculator, the result would be 4. But since the question is simple, the answer is 4.\n",
      "\n",
      "I should also make sure there's no hidden meaning or cultural context. In most basic math problems, 2 + 2 is 4. So the answer is 4. I don't see any other possible interpretations here. The user might just want the direct answer. Let me confirm once more. Yes, 2 plus 2 is 4. So the final answer should be 4.\n",
      " Wait, let me check my answer again. 2 + 2 is definitely 4. I think that's correct. No tricks here. The user might be testing if I can do basic arithmetic. So yeah, the answer is 4.\n",
      " Wait, let me check my answer again. 2 + 2 is 4. I think that's correct. No tricks here. The user might be testing if I can do basic arithmetic. So the answer is 4.\n",
      " Wait, let me check my answer again. 2 + 2 is 4. I think that's correct. No tricks here. The user might be testing if I can do basic arithmetic. So the answer is 4.\n",
      " Okay, I think I'm confident with this answer. The user is probably just looking for the sum of 2 plus 2, which is 4.\n",
      "</think>\n",
      "\n",
      "2 + 2 equals 4.\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "assistant_response, prompt_length, thinking_length, response_length = generate_response(\n",
    "    user_input,\n",
    "    logits_processor=logits_processor,\n",
    "    enable_thinking=True\n",
    ")\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b83cb1-ceb9-4c19-8e23-7490ec3d5e3b",
   "metadata": {},
   "source": [
    "Here are the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "040d3b54-1f79-46e9-8a00-a8ed14f0673b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T15:00:26.784008Z",
     "iopub.status.busy": "2025-07-16T15:00:26.783485Z",
     "iopub.status.idle": "2025-07-16T15:00:26.788805Z",
     "shell.execute_reply": "2025-07-16T15:00:26.787803Z",
     "shell.execute_reply.started": "2025-07-16T15:00:26.783968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# prompt tokens: 16\n",
      "# thinking tokens: 445\n",
      "# response tokens: 10\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show the code\"\n",
    "print(\n",
    "    f\"# prompt tokens: {prompt_length}\\n# thinking tokens: {thinking_length}\\n# response tokens: {response_length}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c49b33-9457-4de4-a237-d680a47ed6ab",
   "metadata": {},
   "source": [
    "We managed to make the language model to think for a longer time. The answer did not change for this simple question but it would be interesting to see how the answer changes for other questions like in the [s1: Simple test-time scaling](https://arxiv.org/abs/2501.19393) paper.\n",
    "\n",
    "It would be interesting to study if other phrases make the model take completely different reasoning paths and if those reasoning paths improve the language model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6242d6-3227-4df8-9dec-cb781379adb5",
   "metadata": {},
   "source": [
    "In this post, we have seen what logits processors are, how to create them, how to use them, as well as some practical applications. There are many more interesting applications and we are just scratching the surface of what's possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca09b19-b137-490e-9bd0-f481b77c14e8",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [logits-processor-zoo](https://github.com/NVIDIA/logits-processor-zoo): You can learn a lot by looking at the code in this repo. The trick to get the prompt length as well as the trick for the replacing phrase are taking from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dba7e8-5b02-4c15-859a-cb04d9b57dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
