{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9c8c40b2-5446-454c-9948-2f41b1030e4b",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Understanding Chat Templates\"\n",
    "author: \"Alonso Silva\"\n",
    "date: \"2025-06-20\"\n",
    "categories: [code, analysis]\n",
    "format:\n",
    "    html:\n",
    "        code-tools: true\n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc8c2a-e886-4996-bd63-2f4caa76e870",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a4fe28-fea6-4eb1-9ce8-c03c40cc5018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:46:49.540092Z",
     "iopub.status.busy": "2025-06-20T16:46:49.539545Z",
     "iopub.status.idle": "2025-06-20T16:46:49.547486Z",
     "shell.execute_reply": "2025-06-20T16:46:49.546596Z",
     "shell.execute_reply.started": "2025-06-20T16:46:49.540033Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "import urllib.parse\n",
    "from IPython.display import IFrame\n",
    "\n",
    "def embed_repl(code_str: str):\n",
    "    code = urllib.parse.quote(code_str)\n",
    "    return IFrame(f\"https://jupyterlite.github.io/demo/repl/index.html?toolbar=1&kernel=python&promptCellPosition=left&hideCodeInput=0&clearCodeContentOnExecute=1&code={code}&execute=0\", width=900, height=400)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c296299-8737-449a-8347-abcf77cc0bb6",
   "metadata": {},
   "source": [
    "If you have worked with LLMs, you might have worked with lists of `messages`. Indeed, when you send a request to an LLM, we can call it a `user messsage` and the response can be called an `assistant message`. A conversation would consist of a list of a `user message` followed by an `assistant message` followed by a `user message` followed by an `assistant message`, etc. However, an LLM takes one text and outputs another text, so you might be wondering what's the input text that's being passed to the LLM. So what's going on?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea533636-a575-4805-9565-7a8f5401b3ea",
   "metadata": {},
   "source": [
    "A user message will be something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abee0f48-fd9c-445b-ab46-1e43b0fc369b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:46:49.552824Z",
     "iopub.status.busy": "2025-06-20T16:46:49.552329Z",
     "iopub.status.idle": "2025-06-20T16:46:49.573223Z",
     "shell.execute_reply": "2025-06-20T16:46:49.572292Z",
     "shell.execute_reply.started": "2025-06-20T16:46:49.552753Z"
    }
   },
   "outputs": [],
   "source": [
    "#|echo: true\n",
    "user_message = [{\"role\": \"user\", \"content\": \"Hi!\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f4972-1e96-4276-a93f-9aa1bef482b2",
   "metadata": {},
   "source": [
    "You can pass the user message to the LLM to obtain an assistant message (it will take a few minutes the first time since it will download a small LLM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a335a140-ff57-4936-bc92-8a2f283d17fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:46:49.574893Z",
     "iopub.status.busy": "2025-06-20T16:46:49.574372Z",
     "iopub.status.idle": "2025-06-20T16:46:49.602763Z",
     "shell.execute_reply": "2025-06-20T16:46:49.601825Z",
     "shell.execute_reply.started": "2025-06-20T16:46:49.574849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"https://jupyterlite.github.io/demo/repl/index.html?toolbar=1&kernel=python&promptCellPosition=left&hideCodeInput=0&clearCodeContentOnExecute=1&code=%25pip%20install%20transformers_js_py%0A%0Afrom%20transformers_js_py%20import%20import_transformers_js%0A%0Atransformers%20%3D%20await%20import_transformers_js%28%29%0Apipeline%20%3D%20transformers.pipeline%0Apipe%20%3D%20await%20pipeline%28%27text-generation%27%2C%20%27onnx-community/Qwen2.5-0.5B-Instruct%27%29%0A%0Amessage%20%3D%20%5B%7B%22role%22%3A%20%22user%22%2C%20%22content%22%3A%20%22Hi%21%22%7D%5D%0Aout%20%3D%20await%20pipe%28message%2C%20max_new_tokens%3D10%2C%20temperature%3D0%2C%20seed%3D42%29%0Aout%5B0%5D%5B%27generated_text%27%5D%5B-1%5D&execute=0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x75933029f6b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "embed_repl(\"\"\"%pip install transformers_js_py\n",
    "\n",
    "from transformers_js_py import import_transformers_js\n",
    "\n",
    "transformers = await import_transformers_js()\n",
    "pipeline = transformers.pipeline\n",
    "pipe = await pipeline('text-generation', 'onnx-community/Qwen2.5-0.5B-Instruct')\n",
    "\n",
    "message = [{\"role\": \"user\", \"content\": \"Hi!\"}]\n",
    "out = await pipe(message, max_new_tokens=10, temperature=0, seed=42)\n",
    "out[0]['generated_text'][-1]\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7f3a2-c48d-462e-929b-c19c4d84d81f",
   "metadata": {},
   "source": [
    "The assistant message will be something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "768be70f-4a35-4013-9473-eebf449069b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:46:49.604505Z",
     "iopub.status.busy": "2025-06-20T16:46:49.603897Z",
     "iopub.status.idle": "2025-06-20T16:46:49.619697Z",
     "shell.execute_reply": "2025-06-20T16:46:49.618782Z",
     "shell.execute_reply.started": "2025-06-20T16:46:49.604458Z"
    }
   },
   "outputs": [],
   "source": [
    "#|echo: true\n",
    "assistant_message = [{\"role\": \"user\", \"content\": \"Hello! How can I assist you today?\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b1c1cc-f48a-468f-98aa-53f557eb8235",
   "metadata": {},
   "source": [
    "To have a conversation, you can pass it a list of messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99b7abc-fe71-4c23-993f-d3bc749abcd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:46:49.621371Z",
     "iopub.status.busy": "2025-06-20T16:46:49.620838Z",
     "iopub.status.idle": "2025-06-20T16:46:49.642509Z",
     "shell.execute_reply": "2025-06-20T16:46:49.641529Z",
     "shell.execute_reply.started": "2025-06-20T16:46:49.621326Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e734f-4525-400e-9a89-b4f7ed820f7f",
   "metadata": {},
   "source": [
    "How do we convert a list of messages to an input text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6025f665-4385-421d-bb04-a98382eb876d",
   "metadata": {},
   "source": [
    "The first time I encountered this was with [Thomas Capelle](https://x.com/capetorch) from [W&B](https://wandb.ai/). Our conversation went something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc94749c-c3e8-4aba-aee6-0476328df7bb",
   "metadata": {},
   "source": [
    "> **Thomas:** An LLM has no idea about user or assistant messages, it is just an autocompletion program.\n",
    "> \n",
    "> **Alonso:** So what's going on with the list of messages I'm sending it? Are they just concatenated or what?\n",
    "> \n",
    "> **Thomas**: No, no, no, it's more complex than that, specially when you work with tools. You should check the model's chat template.\n",
    "> \n",
    "> **Alonso**: How do I do that?\n",
    "> \n",
    "> **Thomas**: It's stored in the tokenizer, let me show you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ddd5de-74e2-4b44-8ef4-cd41e87efee7",
   "metadata": {},
   "source": [
    "## The Chat Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe256a63-a7f2-4ec8-a4f8-e74a461437a9",
   "metadata": {},
   "source": [
    "The chat template takes as input a list of messages (and tools but we will talk about it later) and convert them into a single string.\n",
    "Let's see what it does with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a47bdf-761f-4aec-8abd-1db825a542ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:46:49.644528Z",
     "iopub.status.busy": "2025-06-20T16:46:49.643945Z",
     "iopub.status.idle": "2025-06-20T16:46:49.667160Z",
     "shell.execute_reply": "2025-06-20T16:46:49.666310Z",
     "shell.execute_reply.started": "2025-06-20T16:46:49.644482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"https://jupyterlite.github.io/demo/repl/index.html?toolbar=1&kernel=python&promptCellPosition=left&hideCodeInput=0&clearCodeContentOnExecute=1&code=%25pip%20install%20transformers_js_py%0A%0Afrom%20transformers_js_py%20import%20import_transformers_js%0A%0Atransformers%20%3D%20await%20import_transformers_js%28%29%0AAutoTokenizer%20%3D%20transformers.AutoTokenizer%0A%0Amodel_id%20%3D%20%27Qwen/Qwen2.5-0.5B-Instruct%27%0Atokenizer%20%3D%20await%20AutoTokenizer.from_pretrained%28model_id%29%0A%0Amessage%20%3D%20%5B%7B%22role%22%3A%20%22user%22%2C%20%22content%22%3A%20%22Hi%21%22%7D%5D%0Aprint%28tokenizer.apply_chat_template%28message%2C%20tokenize%3DFalse%29%29&execute=0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7593302c3a10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "embed_repl(\"\"\"%pip install transformers_js_py\n",
    "\n",
    "from transformers_js_py import import_transformers_js\n",
    "\n",
    "transformers = await import_transformers_js()\n",
    "AutoTokenizer = transformers.AutoTokenizer\n",
    "\n",
    "model_id = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "tokenizer = await AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "message = [{\"role\": \"user\", \"content\": \"Hi!\"}]\n",
    "print(tokenizer.apply_chat_template(message, tokenize=False))\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206198b8-4464-4e34-a14c-c2ea1974527e",
   "metadata": {},
   "source": [
    "You should get the following:\n",
    "```\n",
    "<|im_start|>system\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "Hi!<|im_end|>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81829bbb-b1d4-45f9-817d-359a378c314d",
   "metadata": {},
   "source": [
    "Notice that when you don't provide a `system message`, the model `Qwen/Qwen2.5-0.5B-Instruct` adds a `system message` (\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"). Perhaps you want to change that to a different `system message`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda790a7-d12f-44c8-ba49-28b7c06eb603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:46:49.669104Z",
     "iopub.status.busy": "2025-06-20T16:46:49.668296Z",
     "iopub.status.idle": "2025-06-20T16:46:49.690506Z",
     "shell.execute_reply": "2025-06-20T16:46:49.689591Z",
     "shell.execute_reply.started": "2025-06-20T16:46:49.669060Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"https://jupyterlite.github.io/demo/repl/index.html?toolbar=1&kernel=python&promptCellPosition=left&hideCodeInput=0&clearCodeContentOnExecute=1&code=%25pip%20install%20transformers_js_py%0A%0Afrom%20transformers_js_py%20import%20import_transformers_js%0A%0Atransformers%20%3D%20await%20import_transformers_js%28%29%0AAutoTokenizer%20%3D%20transformers.AutoTokenizer%0A%0Amodel_id%20%3D%20%27Qwen/Qwen2.5-0.5B-Instruct%27%0Atokenizer%20%3D%20await%20AutoTokenizer.from_pretrained%28model_id%29%0A%0Amessages%20%3D%20%5B%0A%20%20%20%20%7B%22role%22%3A%20%22system%22%2C%20%22content%22%3A%20%22You%20are%20a%20helpful%20assistant.%22%7D%2C%0A%20%20%20%20%7B%22role%22%3A%20%22user%22%2C%20%22content%22%3A%20%22Hi%21%22%7D%2C%0A%5D%0Aprint%28tokenizer.apply_chat_template%28messages%2C%20tokenize%3DFalse%29%29&execute=0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7593302c2840>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "embed_repl(\"\"\"%pip install transformers_js_py\n",
    "\n",
    "from transformers_js_py import import_transformers_js\n",
    "\n",
    "transformers = await import_transformers_js()\n",
    "AutoTokenizer = transformers.AutoTokenizer\n",
    "\n",
    "model_id = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "tokenizer = await AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi!\"},\n",
    "]\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False))\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b493cca-deed-418d-8176-4a06bbae0c4b",
   "metadata": {},
   "source": [
    "We can also see how does the chat template convert a conversation like this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73f7876b-f943-4ce3-8ad9-a2eb77f15d98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:46:49.694746Z",
     "iopub.status.busy": "2025-06-20T16:46:49.694213Z",
     "iopub.status.idle": "2025-06-20T16:46:49.707822Z",
     "shell.execute_reply": "2025-06-20T16:46:49.706881Z",
     "shell.execute_reply.started": "2025-06-20T16:46:49.694701Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6150bf6-be06-442d-a1e8-e31788149627",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:46:49.709692Z",
     "iopub.status.busy": "2025-06-20T16:46:49.708952Z",
     "iopub.status.idle": "2025-06-20T16:46:49.731403Z",
     "shell.execute_reply": "2025-06-20T16:46:49.730435Z",
     "shell.execute_reply.started": "2025-06-20T16:46:49.709645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"https://jupyterlite.github.io/demo/repl/index.html?toolbar=1&kernel=python&promptCellPosition=left&hideCodeInput=0&clearCodeContentOnExecute=1&code=%25pip%20install%20transformers_js_py%0A%0Afrom%20transformers_js_py%20import%20import_transformers_js%0A%0Atransformers%20%3D%20await%20import_transformers_js%28%29%0AAutoTokenizer%20%3D%20transformers.AutoTokenizer%0A%0Amodel_id%20%3D%20%27Qwen/Qwen2.5-0.5B-Instruct%27%0Atokenizer%20%3D%20await%20AutoTokenizer.from_pretrained%28model_id%29%0A%0Amessages%20%3D%20%5B%0A%20%20%20%20%7B%22role%22%3A%20%22system%22%2C%20%22content%22%3A%20%22You%20are%20a%20helpful%20assistant.%22%7D%2C%0A%20%20%20%20%7B%22role%22%3A%20%22user%22%2C%20%22content%22%3A%20%22Hi%21%22%7D%2C%0A%20%20%20%20%7B%22role%22%3A%20%22assistant%22%2C%20%22content%22%3A%20%22Hello%21%20How%20can%20I%20assist%20you%20today%3F%22%7D%2C%0A%20%20%20%20%7B%22role%22%3A%20%22user%22%2C%20%22content%22%3A%20%22What%27s%20the%20capital%20of%20France%3F%22%7D%2C%0A%5D%0Aprint%28tokenizer.apply_chat_template%28messages%2C%20tokenize%3DFalse%29%29&execute=0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7593302c3fb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "embed_repl(\"\"\"%pip install transformers_js_py\n",
    "\n",
    "from transformers_js_py import import_transformers_js\n",
    "\n",
    "transformers = await import_transformers_js()\n",
    "AutoTokenizer = transformers.AutoTokenizer\n",
    "\n",
    "model_id = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "tokenizer = await AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n",
    "]\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False))\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82634f64-f980-48fc-8c0e-79b43d80c116",
   "metadata": {},
   "source": [
    "Each model has its own chat template. Let's take a look at `Mistral-7B-v0.3` chat template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c4cbab2-152f-4d60-97c9-3cd4ee2c4028",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:46:49.733061Z",
     "iopub.status.busy": "2025-06-20T16:46:49.732617Z",
     "iopub.status.idle": "2025-06-20T16:46:49.754597Z",
     "shell.execute_reply": "2025-06-20T16:46:49.753760Z",
     "shell.execute_reply.started": "2025-06-20T16:46:49.733018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"https://jupyterlite.github.io/demo/repl/index.html?toolbar=1&kernel=python&promptCellPosition=left&hideCodeInput=0&clearCodeContentOnExecute=1&code=%25pip%20install%20transformers_js_py%0A%0Afrom%20transformers_js_py%20import%20import_transformers_js%0A%0Atransformers%20%3D%20await%20import_transformers_js%28%29%0AAutoTokenizer%20%3D%20transformers.AutoTokenizer%0A%0Amodel_id%20%3D%20%22MaziyarPanahi/Mistral-7B-Instruct-v0.3%22%0Atokenizer%20%3D%20await%20AutoTokenizer.from_pretrained%28model_id%29%0A%0Amessages%20%3D%20%5B%0A%20%20%20%20%7B%22role%22%3A%20%22user%22%2C%20%22content%22%3A%20%22Hi%21%22%7D%2C%0A%20%20%20%20%7B%22role%22%3A%20%22assistant%22%2C%20%22content%22%3A%20%22Hello%21%20How%20can%20I%20assist%20you%20today%3F%22%7D%2C%0A%20%20%20%20%7B%22role%22%3A%20%22user%22%2C%20%22content%22%3A%20%22What%27s%20the%20capital%20of%20France%3F%22%7D%2C%0A%5D%0Aprint%28tokenizer.apply_chat_template%28messages%2C%20tokenize%3DFalse%29%29&execute=0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x759331f0c0b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "embed_repl(\"\"\"%pip install transformers_js_py\n",
    "\n",
    "from transformers_js_py import import_transformers_js\n",
    "\n",
    "transformers = await import_transformers_js()\n",
    "AutoTokenizer = transformers.AutoTokenizer\n",
    "\n",
    "model_id = \"MaziyarPanahi/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = await AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n",
    "]\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False))\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84385c8e-a2d9-40c6-99be-38febf7418f2",
   "metadata": {},
   "source": [
    "You should get:\n",
    "```\n",
    "<s>[INST] Hi! [/INST]Hello! How can I assist you today?</s>[INST] What's the capital of France? [/INST]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1539b492-c14a-425b-bb60-844ac9153641",
   "metadata": {},
   "source": [
    "Since after sending a `user message`, you expect an `assistant message`, you can help the model by basically saying \"Now, it's your turn!\". This is so useful that it has been incorporated into the chat template itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa94601-5688-4f0a-b41d-aaf08710bae6",
   "metadata": {},
   "source": [
    "The instruction is:\n",
    "```\n",
    "tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79c75c04-5523-4a63-bf96-69f1d7339b72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:46:49.756545Z",
     "iopub.status.busy": "2025-06-20T16:46:49.755706Z",
     "iopub.status.idle": "2025-06-20T16:46:49.777791Z",
     "shell.execute_reply": "2025-06-20T16:46:49.776916Z",
     "shell.execute_reply.started": "2025-06-20T16:46:49.756499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"https://jupyterlite.github.io/demo/repl/index.html?toolbar=1&kernel=python&promptCellPosition=left&hideCodeInput=0&clearCodeContentOnExecute=1&code=%25pip%20install%20transformers_js_py%0A%0Afrom%20transformers_js_py%20import%20import_transformers_js%0A%0Atransformers%20%3D%20await%20import_transformers_js%28%29%0AAutoTokenizer%20%3D%20transformers.AutoTokenizer%0A%0Amodel_id%20%3D%20%27Qwen/Qwen2.5-0.5B-Instruct%27%0Atokenizer%20%3D%20await%20AutoTokenizer.from_pretrained%28model_id%29%0A%0Amessages%20%3D%20%5B%0A%20%20%20%20%7B%22role%22%3A%20%22system%22%2C%20%22content%22%3A%20%22You%20are%20a%20helpful%20assistant.%22%7D%2C%0A%20%20%20%20%7B%22role%22%3A%20%22user%22%2C%20%22content%22%3A%20%22Hi%21%22%7D%2C%0A%5D%0Aprint%28tokenizer.apply_chat_template%28messages%2C%20tokenize%3DFalse%2C%20add_generation_prompt%3DTrue%29%29&execute=0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7593302e5e20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "embed_repl(\"\"\"%pip install transformers_js_py\n",
    "\n",
    "from transformers_js_py import import_transformers_js\n",
    "\n",
    "transformers = await import_transformers_js()\n",
    "AutoTokenizer = transformers.AutoTokenizer\n",
    "\n",
    "model_id = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "tokenizer = await AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi!\"},\n",
    "]\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8727485b-d9ad-4343-8b8e-32a339b10c5c",
   "metadata": {},
   "source": [
    "You should get:\n",
    "```\n",
    "<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "Hi!<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb45c56-e3ab-49e5-8bca-84e7261bfcae",
   "metadata": {},
   "source": [
    "What's interesting to me is that when you suggest to force a tool call or to force a tool, they look at you thinking you're crazy even though it's exactly the same thing (and it's probably what OpenAI [already does](https://platform.openai.com/docs/guides/function-calling?api-mode=chat#additional-configurations) with the `tool_choice=required` and `tool_choice: {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b61144-31c4-4958-9e7b-7991c615e79a",
   "metadata": {},
   "source": [
    "## Tool calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc551c5-c03d-4b86-b2e3-5badfd3739cc",
   "metadata": {},
   "source": [
    "The chat template also handles tool calls. That means that we can provide a list of tools (let's do one as an example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a31433a-abf7-4eb6-9464-21a845dbcfe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:46:49.779814Z",
     "iopub.status.busy": "2025-06-20T16:46:49.778966Z",
     "iopub.status.idle": "2025-06-20T16:46:49.796043Z",
     "shell.execute_reply": "2025-06-20T16:46:49.795139Z",
     "shell.execute_reply.started": "2025-06-20T16:46:49.779765Z"
    }
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"Python_REPL\",\n",
    "            \"description\": \"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "            \"parameters\": {\n",
    "                \"properties\": {\n",
    "                    \"python_code\": {\n",
    "                        \"description\": \"Valid python command.\",\n",
    "                        \"type\": \"string\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"python_code\"],\n",
    "                \"type\": \"object\",\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c5b1f-65ba-4388-8676-6e4561be3266",
   "metadata": {},
   "source": [
    "The instruction is:\n",
    "```\n",
    "tokenizer.apply_chat_template(messages, tokenize=False, tools=tools, add_generation_prompt=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d31b81-c49b-4ae6-a663-46c227ec2857",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:46:49.798101Z",
     "iopub.status.busy": "2025-06-20T16:46:49.797366Z",
     "iopub.status.idle": "2025-06-20T16:46:49.825506Z",
     "shell.execute_reply": "2025-06-20T16:46:49.824468Z",
     "shell.execute_reply.started": "2025-06-20T16:46:49.798056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"https://jupyterlite.github.io/demo/repl/index.html?toolbar=1&kernel=python&promptCellPosition=left&hideCodeInput=0&clearCodeContentOnExecute=1&code=%25pip%20install%20transformers_js_py%0A%0Afrom%20transformers_js_py%20import%20import_transformers_js%0A%0Atransformers%20%3D%20await%20import_transformers_js%28%29%0AAutoTokenizer%20%3D%20transformers.AutoTokenizer%0A%0Amodel_id%20%3D%20%27Qwen/Qwen2.5-0.5B-Instruct%27%0Atokenizer%20%3D%20await%20AutoTokenizer.from_pretrained%28model_id%29%0A%0Amessages%20%3D%20%5B%0A%20%20%20%20%7B%22role%22%3A%20%22system%22%2C%20%22content%22%3A%20%22You%20are%20a%20helpful%20assistant.%22%7D%2C%0A%20%20%20%20%7B%22role%22%3A%20%22user%22%2C%20%22content%22%3A%20%22What%27s%202%20to%20the%20power%20of%205%3F%22%7D%2C%0A%5D%0A%0Atools%20%3D%20%5B%0A%20%20%20%20%7B%0A%20%20%20%20%20%20%20%20%22type%22%3A%20%22function%22%2C%0A%20%20%20%20%20%20%20%20%22function%22%3A%20%7B%0A%20%20%20%20%20%20%20%20%20%20%20%20%22name%22%3A%20%22Python_REPL%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%22description%22%3A%20%22A%20Python%20shell.%20Use%20this%20to%20execute%20python%20commands.%20Input%20should%20be%20a%20valid%20python%20command.%20If%20you%20want%20to%20see%20the%20output%20of%20a%20value%2C%20you%20should%20print%20it%20out%20with%20%60print%28...%29%60.%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%22parameters%22%3A%20%7B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22properties%22%3A%20%7B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22python_code%22%3A%20%7B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22description%22%3A%20%22Valid%20python%20command.%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22type%22%3A%20%22string%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22required%22%3A%20%5B%22python_code%22%5D%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22type%22%3A%20%22object%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%7D%0A%5D%0A%0Aprint%28tokenizer.apply_chat_template%28messages%2C%20tokenize%3DFalse%2C%20tools%3Dtools%2C%20add_generation_prompt%3DTrue%29%29&execute=0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7593302e4230>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "embed_repl(\"\"\"%pip install transformers_js_py\n",
    "\n",
    "from transformers_js_py import import_transformers_js\n",
    "\n",
    "transformers = await import_transformers_js()\n",
    "AutoTokenizer = transformers.AutoTokenizer\n",
    "\n",
    "model_id = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "tokenizer = await AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's 2 to the power of 5?\"},\n",
    "]\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"Python_REPL\",\n",
    "            \"description\": \"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "            \"parameters\": {\n",
    "                \"properties\": {\n",
    "                    \"python_code\": {\n",
    "                        \"description\": \"Valid python command.\",\n",
    "                        \"type\": \"string\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"python_code\"],\n",
    "                \"type\": \"object\",\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False, tools=tools, add_generation_prompt=True))\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7260166-39e1-4de8-9293-a8a98d0c8604",
   "metadata": {},
   "source": [
    "You should get the following:\n",
    "```\n",
    "<|im_start|>system\n",
    "You are a helpful assistant.\n",
    "\n",
    "# Tools\n",
    "\n",
    "You may call one or more functions to assist with the user query.\n",
    "\n",
    "You are provided with function signatures within <tools></tools> XML tags:\n",
    "<tools>\n",
    "{\"type\": \"function\", \"function\": {\"name\": \"Python_REPL\", \"description\": \"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\", \"parameters\": {\"properties\": {\"python_code\": {\"description\": \"Valid python command.\", \"type\": \"string\"}}, \"required\": [\"python_code\"], \"type\": \"object\"}}}\n",
    "</tools>\n",
    "\n",
    "For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
    "<tool_call>\n",
    "{\"name\": <function-name>, \"arguments\": <args-json-object>}\n",
    "</tool_call><|im_end|>\n",
    "<|im_start|>user\n",
    "What's 2 to the power of 5?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266f84c6-eb19-4e90-8ea7-cb0d5d0f0aa8",
   "metadata": {},
   "source": [
    "Quite complex string indeed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa3791-8ab8-48ce-baa4-7a189493ee16",
   "metadata": {},
   "source": [
    "If you look at some other model `NousResearch/Hermes-3-Llama-3.1-8B`, you see the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc0f5946-28e3-405e-b54d-22d21d3f9647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:46:49.827040Z",
     "iopub.status.busy": "2025-06-20T16:46:49.826621Z",
     "iopub.status.idle": "2025-06-20T16:46:49.845243Z",
     "shell.execute_reply": "2025-06-20T16:46:49.844384Z",
     "shell.execute_reply.started": "2025-06-20T16:46:49.826997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"https://jupyterlite.github.io/demo/repl/index.html?toolbar=1&kernel=python&promptCellPosition=left&hideCodeInput=0&clearCodeContentOnExecute=1&code=%25pip%20install%20transformers_js_py%0A%0Afrom%20transformers_js_py%20import%20import_transformers_js%0A%0Atransformers%20%3D%20await%20import_transformers_js%28%29%0AAutoTokenizer%20%3D%20transformers.AutoTokenizer%0A%0Amodel_id%20%3D%20%27NousResearch/Hermes-3-Llama-3.1-8B%27%0Atokenizer%20%3D%20await%20AutoTokenizer.from_pretrained%28model_id%29%0A%0Amessages%20%3D%20%5B%0A%20%20%20%20%7B%22role%22%3A%20%22system%22%2C%20%22content%22%3A%20%22You%20are%20a%20helpful%20assistant.%22%7D%2C%0A%20%20%20%20%7B%22role%22%3A%20%22user%22%2C%20%22content%22%3A%20%22What%27s%202%20to%20the%20power%20of%205%3F%22%7D%2C%0A%5D%0A%0Atools%20%3D%20%5B%0A%20%20%20%20%7B%0A%20%20%20%20%20%20%20%20%22type%22%3A%20%22function%22%2C%0A%20%20%20%20%20%20%20%20%22function%22%3A%20%7B%0A%20%20%20%20%20%20%20%20%20%20%20%20%22name%22%3A%20%22Python_REPL%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%22description%22%3A%20%22A%20Python%20shell.%20Use%20this%20to%20execute%20python%20commands.%20Input%20should%20be%20a%20valid%20python%20command.%20If%20you%20want%20to%20see%20the%20output%20of%20a%20value%2C%20you%20should%20print%20it%20out%20with%20%60print%28...%29%60.%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%22parameters%22%3A%20%7B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22properties%22%3A%20%7B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22python_code%22%3A%20%7B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22description%22%3A%20%22Valid%20python%20command.%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22type%22%3A%20%22string%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22required%22%3A%20%5B%22python_code%22%5D%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%22type%22%3A%20%22object%22%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%7D%0A%5D%0A%0Aprint%28tokenizer.apply_chat_template%28messages%2C%20tokenize%3DFalse%2C%20tools%3Dtools%2C%20add_generation_prompt%3DTrue%29%29&execute=0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7593302e4800>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "embed_repl(\"\"\"%pip install transformers_js_py\n",
    "\n",
    "from transformers_js_py import import_transformers_js\n",
    "\n",
    "transformers = await import_transformers_js()\n",
    "AutoTokenizer = transformers.AutoTokenizer\n",
    "\n",
    "model_id = 'NousResearch/Hermes-3-Llama-3.1-8B'\n",
    "tokenizer = await AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's 2 to the power of 5?\"},\n",
    "]\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"Python_REPL\",\n",
    "            \"description\": \"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "            \"parameters\": {\n",
    "                \"properties\": {\n",
    "                    \"python_code\": {\n",
    "                        \"description\": \"Valid python command.\",\n",
    "                        \"type\": \"string\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"python_code\"],\n",
    "                \"type\": \"object\",\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False, tools=tools, add_generation_prompt=True))\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c9d4f7-c83a-4cdb-adea-ba3e49966ff7",
   "metadata": {},
   "source": [
    "You should get the following:\n",
    "```\n",
    "<|begin_of_text|><|im_start|>system\n",
    "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> {\"type\": \"function\", \"function\": {\"name\": \"Python_REPL\", \"description\": \"Python_REPL(python_code: str) - A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\n",
    "\n",
    "    Args:\n",
    "        python_code(str): Valid python command.\", \"parameters\": {\"properties\": {\"python_code\": {\"description\": \"Valid python command.\", \"type\": \"string\"}}, \"required\": [\"python_code\"], \"type\": \"object\"}} </tools>Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}, \"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}}, \"required\": [\"name\", \"arguments\"], \"title\": \"FunctionCall\", \"type\": \"object\"}}\n",
    "For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
    "<tool_call>\n",
    "{\"name\": <function-name>, \"arguments\": <args-dict>}\n",
    "</tool_call><|im_end|>\n",
    "<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "What's 2 to the power of 5?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec4959-581d-4329-897f-1afcd3ceef1b",
   "metadata": {},
   "source": [
    "I don't like this chat template. It appears that our messages have **two different and consecutive** system prompts. I prefer much more the previous chat template of the model \"Qwen/Qwen2.5-0.5B-Instruct\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f82869-f796-4d32-84b8-d679cdb66ebd",
   "metadata": {},
   "source": [
    "## Thinking mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68634e-78ea-4d26-b090-a227e475f9c7",
   "metadata": {},
   "source": [
    "A recent addition is the `enable_thinking` in some new reasoning models where the model will \"think\" between the XML tags `<think>...</think>`. For example in `Qwen/Qwen3-4B` the model has the possibility to reason (which is *the default*), but you can turn this option off if you want to. The instruction is:\n",
    "```\n",
    "tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ca69f04-6337-4faa-8827-2f4846ba3fd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:46:49.846866Z",
     "iopub.status.busy": "2025-06-20T16:46:49.846350Z",
     "iopub.status.idle": "2025-06-20T16:46:49.872038Z",
     "shell.execute_reply": "2025-06-20T16:46:49.871191Z",
     "shell.execute_reply.started": "2025-06-20T16:46:49.846822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"https://jupyterlite.github.io/demo/repl/index.html?toolbar=1&kernel=python&promptCellPosition=left&hideCodeInput=0&clearCodeContentOnExecute=1&code=%25pip%20install%20transformers_js_py%0A%0Afrom%20transformers_js_py%20import%20import_transformers_js%0A%0Atransformers%20%3D%20await%20import_transformers_js%28%29%0AAutoTokenizer%20%3D%20transformers.AutoTokenizer%0A%0Amodel_id%20%3D%20%27Qwen/Qwen3-4B%27%0Atokenizer%20%3D%20await%20AutoTokenizer.from_pretrained%28model_id%29%0A%0Amessages%20%3D%20%5B%0A%20%20%20%20%7B%22role%22%3A%20%22system%22%2C%20%22content%22%3A%20%22You%20are%20a%20helpful%20assistant.%22%7D%2C%0A%20%20%20%20%7B%22role%22%3A%20%22user%22%2C%20%22content%22%3A%20%22How%20many%20r%27s%20in%20strawberry%3F%22%7D%2C%0A%5D%0A%0Aprint%28tokenizer.apply_chat_template%28messages%2C%20tokenize%3DFalse%2C%20add_generation_prompt%3DTrue%2C%20enable_thinking%3DFalse%29%29&execute=0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7593302e6f60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "embed_repl(\"\"\"%pip install transformers_js_py\n",
    "\n",
    "from transformers_js_py import import_transformers_js\n",
    "\n",
    "transformers = await import_transformers_js()\n",
    "AutoTokenizer = transformers.AutoTokenizer\n",
    "\n",
    "model_id = 'Qwen/Qwen3-4B'\n",
    "tokenizer = await AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many r's in strawberry?\"},\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False))\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a47d30d-9a46-4a72-a9e1-5851bac9b948",
   "metadata": {},
   "source": [
    "You should get:\n",
    "```\n",
    "<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "How many r's in strawberry?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "<think>\n",
    "\n",
    "</think>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375f3bd-778f-4f50-a4c9-02a09c34e215",
   "metadata": {},
   "source": [
    "I saw some tweets (with multiple retweets) claiming they found a \"hack\" to make the model not think and it was appending `<think>\\n\\n</think>` while this is exactly what the chat template does!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e75428f-a461-4c10-955a-3a10252578a1",
   "metadata": {},
   "source": [
    "## How does the chat template handles this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507540eb-88e3-4069-bb9b-5cfa44987977",
   "metadata": {},
   "source": [
    "The chat template has been programmed in [Jinja](https://jinja.palletsprojects.com/en/stable/) which is usually used in web development.\n",
    "You can see the chat template with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "340f1070-d7ea-4221-b632-8fa613fb8061",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T16:46:49.873754Z",
     "iopub.status.busy": "2025-06-20T16:46:49.873225Z",
     "iopub.status.idle": "2025-06-20T16:46:49.891239Z",
     "shell.execute_reply": "2025-06-20T16:46:49.890335Z",
     "shell.execute_reply.started": "2025-06-20T16:46:49.873711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"https://jupyterlite.github.io/demo/repl/index.html?toolbar=1&kernel=python&promptCellPosition=left&hideCodeInput=0&clearCodeContentOnExecute=1&code=%25pip%20install%20transformers_js_py%0A%0Afrom%20transformers_js_py%20import%20import_transformers_js%0A%0Atransformers%20%3D%20await%20import_transformers_js%28%29%0AAutoTokenizer%20%3D%20transformers.AutoTokenizer%0A%0Amodel_id%20%3D%20%27Qwen/Qwen3-4B%27%0Atokenizer%20%3D%20await%20AutoTokenizer.from_pretrained%28model_id%29%0A%0Aprint%28tokenizer.chat_template%29&execute=0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7593302e7440>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "embed_repl(\"\"\"%pip install transformers_js_py\n",
    "\n",
    "from transformers_js_py import import_transformers_js\n",
    "\n",
    "transformers = await import_transformers_js()\n",
    "AutoTokenizer = transformers.AutoTokenizer\n",
    "\n",
    "model_id = 'Qwen/Qwen3-4B'\n",
    "tokenizer = await AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(tokenizer.chat_template)\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a99a531-12dc-4899-a2ae-b207ea03e389",
   "metadata": {},
   "source": [
    "You should see the following:\n",
    "```\n",
    "{%- if tools %}\n",
    "    {{- '<|im_start|>system\\n' }}\n",
    "    {%- if messages[0].role == 'system' %}\n",
    "        {{- messages[0].content + '\\n\\n' }}\n",
    "    {%- endif %}\n",
    "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
    "    {%- for tool in tools %}\n",
    "        {{- \"\\n\" }}\n",
    "        {{- tool | tojson }}\n",
    "    {%- endfor %}\n",
    "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
    "{%- else %}\n",
    "    {%- if messages[0].role == 'system' %}\n",
    "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
    "    {%- endif %}\n",
    "{%- endif %}\n",
    "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
    "{%- for message in messages[::-1] %}\n",
    "    {%- set index = (messages|length - 1) - loop.index0 %}\n",
    "    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n",
    "        {%- set ns.multi_step_tool = false %}\n",
    "        {%- set ns.last_query_index = index %}\n",
    "    {%- endif %}\n",
    "{%- endfor %}\n",
    "{%- for message in messages %}\n",
    "    {%- if message.content is string %}\n",
    "        {%- set content = message.content %}\n",
    "    {%- else %}\n",
    "        {%- set content = '' %}\n",
    "    {%- endif %}\n",
    "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
    "        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n",
    "    {%- elif message.role == \"assistant\" %}\n",
    "        {%- set reasoning_content = '' %}\n",
    "        {%- if message.reasoning_content is string %}\n",
    "            {%- set reasoning_content = message.reasoning_content %}\n",
    "        {%- else %}\n",
    "            {%- if '</think>' in content %}\n",
    "                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n",
    "                {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n",
    "            {%- endif %}\n",
    "        {%- endif %}\n",
    "        {%- if loop.index0 > ns.last_query_index %}\n",
    "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
    "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
    "            {%- else %}\n",
    "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
    "            {%- endif %}\n",
    "        {%- else %}\n",
    "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
    "        {%- endif %}\n",
    "        {%- if message.tool_calls %}\n",
    "            {%- for tool_call in message.tool_calls %}\n",
    "                {%- if (loop.first and content) or (not loop.first) %}\n",
    "                    {{- '\\n' }}\n",
    "                {%- endif %}\n",
    "                {%- if tool_call.function %}\n",
    "                    {%- set tool_call = tool_call.function %}\n",
    "                {%- endif %}\n",
    "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
    "                {{- tool_call.name }}\n",
    "                {{- '\", \"arguments\": ' }}\n",
    "                {%- if tool_call.arguments is string %}\n",
    "                    {{- tool_call.arguments }}\n",
    "                {%- else %}\n",
    "                    {{- tool_call.arguments | tojson }}\n",
    "                {%- endif %}\n",
    "                {{- '}\\n</tool_call>' }}\n",
    "            {%- endfor %}\n",
    "        {%- endif %}\n",
    "        {{- '<|im_end|>\\n' }}\n",
    "    {%- elif message.role == \"tool\" %}\n",
    "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
    "            {{- '<|im_start|>user' }}\n",
    "        {%- endif %}\n",
    "        {{- '\\n<tool_response>\\n' }}\n",
    "        {{- content }}\n",
    "        {{- '\\n</tool_response>' }}\n",
    "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
    "            {{- '<|im_end|>\\n' }}\n",
    "        {%- endif %}\n",
    "    {%- endif %}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "    {{- '<|im_start|>assistant\\n' }}\n",
    "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
    "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
    "    {%- endif %}\n",
    "{%- endif %}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1833cb6-0255-4812-a732-bea3874cc611",
   "metadata": {},
   "source": [
    "After spending some time understanding what's going on, you can create your own if you want to change its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9277be70-48c7-49c5-9087-2bfcc8440c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
