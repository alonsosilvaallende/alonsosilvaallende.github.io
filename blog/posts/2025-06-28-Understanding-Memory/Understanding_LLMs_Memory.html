<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alonso Silva">
<meta name="dcterms.date" content="2025-06-28">

<title>Understanding LLM Memory – Homepage</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-655c10e94217a44233fae49b0ef42ab6.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-ded2ac748760c3f4308a83a578027308.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script type="module" src="https://cdn.jsdelivr.net/npm/@marimo-team/islands@0.14.11/dist/main.js"></script>
<link href="https://cdn.jsdelivr.net/npm/@marimo-team/islands@0.14.11/dist/style.css" rel="stylesheet" crossorigin="anonymous">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Fira+Mono:wght@400;500;700&amp;family=Lora&amp;family=PT+Sans:wght@400;700&amp;display=swap" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
</head><body class="nav-fixed quarto-dark"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = true;
    const darkModeDefault = authorPrefersDark;
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script><marimo-filename hidden=""></marimo-filename>


<link rel="stylesheet" href="../../../styles.css">




<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Homepage</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../publications.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../blog/"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../til/"> 
<span class="menu-text">TIL</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/alonsosilvaallende" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="https://twitter.com/alonsosilva" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-twitter"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Understanding LLM Memory</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">code</div>
                <div class="quarto-category">analysis</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alonso Silva </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 28, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#understanding-llm-memory" id="toc-understanding-llm-memory" class="nav-link active" data-scroll-target="#understanding-llm-memory">Understanding LLM Memory</a></li>
  <li><a href="#handling-memory" id="toc-handling-memory" class="nav-link" data-scroll-target="#handling-memory">Handling Memory</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">






<section id="understanding-llm-memory" class="level2">
<h2 class="anchored" data-anchor-id="understanding-llm-memory">Understanding LLM Memory</h2>
<p>Despite what some people think (even some researchers I’ve met), language models don’t have any memory. The confusion comes, I suppose, from the fact that most people interact with models through some user interface like <a href="www.chatgpt.com">www.chatgpt.com</a>, which handles the memory for them (incidentally, in some interesting ways as explained below).</p>
<p>Let’s explore this further. Let’s use <a href="https://github.com/whitphx/transformers.js.py"><code>transformers_js_py</code></a> which allows us to use language models in the browser.</p>
<marimo-island data-app-id="main" data-cell-id="Hbol" data-reactive="true">
    <marimo-cell-output>
    <marimo-mime-renderer data-mime="&quot;application/vnd.marimo+error&quot;" data-data="[{&quot;msg&quot;: &quot;No module named 'numpy'&quot;, &quot;exception_type&quot;: &quot;ModuleNotFoundError&quot;, &quot;raising_cell&quot;: null, &quot;type&quot;: &quot;exception&quot;}]"></marimo-mime-renderer>
    </marimo-cell-output>
    <marimo-ui-element object-id="383358cd-65e5-7f64-2efa-6fc75c6893be" random-id="383358cd-65e5-7f64-2efa-6fc75c6893be"><marimo-code-editor data-initial-value="&quot;# Installation:\n# %pip install transformers_js_py\nimport marimo as mo\nimport numpy as np\nfrom transformers_js_py import import_transformers_js\n\ntransformers = await import_transformers_js()&quot;" data-label="null" data-language="&quot;python&quot;" data-placeholder="&quot;&quot;" data-disabled="false" data-show-copy-button="true" data-debounce="false"></marimo-code-editor></marimo-ui-element>
</marimo-island>
<p>Let’s download a small model and its tokenizer (it takes a few minutes):</p>
<marimo-island data-app-id="main" data-cell-id="MJUe" data-reactive="true">
    <marimo-cell-output>
    <marimo-mime-renderer data-mime="&quot;application/vnd.marimo+error&quot;" data-data="[{&quot;msg&quot;: &quot;An ancestor raised an exception (ModuleNotFoundError): &quot;, &quot;exception_type&quot;: &quot;ModuleNotFoundError&quot;, &quot;raising_cell&quot;: &quot;Hbol&quot;, &quot;type&quot;: &quot;exception&quot;}]"></marimo-mime-renderer>
    </marimo-cell-output>
    <marimo-ui-element object-id="ebf474b6-4b38-8444-bc3b-38183a4db28d" random-id="ebf474b6-4b38-8444-bc3b-38183a4db28d"><marimo-code-editor data-initial-value="&quot;model_id = \&quot;onnx-community/Qwen3-0.6B-ONNX\&quot;\n\nAutoTokenizer = transformers.AutoTokenizer\ntokenizer = await AutoTokenizer.from_pretrained(model_id)&quot;" data-label="null" data-language="&quot;python&quot;" data-placeholder="&quot;&quot;" data-disabled="false" data-show-copy-button="true" data-debounce="false"></marimo-code-editor></marimo-ui-element>
</marimo-island>
<marimo-island data-app-id="main" data-cell-id="vblA" data-reactive="true">
    <marimo-cell-output>
    <marimo-mime-renderer data-mime="&quot;application/vnd.marimo+error&quot;" data-data="[{&quot;msg&quot;: &quot;An ancestor raised an exception (ModuleNotFoundError): &quot;, &quot;exception_type&quot;: &quot;ModuleNotFoundError&quot;, &quot;raising_cell&quot;: &quot;Hbol&quot;, &quot;type&quot;: &quot;exception&quot;}]"></marimo-mime-renderer>
    </marimo-cell-output>
    <marimo-ui-element object-id="4fc32127-154b-a7ff-0653-3e6784d51da4" random-id="4fc32127-154b-a7ff-0653-3e6784d51da4"><marimo-code-editor data-initial-value="&quot;AutoModelForCausalLM = transformers.AutoModelForCausalLM\nwith mo.status.spinner(title=\&quot;Loading...\&quot;) as _spinner:\n    model = await AutoModelForCausalLM.from_pretrained(model_id)&quot;" data-label="null" data-language="&quot;python&quot;" data-placeholder="&quot;&quot;" data-disabled="false" data-show-copy-button="true" data-debounce="false"></marimo-code-editor></marimo-ui-element>
</marimo-island>
<p>We can ask the question <code>What's 2 + 2?</code> and get the following response:</p>
<marimo-island data-app-id="main" data-cell-id="bkHC" data-reactive="true">
    <marimo-cell-output>
    <marimo-mime-renderer data-mime="&quot;application/vnd.marimo+error&quot;" data-data="[{&quot;msg&quot;: &quot;An ancestor raised an exception (ModuleNotFoundError): &quot;, &quot;exception_type&quot;: &quot;ModuleNotFoundError&quot;, &quot;raising_cell&quot;: &quot;Hbol&quot;, &quot;type&quot;: &quot;exception&quot;}]"></marimo-mime-renderer>
    </marimo-cell-output>
    <marimo-ui-element object-id="8669a401-9b96-5727-14e5-b9665bef6fe8" random-id="8669a401-9b96-5727-14e5-b9665bef6fe8"><marimo-code-editor data-initial-value="&quot;user_input_0 = mo.ui.text(value=\&quot;What's 2 + 2?\&quot;)&quot;" data-label="null" data-language="&quot;python&quot;" data-placeholder="&quot;&quot;" data-disabled="false" data-show-copy-button="true" data-debounce="false"></marimo-code-editor></marimo-ui-element>
</marimo-island>
<marimo-island data-app-id="main" data-cell-id="lEQa" data-reactive="true">
    <marimo-cell-output>
    <marimo-mime-renderer data-mime="&quot;application/vnd.marimo+error&quot;" data-data="[{&quot;msg&quot;: &quot;An ancestor raised an exception (ModuleNotFoundError): &quot;, &quot;exception_type&quot;: &quot;ModuleNotFoundError&quot;, &quot;raising_cell&quot;: &quot;Hbol&quot;, &quot;type&quot;: &quot;exception&quot;}]"></marimo-mime-renderer>
    </marimo-cell-output>
    <marimo-ui-element object-id="98c281fb-6071-0264-8024-4a5c209d74ad" random-id="98c281fb-6071-0264-8024-4a5c209d74ad"><marimo-code-editor data-initial-value="&quot;messages_0 = [\n    {\&quot;role\&quot;: \&quot;user\&quot;, \&quot;content\&quot;: user_input_0.value},\n]\nprompt_0 = tokenizer.apply_chat_template(\n    messages_0, \n    tokenize=False, \n    add_generation_prompt=True, \n    enable_thinking=False\n)\ninputs_0 = tokenizer(prompt_0)\ninput_tokens_0 = np.array(inputs_0['input_ids'].tolist(), dtype=np.uint32)\ninput_len_0 = input_tokens_0.shape[1]\noutputs_0 = await model.generate(\n    **inputs_0, max_new_tokens=50, do_sample=False\n)\ntokens_as_uint32_0 = np.array(outputs_0.tolist(), dtype=np.uint32)\nassistant_response_0 = tokenizer.decode([int(token) for token in tokens_as_uint32_0[0]][input_len_0:-1])\nresponse_0 = mo.md(f\&quot;**Assistant response:**\\n\\n {assistant_response_0}\&quot;).callout(\&quot;info\&quot;)\nmo.vstack([user_input_0,response_0])&quot;" data-label="null" data-language="&quot;python&quot;" data-placeholder="&quot;&quot;" data-disabled="false" data-show-copy-button="true" data-debounce="false"></marimo-code-editor></marimo-ui-element>
</marimo-island>
<p>If we ask the model to add to the result 2 more, we get the following response:</p>
<marimo-island data-app-id="main" data-cell-id="PKri" data-reactive="true">
    <marimo-cell-output>
    <marimo-mime-renderer data-mime="&quot;application/vnd.marimo+error&quot;" data-data="[{&quot;msg&quot;: &quot;An ancestor raised an exception (ModuleNotFoundError): &quot;, &quot;exception_type&quot;: &quot;ModuleNotFoundError&quot;, &quot;raising_cell&quot;: &quot;Hbol&quot;, &quot;type&quot;: &quot;exception&quot;}]"></marimo-mime-renderer>
    </marimo-cell-output>
    <marimo-ui-element object-id="cf5d1acd-f7d1-75fa-7c7b-bb1233d37c94" random-id="cf5d1acd-f7d1-75fa-7c7b-bb1233d37c94"><marimo-code-editor data-initial-value="&quot;user_input_1 = mo.ui.text(value=\&quot;Add to the result 2 more.\&quot;, full_width=True)&quot;" data-label="null" data-language="&quot;python&quot;" data-placeholder="&quot;&quot;" data-disabled="false" data-show-copy-button="true" data-debounce="false"></marimo-code-editor></marimo-ui-element>
</marimo-island>
<marimo-island data-app-id="main" data-cell-id="Xref" data-reactive="true">
    <marimo-cell-output>
    <marimo-mime-renderer data-mime="&quot;application/vnd.marimo+error&quot;" data-data="[{&quot;msg&quot;: &quot;An ancestor raised an exception (ModuleNotFoundError): &quot;, &quot;exception_type&quot;: &quot;ModuleNotFoundError&quot;, &quot;raising_cell&quot;: &quot;Hbol&quot;, &quot;type&quot;: &quot;exception&quot;}]"></marimo-mime-renderer>
    </marimo-cell-output>
    <marimo-ui-element object-id="81b7e352-5601-00aa-b27f-69138a94f6de" random-id="81b7e352-5601-00aa-b27f-69138a94f6de"><marimo-code-editor data-initial-value="&quot;messages_1 = [\n    {\&quot;role\&quot;: \&quot;user\&quot;, \&quot;content\&quot;: user_input_1.value},\n]\nprompt_1 = tokenizer.apply_chat_template(messages_1, tokenize=False, add_generation_prompt=True, enable_thinking=False)\ninputs_1 = tokenizer(prompt_1)\ninput_tokens_1 = np.array(inputs_1['input_ids'].tolist(), dtype=np.uint32)\ninput_len_1 = input_tokens_1.shape[1]\noutputs_1 = await model.generate(\n    **inputs_1, max_new_tokens=50, do_sample=False\n)\ntokens_as_uint32_1 = np.array(outputs_1.tolist(), dtype=np.uint32)\nassistant_response_1 = tokenizer.decode([int(token) for token in tokens_as_uint32_1[0]][input_len_1:-1])\nresponse_1 = mo.md(f\&quot;**Assistant response:**\\n\\n {assistant_response_1}\&quot;).callout(\&quot;info\&quot;)\nmo.vstack([user_input_1,response_1])&quot;" data-label="null" data-language="&quot;python&quot;" data-placeholder="&quot;&quot;" data-disabled="false" data-show-copy-button="true" data-debounce="false"></marimo-code-editor></marimo-ui-element>
</marimo-island>
<p>The model doesn’t have any recollection of our previous conversation. Now, that’s completely expected since we haven’t provided the model any way to access that information.</p>
</section>
<section id="handling-memory" class="level2">
<h2 class="anchored" data-anchor-id="handling-memory">Handling Memory</h2>
<p>The simplest way to handle memory is to provide our previous conversation within a list of messages:</p>
<marimo-island data-app-id="main" data-cell-id="SFPL" data-reactive="true">
    <marimo-cell-output>
    <marimo-mime-renderer data-mime="&quot;application/vnd.marimo+error&quot;" data-data="[{&quot;msg&quot;: &quot;An ancestor raised an exception (ModuleNotFoundError): &quot;, &quot;exception_type&quot;: &quot;ModuleNotFoundError&quot;, &quot;raising_cell&quot;: &quot;Hbol&quot;, &quot;type&quot;: &quot;exception&quot;}]"></marimo-mime-renderer>
    </marimo-cell-output>
    <marimo-ui-element object-id="d408de81-eed8-f525-4c6a-df7263a77835" random-id="d408de81-eed8-f525-4c6a-df7263a77835"><marimo-code-editor data-initial-value="&quot;messages_2 = messages_0 + [\n    {\&quot;role\&quot;: \&quot;assistant\&quot;, \&quot;content\&quot;: assistant_response_0},\n] +  messages_1\nprompt_2 = tokenizer.apply_chat_template(messages_2, tokenize=False, add_generation_prompt=True, enable_thinking=False)&quot;" data-label="null" data-language="&quot;python&quot;" data-placeholder="&quot;&quot;" data-disabled="false" data-show-copy-button="true" data-debounce="false"></marimo-code-editor></marimo-ui-element>
</marimo-island>
<p>With all these messages we obtain the following response:</p>
<marimo-island data-app-id="main" data-cell-id="BYtC" data-reactive="true">
    <marimo-cell-output>
    <marimo-mime-renderer data-mime="&quot;application/vnd.marimo+error&quot;" data-data="[{&quot;msg&quot;: &quot;An ancestor raised an exception (ModuleNotFoundError): &quot;, &quot;exception_type&quot;: &quot;ModuleNotFoundError&quot;, &quot;raising_cell&quot;: &quot;Hbol&quot;, &quot;type&quot;: &quot;exception&quot;}]"></marimo-mime-renderer>
    </marimo-cell-output>
    <marimo-ui-element object-id="189e1ec9-a587-b744-7f90-cbecf87748fa" random-id="189e1ec9-a587-b744-7f90-cbecf87748fa"><marimo-code-editor data-initial-value="&quot;inputs_2 = tokenizer(prompt_2)\ninput_tokens_2 = np.array(inputs_2['input_ids'].tolist(), dtype=np.uint32)\ninput_len_2 = input_tokens_2.shape[1]\noutputs_2 = await model.generate(\n    **inputs_2, max_new_tokens=50, do_sample=False\n)\ntokens_as_uint32_2 = np.array(outputs_2.tolist(), dtype=np.uint32)\nassistant_response_2 = tokenizer.decode([int(token) for token in tokens_as_uint32_2[0]][input_len_2:-1])\nresponse_2 = mo.md(f\&quot;**Assistant response:**\\n\\n {assistant_response_2}\&quot;).callout(\&quot;info\&quot;)\nmo.vstack([user_input_1,response_2])&quot;" data-label="null" data-language="&quot;python&quot;" data-placeholder="&quot;&quot;" data-disabled="false" data-show-copy-button="true" data-debounce="false"></marimo-code-editor></marimo-ui-element>
</marimo-island>
<p>That works! The problem with that approach is that we need to be mindful of the context length of the model. We could for example store only the last 10 messages or so and perhaps the system message if there is one (in this example, we don’t have one).</p>
<p>More sophisticated approaches could be to store the messages and its responses in a vector database and retrieve the most closely related ones. Similarly, we could store the information as a graph in a graph database and retrieve the nodes and edges most closely related.</p>
<p>ChatGPT’s memory feature is very interesting because it uses memory as a tool. Let’s take a look at that.</p>
<p>We can define the tools as follows:</p>
<pre><code>tools = [
    {
        "type": "function",
        "function": {
            "name": "biography",
            "description": "The biography tool allows you to persist user information across conversations. Use this tool and write whatever user information you want to remember. The information will appear in the model set context below in future conversations.",
            "parameters": {
                "properties": {
                    "user_information": {
                        "description": "Information from the user you want to remember across conversations.",
                        "type": "string",
                    }
                },
                "required": ["user_information"],
                "type": "object",
            },
        },
    }
]</code></pre>
<marimo-island data-app-id="main" data-cell-id="RGSE" data-reactive="true">
    <marimo-cell-output>
    <marimo-mime-renderer data-mime="&quot;application/vnd.marimo+error&quot;" data-data="[{&quot;msg&quot;: &quot;An ancestor raised an exception (ModuleNotFoundError): &quot;, &quot;exception_type&quot;: &quot;ModuleNotFoundError&quot;, &quot;raising_cell&quot;: &quot;Hbol&quot;, &quot;type&quot;: &quot;exception&quot;}]"></marimo-mime-renderer>
    </marimo-cell-output>
    <marimo-ui-element object-id="4b29a179-f507-dc96-a28d-80b8e1ddfbf5" random-id="4b29a179-f507-dc96-a28d-80b8e1ddfbf5"><marimo-code-editor data-initial-value="&quot;user_input_3 = mo.ui.text(value=\&quot;Hello! In our conversations, remember that I prefer Polars to Pandas.\&quot;, full_width=True)&quot;" data-label="null" data-language="&quot;python&quot;" data-placeholder="&quot;&quot;" data-disabled="false" data-show-copy-button="true" data-debounce="false"></marimo-code-editor></marimo-ui-element>
</marimo-island>
<marimo-island data-app-id="main" data-cell-id="Kclp" data-reactive="true">
    <marimo-cell-output>
    <marimo-mime-renderer data-mime="&quot;application/vnd.marimo+error&quot;" data-data="[{&quot;msg&quot;: &quot;An ancestor raised an exception (ModuleNotFoundError): &quot;, &quot;exception_type&quot;: &quot;ModuleNotFoundError&quot;, &quot;raising_cell&quot;: &quot;Hbol&quot;, &quot;type&quot;: &quot;exception&quot;}]"></marimo-mime-renderer>
    </marimo-cell-output>
    <marimo-ui-element object-id="bf096952-f868-b41d-e622-ddb344c273c2" random-id="bf096952-f868-b41d-e622-ddb344c273c2"><marimo-code-editor data-initial-value="&quot;messages = [\n    {\&quot;role\&quot;: \&quot;user\&quot;, \&quot;content\&quot;: user_input_3.value},\n]\n\ntools = [\n    {\n        \&quot;type\&quot;: \&quot;function\&quot;,\n        \&quot;function\&quot;: {\n            \&quot;name\&quot;: \&quot;biography\&quot;,\n            \&quot;description\&quot;: \&quot;The biography tool allows you to persist user information across conversations. Use this tool and write whatever user information you want to remember. The information will appear in the model set context below in future conversations.\&quot;,\n            \&quot;parameters\&quot;: {\n                \&quot;properties\&quot;: {\n                    \&quot;user_information\&quot;: {\n                        \&quot;description\&quot;: \&quot;Information from the user you want to remember across conversations.\&quot;,\n                        \&quot;type\&quot;: \&quot;string\&quot;,\n                    }\n                },\n                \&quot;required\&quot;: [\&quot;user_information\&quot;],\n                \&quot;type\&quot;: \&quot;object\&quot;,\n            },\n        },\n    }\n]\n\nprompt_3 = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, tools=tools, enable_thinking=False)\nprompt_3 += \&quot;<tool_call>\&quot;\ninputs_3 = tokenizer(prompt_3)\ninput_tokens_3 = np.array(inputs_3['input_ids'].tolist(), dtype=np.uint32)\ninput_len_3 = input_tokens_3.shape[1]\noutputs_3 = await model.generate(\n    **inputs_3, max_new_tokens=50, do_sample=False\n)\ntokens_as_uint32_3 = np.array(outputs_3.tolist(), dtype=np.uint32)\nassistant_response_3 = tokenizer.decode([int(token) for token in tokens_as_uint32_3[0]][input_len_3:-1])\nassistant_response_3_md = assistant_response_3.replace(\&quot;<\&quot;, \&quot;&amp;lt;\&quot;).replace(\&quot;>\&quot;, \&quot;&amp;gt;\&quot;)\nresponse_3 = mo.md(f\&quot;**Assistant response:**\\n\\n {assistant_response_3_md}\&quot;).callout(\&quot;info\&quot;)\nmo.vstack([user_input_3,response_3])&quot;" data-label="null" data-language="&quot;python&quot;" data-placeholder="&quot;&quot;" data-disabled="false" data-show-copy-button="true" data-debounce="false"></marimo-code-editor></marimo-ui-element>
</marimo-island>
<p>We can then store the tool call as a text file and provide it in the context.</p>
<p>I really like that idea. It’s very simple and clean!</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/alonsosilvaallende\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb2" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Understanding LLM Memory</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> Alonso Silva</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> '2025-06-28'</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">  - code</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - analysis</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-summary: Show the code</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="an">filters:</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co">  - marimo-team/marimo</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co">  jupytext:</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co">    text_representation:</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co">      extension: .qmd</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co">      format_name: quarto</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co">      format_version: '1.0'</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co">      jupytext_version: 1.17.2</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co">  kernelspec:</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co">    display_name: Python 3 (ipykernel)</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co">    language: python</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co">    name: python3</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="fu">## Understanding LLM Memory</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>Despite what some people think (even some researchers I've met), language models don't have any memory.</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>The confusion comes, I suppose, from the fact that most people interact with models</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>through some user interface like <span class="co">[</span><span class="ot">www.chatgpt.com</span><span class="co">](www.chatgpt.com)</span>, which handles the memory for them</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>(incidentally, in some interesting ways as explained below).</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>Let's explore this further. Let's use <span class="co">[</span><span class="ot">`transformers_js_py`</span><span class="co">](https://github.com/whitphx/transformers.js.py)</span> which allows us to use language models in the browser.</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="in">```python {.marimo}</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Installation:</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a><span class="co"># %pip install transformers_js_py</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> marimo <span class="im">as</span> mo</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers_js_py <span class="im">import</span> import_transformers_js</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>transformers <span class="op">=</span> <span class="cf">await</span> import_transformers_js()</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>Let's download a small model and its tokenizer (it takes a few minutes):</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a><span class="in">```python {.marimo}</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>model_id <span class="op">=</span> <span class="st">"onnx-community/Qwen3-0.6B-ONNX"</span></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>AutoTokenizer <span class="op">=</span> transformers.AutoTokenizer</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> <span class="cf">await</span> AutoTokenizer.from_pretrained(model_id)</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a><span class="in">```python {.marimo}</span></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>AutoModelForCausalLM <span class="op">=</span> transformers.AutoModelForCausalLM</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> mo.status.spinner(title<span class="op">=</span><span class="st">"Loading..."</span>) <span class="im">as</span> _spinner:</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> <span class="cf">await</span> AutoModelForCausalLM.from_pretrained(model_id)</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>We can ask the question <span class="in">`What's 2 + 2?`</span> and get the following response:</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a><span class="in">```python {.marimo}</span></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>user_input_0 <span class="op">=</span> mo.ui.text(value<span class="op">=</span><span class="st">"What's 2 + 2?"</span>)</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a><span class="in">```python {.marimo}</span></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>messages_0 <span class="op">=</span> [</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>    {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: user_input_0.value},</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>prompt_0 <span class="op">=</span> tokenizer.apply_chat_template(</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>    messages_0, </span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>    tokenize<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>    add_generation_prompt<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>    enable_thinking<span class="op">=</span><span class="va">False</span></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>inputs_0 <span class="op">=</span> tokenizer(prompt_0)</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>input_tokens_0 <span class="op">=</span> np.array(inputs_0[<span class="st">'input_ids'</span>].tolist(), dtype<span class="op">=</span>np.uint32)</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>input_len_0 <span class="op">=</span> input_tokens_0.shape[<span class="dv">1</span>]</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>outputs_0 <span class="op">=</span> <span class="cf">await</span> model.generate(</span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>    <span class="op">**</span>inputs_0, max_new_tokens<span class="op">=</span><span class="dv">50</span>, do_sample<span class="op">=</span><span class="va">False</span></span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>tokens_as_uint32_0 <span class="op">=</span> np.array(outputs_0.tolist(), dtype<span class="op">=</span>np.uint32)</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>assistant_response_0 <span class="op">=</span> tokenizer.decode([<span class="bu">int</span>(token) <span class="cf">for</span> token <span class="kw">in</span> tokens_as_uint32_0[<span class="dv">0</span>]][input_len_0:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>response_0 <span class="op">=</span> mo.md(<span class="ss">f"**Assistant response:**</span><span class="ch">\n\n</span><span class="ss"> </span><span class="sc">{</span>assistant_response_0<span class="sc">}</span><span class="ss">"</span>).callout(<span class="st">"info"</span>)</span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a>mo.vstack([user_input_0,response_0])</span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>If we ask the model to add to the result 2 more, we get the following response:</span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a><span class="in">```python {.marimo}</span></span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a>user_input_1 <span class="op">=</span> mo.ui.text(value<span class="op">=</span><span class="st">"Add to the result 2 more."</span>, full_width<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a><span class="in">```python {.marimo}</span></span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>messages_1 <span class="op">=</span> [</span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a>    {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: user_input_1.value},</span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>prompt_1 <span class="op">=</span> tokenizer.apply_chat_template(messages_1, tokenize<span class="op">=</span><span class="va">False</span>, add_generation_prompt<span class="op">=</span><span class="va">True</span>, enable_thinking<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>inputs_1 <span class="op">=</span> tokenizer(prompt_1)</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a>input_tokens_1 <span class="op">=</span> np.array(inputs_1[<span class="st">'input_ids'</span>].tolist(), dtype<span class="op">=</span>np.uint32)</span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a>input_len_1 <span class="op">=</span> input_tokens_1.shape[<span class="dv">1</span>]</span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a>outputs_1 <span class="op">=</span> <span class="cf">await</span> model.generate(</span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>    <span class="op">**</span>inputs_1, max_new_tokens<span class="op">=</span><span class="dv">50</span>, do_sample<span class="op">=</span><span class="va">False</span></span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a>tokens_as_uint32_1 <span class="op">=</span> np.array(outputs_1.tolist(), dtype<span class="op">=</span>np.uint32)</span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a>assistant_response_1 <span class="op">=</span> tokenizer.decode([<span class="bu">int</span>(token) <span class="cf">for</span> token <span class="kw">in</span> tokens_as_uint32_1[<span class="dv">0</span>]][input_len_1:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a>response_1 <span class="op">=</span> mo.md(<span class="ss">f"**Assistant response:**</span><span class="ch">\n\n</span><span class="ss"> </span><span class="sc">{</span>assistant_response_1<span class="sc">}</span><span class="ss">"</span>).callout(<span class="st">"info"</span>)</span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a>mo.vstack([user_input_1,response_1])</span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a>The model doesn't have any recollection of our previous conversation.</span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a>Now, that's completely expected since we haven't provided the model any way to access that information. </span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a><span class="fu">## Handling Memory</span></span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a>The simplest way to handle memory is to provide our previous conversation within a list of messages:</span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a><span class="in">```python {.marimo}</span></span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a>messages_2 <span class="op">=</span> messages_0 <span class="op">+</span> [</span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a>    {<span class="st">"role"</span>: <span class="st">"assistant"</span>, <span class="st">"content"</span>: assistant_response_0},</span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a>] <span class="op">+</span>  messages_1</span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a>prompt_2 <span class="op">=</span> tokenizer.apply_chat_template(messages_2, tokenize<span class="op">=</span><span class="va">False</span>, add_generation_prompt<span class="op">=</span><span class="va">True</span>, enable_thinking<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a>With all these messages we obtain the following response:</span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a><span class="in">```python {.marimo}</span></span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a>inputs_2 <span class="op">=</span> tokenizer(prompt_2)</span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a>input_tokens_2 <span class="op">=</span> np.array(inputs_2[<span class="st">'input_ids'</span>].tolist(), dtype<span class="op">=</span>np.uint32)</span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a>input_len_2 <span class="op">=</span> input_tokens_2.shape[<span class="dv">1</span>]</span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a>outputs_2 <span class="op">=</span> <span class="cf">await</span> model.generate(</span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a>    <span class="op">**</span>inputs_2, max_new_tokens<span class="op">=</span><span class="dv">50</span>, do_sample<span class="op">=</span><span class="va">False</span></span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a>tokens_as_uint32_2 <span class="op">=</span> np.array(outputs_2.tolist(), dtype<span class="op">=</span>np.uint32)</span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a>assistant_response_2 <span class="op">=</span> tokenizer.decode([<span class="bu">int</span>(token) <span class="cf">for</span> token <span class="kw">in</span> tokens_as_uint32_2[<span class="dv">0</span>]][input_len_2:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a>response_2 <span class="op">=</span> mo.md(<span class="ss">f"**Assistant response:**</span><span class="ch">\n\n</span><span class="ss"> </span><span class="sc">{</span>assistant_response_2<span class="sc">}</span><span class="ss">"</span>).callout(<span class="st">"info"</span>)</span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a>mo.vstack([user_input_1,response_2])</span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a>That works! The problem with that approach is that we need to be mindful of the context length of the model.</span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a>We could for example store only the last 10 messages or so and perhaps the system message if there is one (in this example, we don't have one).</span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a>More sophisticated approaches could be to store the messages and its responses in a vector database and retrieve the most closely related ones.</span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a>Similarly, we could store the information as a graph in a graph database and retrieve the nodes and edges most closely related.</span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a>ChatGPT's memory feature is very interesting because it uses memory as a tool.</span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a>Let's take a look at that.</span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a>We can define the tools as follows:</span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a><span class="in">tools = [</span></span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a><span class="in">    {</span></span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a><span class="in">        "type": "function",</span></span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a><span class="in">        "function": {</span></span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a><span class="in">            "name": "biography",</span></span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a><span class="in">            "description": "The biography tool allows you to persist user information across conversations. Use this tool and write whatever user information you want to remember. The information will appear in the model set context below in future conversations.",</span></span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a><span class="in">            "parameters": {</span></span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a><span class="in">                "properties": {</span></span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a><span class="in">                    "user_information": {</span></span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a><span class="in">                        "description": "Information from the user you want to remember across conversations.",</span></span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a><span class="in">                        "type": "string",</span></span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a><span class="in">                    }</span></span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a><span class="in">                },</span></span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a><span class="in">                "required": ["user_information"],</span></span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a><span class="in">                "type": "object",</span></span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a><span class="in">            },</span></span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a><span class="in">        },</span></span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a><span class="in">    }</span></span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a><span class="in">]</span></span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a><span class="in">```python {.marimo}</span></span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a>user_input_3 <span class="op">=</span> mo.ui.text(value<span class="op">=</span><span class="st">"Hello! In our conversations, remember that I prefer Polars to Pandas."</span>, full_width<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a><span class="in">```python {.marimo}</span></span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a>messages <span class="op">=</span> [</span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a>    {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: user_input_3.value},</span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a>tools <span class="op">=</span> [</span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a>        <span class="st">"type"</span>: <span class="st">"function"</span>,</span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a>        <span class="st">"function"</span>: {</span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a>            <span class="st">"name"</span>: <span class="st">"biography"</span>,</span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a>            <span class="st">"description"</span>: <span class="st">"The biography tool allows you to persist user information across conversations. Use this tool and write whatever user information you want to remember. The information will appear in the model set context below in future conversations."</span>,</span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a>            <span class="st">"parameters"</span>: {</span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a>                <span class="st">"properties"</span>: {</span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"user_information"</span>: {</span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"description"</span>: <span class="st">"Information from the user you want to remember across conversations."</span>,</span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"type"</span>: <span class="st">"string"</span>,</span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a>                    }</span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a>                },</span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a>                <span class="st">"required"</span>: [<span class="st">"user_information"</span>],</span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a>                <span class="st">"type"</span>: <span class="st">"object"</span>,</span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a>            },</span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a>prompt_3 <span class="op">=</span> tokenizer.apply_chat_template(messages, tokenize<span class="op">=</span><span class="va">False</span>, add_generation_prompt<span class="op">=</span><span class="va">True</span>, tools<span class="op">=</span>tools, enable_thinking<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-213"><a href="#cb2-213" aria-hidden="true" tabindex="-1"></a>prompt_3 <span class="op">+=</span> <span class="st">"&lt;tool_call&gt;"</span></span>
<span id="cb2-214"><a href="#cb2-214" aria-hidden="true" tabindex="-1"></a>inputs_3 <span class="op">=</span> tokenizer(prompt_3)</span>
<span id="cb2-215"><a href="#cb2-215" aria-hidden="true" tabindex="-1"></a>input_tokens_3 <span class="op">=</span> np.array(inputs_3[<span class="st">'input_ids'</span>].tolist(), dtype<span class="op">=</span>np.uint32)</span>
<span id="cb2-216"><a href="#cb2-216" aria-hidden="true" tabindex="-1"></a>input_len_3 <span class="op">=</span> input_tokens_3.shape[<span class="dv">1</span>]</span>
<span id="cb2-217"><a href="#cb2-217" aria-hidden="true" tabindex="-1"></a>outputs_3 <span class="op">=</span> <span class="cf">await</span> model.generate(</span>
<span id="cb2-218"><a href="#cb2-218" aria-hidden="true" tabindex="-1"></a>    <span class="op">**</span>inputs_3, max_new_tokens<span class="op">=</span><span class="dv">50</span>, do_sample<span class="op">=</span><span class="va">False</span></span>
<span id="cb2-219"><a href="#cb2-219" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-220"><a href="#cb2-220" aria-hidden="true" tabindex="-1"></a>tokens_as_uint32_3 <span class="op">=</span> np.array(outputs_3.tolist(), dtype<span class="op">=</span>np.uint32)</span>
<span id="cb2-221"><a href="#cb2-221" aria-hidden="true" tabindex="-1"></a>assistant_response_3 <span class="op">=</span> tokenizer.decode([<span class="bu">int</span>(token) <span class="cf">for</span> token <span class="kw">in</span> tokens_as_uint32_3[<span class="dv">0</span>]][input_len_3:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb2-222"><a href="#cb2-222" aria-hidden="true" tabindex="-1"></a>assistant_response_3_md <span class="op">=</span> assistant_response_3.replace(<span class="st">"&lt;"</span>, <span class="st">"&amp;lt;"</span>).replace(<span class="st">"&gt;"</span>, <span class="st">"&amp;gt;"</span>)</span>
<span id="cb2-223"><a href="#cb2-223" aria-hidden="true" tabindex="-1"></a>response_3 <span class="op">=</span> mo.md(<span class="ss">f"**Assistant response:**</span><span class="ch">\n\n</span><span class="ss"> </span><span class="sc">{</span>assistant_response_3_md<span class="sc">}</span><span class="ss">"</span>).callout(<span class="st">"info"</span>)</span>
<span id="cb2-224"><a href="#cb2-224" aria-hidden="true" tabindex="-1"></a>mo.vstack([user_input_3,response_3])</span>
<span id="cb2-225"><a href="#cb2-225" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-226"><a href="#cb2-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-227"><a href="#cb2-227" aria-hidden="true" tabindex="-1"></a>We can then store the tool call as a text file and provide it in the context.</span>
<span id="cb2-228"><a href="#cb2-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-229"><a href="#cb2-229" aria-hidden="true" tabindex="-1"></a>I really like that idea. It's very simple and clean!</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>